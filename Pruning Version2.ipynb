{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d78e987-a20f-4e20-82e8-cd310e7efef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b707d6c1-658f-4f2d-ac85-b23002d55230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QuadraticConstraintModel import get_leaf_samples\n",
    "\n",
    "from QuadraticConstraintModel import constrained_optimization_gurobi\n",
    "\n",
    "from QuadraticConstraintModel import predict_from_COF\n",
    "\n",
    "from QuadraticConstraintModel import  get_h_from_COF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b557d1a9-33a4-4a93-9d39-87f78ed3b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_root_mean_square_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Normalized Root Mean Square Error (NRMSE) between y_true and y_pred.\n",
    "    If the range of y_true is zero, it normalizes by the number of samples * outputs.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (np.ndarray): Ground truth values, shape (n_samples, n_outputs)\n",
    "        y_pred (np.ndarray): Predicted values, shape (n_samples, n_outputs)\n",
    "\n",
    "    Returns:\n",
    "        float: NRMSE value\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Compute RMSE\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    \n",
    "    # Compute range\n",
    "    y_range = np.max(y_true) - np.min(y_true)\n",
    "    \n",
    "    if y_range != 0:\n",
    "        # Normalize by range\n",
    "        return rmse / y_range\n",
    "    else:\n",
    "        # Normalize by n_samples * n_outputs\n",
    "        n_samples, n_outputs = y_true.shape\n",
    "        return np.sqrt(np.sum((y_true - y_pred) ** 2) / (n_samples * n_outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b1eea4-6d2b-4f75-af36-ff2deaa33095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load DataSet\n",
    "def load_dataset(file_path, num_attributes=2, num_classes=2):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.iloc[:, 0 :  num_attributes].values\n",
    "    y = data.iloc[:,  num_attributes:  num_attributes + num_classes].values\n",
    "    # y = data.iloc[:, 9:10].values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5aaa130d-746a-44d7-bf42-317ea6a6b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_name = \"navigation_old\"\n",
    "n_samples = 500000\n",
    "X, y = load_dataset(f\"Dataset/{sys_name}/{sys_name}_{n_samples}/data_{sys_name}_{n_samples}.csv\",num_attributes=4, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9db64a7-76be-4354-abd6-18592925d0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of X_Training = (249999, 4) \n",
      " Shape of X_Testing = (250000, 4)\n",
      " Shape of Y_Training = (249999, 4) \n",
      " Shape of Y_Testing = (250000, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.5)\n",
    "print(f\" Shape of X_Training = {X_train.shape} \\n Shape of X_Testing = {X_test.shape}\")\n",
    "print(f\" Shape of Y_Training = {y_train.shape} \\n Shape of Y_Testing = {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "533224d0-b002-47d4-ab13-4bd90da0ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTreeStats(tree):\n",
    "    # Access the tree_ object\n",
    "    n_nodes = tree.tree_.node_count\n",
    "    n_leaves = tree.get_n_leaves()\n",
    "    children_left = tree.tree_.children_left\n",
    "    children_right = tree.tree_.children_right\n",
    "    feature = tree.tree_.feature\n",
    "    threshold = tree.tree_.threshold\n",
    "    impurity = tree.tree_.impurity\n",
    "    n_node_samples = tree.tree_.n_node_samples\n",
    "    values = tree.tree_.value\n",
    "    \n",
    "    print(f\"Total nodes: {n_nodes}\\n\")\n",
    "    \n",
    "    print(\"Number of leaves:\", n_leaves)\n",
    "    \n",
    "    for node in range(n_nodes):\n",
    "        if children_left[node] == children_right[node]:  # leaf\n",
    "            print(f\"🌿 Leaf {node}:\")\n",
    "        else:\n",
    "            print(f\"📌 Node {node}: feature {feature[node]} ≤ {threshold[node]:.4f}\")\n",
    "    \n",
    "        print(f\"   Samples: {n_node_samples[node]}\")\n",
    "        print(f\"   Impurity (MSE): {impurity[node]:.4f}\")\n",
    "        print(f\"   Value (mean target): {values[node].ravel()[0]:.4f}\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93dbf0a3-f6a4-4025-88bb-50c379cc3826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_tree_hmin(tree, leaf_COFS_dict, h_min, alpha=2):\n",
    "    \"\"\"\n",
    "    Prune COF tree based on h_min threshold.\n",
    "    \n",
    "    Args:\n",
    "        tree: Trained COF tree object\n",
    "        leaf_COFS_dict: dict storing leaf information (indices, M, m0, h)\n",
    "        h_min: Minimum h value threshold\n",
    "        alpha: pruning parameter (optional)\n",
    "        \n",
    "    Returns:\n",
    "        pruned_leaf_dict: updated leaf_COFS_dict after pruning\n",
    "    \"\"\"\n",
    "    pruned_leaf_dict = leaf_COFS_dict.copy()\n",
    "    pruning_happened = True\n",
    "    \n",
    "    while pruning_happened:\n",
    "        pruning_happened = False\n",
    "        for node, leaf_info in list(pruned_leaf_dict.items()):\n",
    "            h = leaf_info[\"CO_Model\"][\"h\"]\n",
    "            if h < h_min:\n",
    "                # Prune node by merging with parent\n",
    "                parent = tree.get_parent(node)\n",
    "                if parent is None:\n",
    "                    continue\n",
    "                \n",
    "                combined_indices = leaf_info[\"indices\"] + pruned_leaf_dict[parent][\"indices\"]\n",
    "                \n",
    "                # Recompute M, m0, h using optimizer on combined indices\n",
    "                M, m0, new_h = optimize_leaf(tree, combined_indices)\n",
    "                \n",
    "                # Update parent\n",
    "                pruned_leaf_dict[parent] = {\n",
    "                    \"leaf_id\": parent,\n",
    "                    \"CO_Model\": {\"h\": new_h, \"M\": M, \"m0\": m0},\n",
    "                    \"indices\": combined_indices,\n",
    "                    \"no_samples\": len(combined_indices)\n",
    "                }\n",
    "                \n",
    "                # Delete child node\n",
    "                del pruned_leaf_dict[node]\n",
    "                \n",
    "                pruning_happened = True\n",
    "                print(f\"🌳 Pruning triggered at node {node} with h={h:.6f}, alpha={alpha}\")\n",
    "    \n",
    "    print(f\"➡️ Pruning completed. Remaining leaves: {len(pruned_leaf_dict)}\")\n",
    "    return pruned_leaf_dict\n",
    "\n",
    "\n",
    "def prune_tree_cost(tree, leaf_COFS_dict, alpha=2):\n",
    "    \"\"\"\n",
    "    Prune COF tree based on cost function: sum(h) + alpha * number_of_leaves\n",
    "    \n",
    "    Args:\n",
    "        tree: Trained COF tree object\n",
    "        leaf_COFS_dict: dict storing leaf info (indices, M, m0, h)\n",
    "        alpha: regularization parameter\n",
    "        \n",
    "    Returns:\n",
    "        pruned_leaf_dict: updated leaf_COFS_dict after cost-based pruning\n",
    "    \"\"\"\n",
    "    pruned_leaf_dict = leaf_COFS_dict.copy()\n",
    "    pruning_happened = True\n",
    "    \n",
    "    while pruning_happened:\n",
    "        pruning_happened = False\n",
    "        min_cost_reduction = 0\n",
    "        leaf_to_prune = None\n",
    "        merge_with = None\n",
    "        \n",
    "        # Evaluate cost reduction for each leaf\n",
    "        for node, leaf_info in pruned_leaf_dict.items():\n",
    "            parent = tree.get_parent(node)\n",
    "            if parent is None:\n",
    "                continue\n",
    "            \n",
    "            combined_indices = leaf_info[\"indices\"] + pruned_leaf_dict[parent][\"indices\"]\n",
    "            M, m0, new_h = optimize_leaf(tree, combined_indices)\n",
    "            \n",
    "            # Compute current cost and merged cost\n",
    "            current_cost = sum([info[\"CO_Model\"][\"h\"] for info in pruned_leaf_dict.values()]) + alpha * len(pruned_leaf_dict)\n",
    "            merged_cost = sum([info[\"CO_Model\"][\"h\"] for n, info in pruned_leaf_dict.items() if n not in [node, parent]] + [new_h]) + alpha * (len(pruned_leaf_dict)-1)\n",
    "            \n",
    "            cost_reduction = current_cost - merged_cost\n",
    "            if cost_reduction > min_cost_reduction:\n",
    "                min_cost_reduction = cost_reduction\n",
    "                leaf_to_prune = node\n",
    "                merge_with = parent\n",
    "        \n",
    "        if leaf_to_prune is not None:\n",
    "            # Perform the merge\n",
    "            combined_indices = pruned_leaf_dict[leaf_to_prune][\"indices\"] + pruned_leaf_dict[merge_with][\"indices\"]\n",
    "            M, m0, new_h = optimize_leaf(tree, combined_indices)\n",
    "            \n",
    "            pruned_leaf_dict[merge_with] = {\n",
    "                \"leaf_id\": merge_with,\n",
    "                \"CO_Model\": {\"h\": new_h, \"M\": M, \"m0\": m0},\n",
    "                \"indices\": combined_indices,\n",
    "                \"no_samples\": len(combined_indices)\n",
    "            }\n",
    "            del pruned_leaf_dict[leaf_to_prune]\n",
    "            pruning_happened = True\n",
    "            print(f\"🌳 Cost-based pruning: merged node {leaf_to_prune} into parent {merge_with}, cost reduction={min_cost_reduction:.6f}\")\n",
    "    \n",
    "    print(f\"➡️ Cost-based pruning completed. Remaining leaves: {len(pruned_leaf_dict)}\")\n",
    "    return pruned_leaf_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b4059da-1d08-4c76-a82e-c52190df66be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Your least squares optimizer\n",
    "# ----------------------------------------\n",
    "def least_squares_solution(X_leaf, y_leaf):\n",
    "    n_samples, n_features = X_leaf.shape\n",
    "\n",
    "    # Step 1: augment X with a column of ones for intercept\n",
    "    X_aug = np.hstack([np.ones((n_samples, 1)), X_leaf])  # (n × (p+1))\n",
    "\n",
    "    # Step 2: closed-form least squares\n",
    "    XtX = X_aug.T @ X_aug\n",
    "    XtY = X_aug.T @ y_leaf\n",
    "    Theta = np.linalg.pinv(XtX) @ XtY   # pseudo-inverse for safety\n",
    "\n",
    "    # Step 3: extract intercepts and coefficients\n",
    "    m0 = Theta[0, :] if Theta.ndim > 1 else Theta[0]       # (n_outputs,) or scalar\n",
    "    M = Theta[1:, :].T if Theta.ndim > 1 else Theta[1:].reshape(1, -1)  # (n_outputs × n_features) or (1 × n_features)\n",
    "\n",
    "    # Step 4: compute residual sum of squares\n",
    "    Y_hat = X_aug @ Theta\n",
    "    residuals = y_leaf - Y_hat\n",
    "    h_val = np.sum(residuals**2)\n",
    "\n",
    "    return M, m0, h_val\n",
    "\n",
    "# ----------------------------------------\n",
    "# Updated prune function using least squares\n",
    "# ----------------------------------------\n",
    "def train_and_prune_COF_tree(X_train, y_train, \n",
    "                             initial_tree_params=None,\n",
    "                             alpha=1,\n",
    "                             h_min=1,\n",
    "                             pruning_mode=\"hmin\"):\n",
    "    \n",
    "    tree = DecisionTreeRegressor(**(initial_tree_params or {}))\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    # Leaf info\n",
    "    leaf_COFS_dict = {}\n",
    "    leaf_ids = tree.apply(X_train)\n",
    "    unique_leaves = np.unique(leaf_ids)\n",
    "    \n",
    "    for leaf in unique_leaves:\n",
    "        indices = np.where(leaf_ids == leaf)[0]\n",
    "        X_leaf = X_train[indices]\n",
    "        y_leaf = y_train[indices]\n",
    "        M, m0, h = least_squares_solution(X_leaf, y_leaf)\n",
    "        leaf_COFS_dict[leaf] = {\"indices\": indices, \"M\": M, \"m0\": m0, \"h\": h}\n",
    "    \n",
    "    # Start pruning\n",
    "    pruning_done = True\n",
    "    while pruning_done:\n",
    "        pruning_done = False\n",
    "        # Sort leaves by h\n",
    "        leaves_sorted = sorted(leaf_COFS_dict.items(), key=lambda x: x[1][\"h\"])\n",
    "        \n",
    "        for leaf, info in leaves_sorted:\n",
    "            h_leaf = info[\"h\"]\n",
    "            \n",
    "            if pruning_mode == \"hmin\" and h_leaf <= h_min:\n",
    "                # Prune leaf\n",
    "                print(f\"🌳 h_min pruning: Removing leaf {leaf} with h={h_leaf:.6f}\")\n",
    "                del leaf_COFS_dict[leaf]\n",
    "                pruning_done = True\n",
    "                break\n",
    "                \n",
    "            elif pruning_mode == \"cost\":\n",
    "                current_cost = sum([v[\"h\"] for v in leaf_COFS_dict.values()]) + alpha * len(leaf_COFS_dict)\n",
    "                temp_dict = leaf_COFS_dict.copy()\n",
    "                del temp_dict[leaf]\n",
    "                new_cost = sum([v[\"h\"] for v in temp_dict.values()]) + alpha * len(temp_dict)\n",
    "                \n",
    "                if new_cost < current_cost:\n",
    "                    print(f\"🌳 Cost pruning: Removing leaf {leaf}, cost {current_cost:.4f} -> {new_cost:.4f}\")\n",
    "                    leaf_COFS_dict = temp_dict\n",
    "                    pruning_done = True\n",
    "                    break\n",
    "    \n",
    "    return tree, leaf_COFS_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd679f3e-abcc-4d43-929b-9e3530eaa30d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌳 h_min pruning: Removing leaf 30 with h=0.000000\n",
      "🌳 h_min pruning: Removing leaf 29 with h=0.000000\n",
      "🌳 h_min pruning: Removing leaf 70 with h=0.000000\n",
      "🌳 h_min pruning: Removing leaf 8 with h=0.000000\n",
      "🌳 h_min pruning: Removing leaf 52 with h=0.000000\n",
      "🌳 h_min pruning: Removing leaf 9 with h=0.000000\n",
      "🌳 h_min pruning: Removing leaf 53 with h=0.000000\n",
      "🌳 h_min pruning: Removing leaf 71 with h=0.000000\n",
      "🌳 h_min pruning: Removing leaf 10 with h=0.000000\n",
      "🌳 h_min pruning: Removing leaf 31 with h=0.000000\n",
      "🌳 h_min pruning: Removing leaf 50 with h=0.000000\n",
      "🌳 h_min pruning: Removing leaf 72 with h=0.000000\n",
      "🌳 h_min pruning: Removing leaf 14 with h=0.000000\n",
      "🌳 h_min pruning: Removing leaf 37 with h=0.000000\n",
      "🌳 h_min pruning: Removing leaf 38 with h=0.000000\n",
      "🌳 h_min pruning: Removing leaf 34 with h=0.000000\n",
      "🌳 h_min pruning: Removing leaf 13 with h=0.000000\n",
      "🌳 h_min pruning: Removing leaf 17 with h=0.000001\n",
      "🌳 h_min pruning: Removing leaf 16 with h=0.000001\n",
      "🌳 h_min pruning: Removing leaf 35 with h=0.000001\n",
      "🌳 h_min pruning: Removing leaf 75 with h=0.000001\n",
      "🌳 h_min pruning: Removing leaf 57 with h=0.000001\n",
      "🌳 h_min pruning: Removing leaf 76 with h=0.000001\n",
      "🌳 h_min pruning: Removing leaf 58 with h=0.000001\n",
      "🌳 h_min pruning: Removing leaf 55 with h=0.000001\n",
      "🌳 h_min pruning: Removing leaf 77 with h=0.000001\n",
      "🌳 h_min pruning: Removing leaf 114 with h=0.000002\n",
      "🌳 h_min pruning: Removing leaf 115 with h=0.000002\n",
      "🌳 h_min pruning: Removing leaf 192 with h=0.000002\n",
      "🌳 h_min pruning: Removing leaf 211 with h=0.000002\n",
      "🌳 h_min pruning: Removing leaf 212 with h=0.000002\n",
      "🌳 h_min pruning: Removing leaf 173 with h=0.000002\n",
      "🌳 h_min pruning: Removing leaf 235 with h=0.000002\n",
      "🌳 h_min pruning: Removing leaf 234 with h=0.000002\n",
      "🌳 h_min pruning: Removing leaf 191 with h=0.000002\n",
      "🌳 h_min pruning: Removing leaf 172 with h=0.000002\n",
      "🌳 h_min pruning: Removing leaf 110 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 153 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 132 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 135 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 94 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 151 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 93 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 134 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 226 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 343 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 91 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 112 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 154 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 90 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 131 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 150 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 345 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 298 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 249 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 230 with h=0.000003\n",
      "🌳 h_min pruning: Removing leaf 275 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 109 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 227 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 319 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 320 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 229 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 272 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 323 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 342 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 104 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 299 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 250 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 126 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 346 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 146 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 105 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 276 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 125 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 296 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 295 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 322 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 145 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 273 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 164 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 165 with h=0.000004\n",
      "🌳 h_min pruning: Removing leaf 308 with h=0.000006\n",
      "🌳 h_min pruning: Removing leaf 331 with h=0.000006\n",
      "🌳 h_min pruning: Removing leaf 330 with h=0.000006\n",
      "🌳 h_min pruning: Removing leaf 284 with h=0.000006\n",
      "🌳 h_min pruning: Removing leaf 187 with h=0.000006\n",
      "🌳 h_min pruning: Removing leaf 283 with h=0.000006\n",
      "🌳 h_min pruning: Removing leaf 203 with h=0.000006\n",
      "🌳 h_min pruning: Removing leaf 307 with h=0.000006\n",
      "🌳 h_min pruning: Removing leaf 184 with h=0.000006\n",
      "🌳 h_min pruning: Removing leaf 260 with h=0.000006\n",
      "🌳 h_min pruning: Removing leaf 261 with h=0.000006\n",
      "🌳 h_min pruning: Removing leaf 186 with h=0.000006\n",
      "🌳 h_min pruning: Removing leaf 222 with h=0.000006\n",
      "🌳 h_min pruning: Removing leaf 245 with h=0.000006\n",
      "🌳 h_min pruning: Removing leaf 205 with h=0.000006\n",
      "🌳 h_min pruning: Removing leaf 246 with h=0.000007\n",
      "🌳 h_min pruning: Removing leaf 202 with h=0.000007\n",
      "🌳 h_min pruning: Removing leaf 183 with h=0.000007\n",
      "🌳 h_min pruning: Removing leaf 251 with h=0.000007\n",
      "🌳 h_min pruning: Removing leaf 206 with h=0.000007\n",
      "🌳 h_min pruning: Removing leaf 223 with h=0.000007\n",
      "Final leaf count (h_min): 72\n",
      "h values: [np.float64(2.226124926936685), np.float64(2.3088202527794985), np.float64(2.596353363342454), np.float64(2.735179635755072), np.float64(2.3725805317135444), np.float64(2.197745703328578), np.float64(2.5011893792540203), np.float64(2.770638567942994), np.float64(2.2474744811695913), np.float64(2.2661582514294554), np.float64(2.7767573009217528), np.float64(2.430205752442469), np.float64(2.091590916306564), np.float64(2.288859072186388), np.float64(2.7728805368173), np.float64(2.6918078884664762), np.float64(0.9693471491480091), np.float64(1.0251668945210892), np.float64(0.9018571411584935), np.float64(1.0285352903226066), np.float64(1.0023695694091623), np.float64(0.9755781693909475), np.float64(0.9101196340406721), np.float64(1.0052560803048345), np.float64(0.8760534616619303), np.float64(0.9332258420836465), np.float64(0.9124116448846633), np.float64(1.0391700286326695), np.float64(1.052128195446812), np.float64(0.9406757171723887), np.float64(1.0500215686216194), np.float64(0.9594716063475927), np.float64(0.9835855603875546), np.float64(0.9333449923039145), np.float64(1.0181093777673929), np.float64(0.9680491087088114), np.float64(0.9429823776038704), np.float64(1.0757273286610194), np.float64(0.9958231317099959), np.float64(1.0594583385424028), np.float64(0.7961917778286686), np.float64(0.8953517531109938), np.float64(0.8387635421460296), np.float64(0.7799274766793814), np.float64(1.0884528269714489), np.float64(1.0415232910362406), np.float64(0.8318637323736081), np.float64(1.0780467777394045), np.float64(2.3769297198981905), np.float64(2.3833298408140036), np.float64(1.8418568496153163), np.float64(2.061550007891874), np.float64(1.7330590135262778), np.float64(1.8562328437880562), np.float64(2.2152589828745475), np.float64(2.299202992737377), np.float64(1.9220673304776201), np.float64(1.755175251198094), np.float64(1.773822336384418), np.float64(2.010643349297391), np.float64(2.3182291706585927), np.float64(2.4332741195387158), np.float64(1.7455831183147117), np.float64(2.044210453527981), np.float64(1.8210502098330794), np.float64(1.8342900346771227), np.float64(2.2770661369730782), np.float64(2.276370692257454), np.float64(1.8716649709546385), np.float64(1.8642778292261593), np.float64(1.9284361515991293), np.float64(1.95265618529436)]\n",
      "🌳 Cost pruning: Removing leaf 14, cost 327.1820 -> 312.3215\n",
      "🌳 Cost pruning: Removing leaf 19, cost 312.3215 -> 297.3796\n",
      "🌳 Cost pruning: Removing leaf 12, cost 297.3796 -> 282.3547\n",
      "🌳 Cost pruning: Removing leaf 20, cost 282.3547 -> 267.3066\n",
      "🌳 Cost pruning: Removing leaf 11, cost 267.3066 -> 252.1272\n",
      "🌳 Cost pruning: Removing leaf 22, cost 252.1272 -> 236.8929\n",
      "🌳 Cost pruning: Removing leaf 23, cost 236.8929 -> 221.5275\n",
      "🌳 Cost pruning: Removing leaf 15, cost 221.5275 -> 206.1099\n",
      "🌳 Cost pruning: Removing leaf 8, cost 206.1099 -> 181.8031\n",
      "🌳 Cost pruning: Removing leaf 7, cost 181.8031 -> 157.3674\n",
      "🌳 Cost pruning: Removing leaf 5, cost 157.3674 -> 132.6312\n",
      "🌳 Cost pruning: Removing leaf 4, cost 132.6312 -> 107.7802\n",
      "🌳 Cost pruning: Removing leaf 27, cost 107.7802 -> 81.2497\n",
      "🌳 Cost pruning: Removing leaf 26, cost 81.2497 -> 54.2053\n",
      "🌳 Cost pruning: Removing leaf 29, cost 54.2053 -> 27.1475\n",
      "🌳 Cost pruning: Removing leaf 30, cost 27.1475 -> 0.0000\n",
      "Final leaf count (cost): 0\n",
      "h values: []\n"
     ]
    }
   ],
   "source": [
    "# H_min pruning\n",
    "tree_h, leaves_h = train_and_prune_COF_tree(\n",
    "    X_train, y_train, initial_tree_params={\"min_samples_leaf\":1000}, \n",
    "    h_min=0.5, pruning_mode=\"hmin\"\n",
    ")\n",
    "\n",
    "print(f\"Final leaf count (h_min): {len(leaves_h)}\")\n",
    "print(\"h values:\", [v[\"h\"] for v in leaves_h.values()])\n",
    "\n",
    "# Cost-based pruning\n",
    "tree_c, leaves_c = train_and_prune_COF_tree(\n",
    "    X_train, y_train, initial_tree_params={\"min_samples_leaf\":10000}, \n",
    "    alpha=2, pruning_mode=\"cost\"\n",
    ")\n",
    "\n",
    "print(f\"Final leaf count (cost): {len(leaves_c)}\")\n",
    "print(\"h values:\", [v[\"h\"] for v in leaves_c.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5da99342-24fc-4254-b56d-5b29ef6e349a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 347\n",
      "\n",
      "Number of leaves: 174\n",
      "📌 Node 0: feature 1 ≤ 1.4658\n",
      "   Samples: 249999\n",
      "   Impurity (MSE): 0.4648\n",
      "   Value (mean target): 1.5064\n",
      "----------------------------------------\n",
      "📌 Node 1: feature 0 ≤ 1.1519\n",
      "   Samples: 122305\n",
      "   Impurity (MSE): 0.2945\n",
      "   Value (mean target): 1.1703\n",
      "----------------------------------------\n",
      "📌 Node 2: feature 2 ≤ 0.0015\n",
      "   Samples: 60571\n",
      "   Impurity (MSE): 0.2009\n",
      "   Value (mean target): 0.5968\n",
      "----------------------------------------\n",
      "📌 Node 3: feature 3 ≤ 0.0013\n",
      "   Samples: 30521\n",
      "   Impurity (MSE): 0.1513\n",
      "   Value (mean target): 0.5532\n",
      "----------------------------------------\n",
      "📌 Node 4: feature 1 ≤ 0.6804\n",
      "   Samples: 15303\n",
      "   Impurity (MSE): 0.1013\n",
      "   Value (mean target): 0.5543\n",
      "----------------------------------------\n",
      "📌 Node 5: feature 0 ≤ 0.5707\n",
      "   Samples: 8166\n",
      "   Impurity (MSE): 0.0710\n",
      "   Value (mean target): 0.5442\n",
      "----------------------------------------\n",
      "📌 Node 6: feature 3 ≤ -0.4880\n",
      "   Samples: 4011\n",
      "   Impurity (MSE): 0.0500\n",
      "   Value (mean target): 0.2491\n",
      "----------------------------------------\n",
      "📌 Node 7: feature 2 ≤ -0.4948\n",
      "   Samples: 2076\n",
      "   Impurity (MSE): 0.0381\n",
      "   Value (mean target): 0.2492\n",
      "----------------------------------------\n",
      "🌿 Leaf 8:\n",
      "   Samples: 1000\n",
      "   Impurity (MSE): 0.0251\n",
      "   Value (mean target): 0.2297\n",
      "----------------------------------------\n",
      "🌿 Leaf 9:\n",
      "   Samples: 1076\n",
      "   Impurity (MSE): 0.0254\n",
      "   Value (mean target): 0.2673\n",
      "----------------------------------------\n",
      "🌿 Leaf 10:\n",
      "   Samples: 1935\n",
      "   Impurity (MSE): 0.0365\n",
      "   Value (mean target): 0.2490\n",
      "----------------------------------------\n",
      "📌 Node 11: feature 2 ≤ -0.4968\n",
      "   Samples: 4155\n",
      "   Impurity (MSE): 0.0499\n",
      "   Value (mean target): 0.8291\n",
      "----------------------------------------\n",
      "📌 Node 12: feature 3 ≤ -0.4973\n",
      "   Samples: 2045\n",
      "   Impurity (MSE): 0.0372\n",
      "   Value (mean target): 0.8050\n",
      "----------------------------------------\n",
      "🌿 Leaf 13:\n",
      "   Samples: 1026\n",
      "   Impurity (MSE): 0.0251\n",
      "   Value (mean target): 0.8107\n",
      "----------------------------------------\n",
      "🌿 Leaf 14:\n",
      "   Samples: 1019\n",
      "   Impurity (MSE): 0.0248\n",
      "   Value (mean target): 0.7992\n",
      "----------------------------------------\n",
      "📌 Node 15: feature 3 ≤ -0.4912\n",
      "   Samples: 2110\n",
      "   Impurity (MSE): 0.0377\n",
      "   Value (mean target): 0.8524\n",
      "----------------------------------------\n",
      "🌿 Leaf 16:\n",
      "   Samples: 1061\n",
      "   Impurity (MSE): 0.0255\n",
      "   Value (mean target): 0.8478\n",
      "----------------------------------------\n",
      "🌿 Leaf 17:\n",
      "   Samples: 1049\n",
      "   Impurity (MSE): 0.0250\n",
      "   Value (mean target): 0.8570\n",
      "----------------------------------------\n",
      "📌 Node 18: feature 0 ≤ 0.5873\n",
      "   Samples: 7137\n",
      "   Impurity (MSE): 0.0762\n",
      "   Value (mean target): 0.5659\n",
      "----------------------------------------\n",
      "📌 Node 19: feature 3 ≤ -0.5184\n",
      "   Samples: 3420\n",
      "   Impurity (MSE): 0.0531\n",
      "   Value (mean target): 0.2517\n",
      "----------------------------------------\n",
      "🌿 Leaf 20:\n",
      "   Samples: 1666\n",
      "   Impurity (MSE): 0.0408\n",
      "   Value (mean target): 0.2484\n",
      "----------------------------------------\n",
      "🌿 Leaf 21:\n",
      "   Samples: 1754\n",
      "   Impurity (MSE): 0.0403\n",
      "   Value (mean target): 0.2549\n",
      "----------------------------------------\n",
      "📌 Node 22: feature 3 ≤ -0.5020\n",
      "   Samples: 3717\n",
      "   Impurity (MSE): 0.0539\n",
      "   Value (mean target): 0.8549\n",
      "----------------------------------------\n",
      "🌿 Leaf 23:\n",
      "   Samples: 1844\n",
      "   Impurity (MSE): 0.0405\n",
      "   Value (mean target): 0.8568\n",
      "----------------------------------------\n",
      "🌿 Leaf 24:\n",
      "   Samples: 1873\n",
      "   Impurity (MSE): 0.0416\n",
      "   Value (mean target): 0.8530\n",
      "----------------------------------------\n",
      "📌 Node 25: feature 1 ≤ 0.6792\n",
      "   Samples: 15218\n",
      "   Impurity (MSE): 0.1016\n",
      "   Value (mean target): 0.5521\n",
      "----------------------------------------\n",
      "📌 Node 26: feature 0 ≤ 0.5685\n",
      "   Samples: 8104\n",
      "   Impurity (MSE): 0.0700\n",
      "   Value (mean target): 0.5446\n",
      "----------------------------------------\n",
      "📌 Node 27: feature 3 ≤ 0.5076\n",
      "   Samples: 3963\n",
      "   Impurity (MSE): 0.0489\n",
      "   Value (mean target): 0.2500\n",
      "----------------------------------------\n",
      "📌 Node 28: feature 2 ≤ -0.5023\n",
      "   Samples: 2009\n",
      "   Impurity (MSE): 0.0360\n",
      "   Value (mean target): 0.2518\n",
      "----------------------------------------\n",
      "🌿 Leaf 29:\n",
      "   Samples: 1009\n",
      "   Impurity (MSE): 0.0243\n",
      "   Value (mean target): 0.2272\n",
      "----------------------------------------\n",
      "🌿 Leaf 30:\n",
      "   Samples: 1000\n",
      "   Impurity (MSE): 0.0249\n",
      "   Value (mean target): 0.2766\n",
      "----------------------------------------\n",
      "🌿 Leaf 31:\n",
      "   Samples: 1954\n",
      "   Impurity (MSE): 0.0365\n",
      "   Value (mean target): 0.2481\n",
      "----------------------------------------\n",
      "📌 Node 32: feature 3 ≤ 0.4978\n",
      "   Samples: 4141\n",
      "   Impurity (MSE): 0.0495\n",
      "   Value (mean target): 0.8266\n",
      "----------------------------------------\n",
      "📌 Node 33: feature 2 ≤ -0.4943\n",
      "   Samples: 2136\n",
      "   Impurity (MSE): 0.0368\n",
      "   Value (mean target): 0.8290\n",
      "----------------------------------------\n",
      "🌿 Leaf 34:\n",
      "   Samples: 1070\n",
      "   Impurity (MSE): 0.0245\n",
      "   Value (mean target): 0.8064\n",
      "----------------------------------------\n",
      "🌿 Leaf 35:\n",
      "   Samples: 1066\n",
      "   Impurity (MSE): 0.0252\n",
      "   Value (mean target): 0.8518\n",
      "----------------------------------------\n",
      "📌 Node 36: feature 2 ≤ -0.4906\n",
      "   Samples: 2005\n",
      "   Impurity (MSE): 0.0374\n",
      "   Value (mean target): 0.8239\n",
      "----------------------------------------\n",
      "🌿 Leaf 37:\n",
      "   Samples: 1001\n",
      "   Impurity (MSE): 0.0251\n",
      "   Value (mean target): 0.8055\n",
      "----------------------------------------\n",
      "🌿 Leaf 38:\n",
      "   Samples: 1004\n",
      "   Impurity (MSE): 0.0246\n",
      "   Value (mean target): 0.8423\n",
      "----------------------------------------\n",
      "📌 Node 39: feature 0 ≤ 0.5916\n",
      "   Samples: 7114\n",
      "   Impurity (MSE): 0.0766\n",
      "   Value (mean target): 0.5607\n",
      "----------------------------------------\n",
      "📌 Node 40: feature 2 ≤ -0.4812\n",
      "   Samples: 3521\n",
      "   Impurity (MSE): 0.0541\n",
      "   Value (mean target): 0.2584\n",
      "----------------------------------------\n",
      "🌿 Leaf 41:\n",
      "   Samples: 1859\n",
      "   Impurity (MSE): 0.0416\n",
      "   Value (mean target): 0.2368\n",
      "----------------------------------------\n",
      "🌿 Leaf 42:\n",
      "   Samples: 1662\n",
      "   Impurity (MSE): 0.0408\n",
      "   Value (mean target): 0.2826\n",
      "----------------------------------------\n",
      "📌 Node 43: feature 2 ≤ -0.5282\n",
      "   Samples: 3593\n",
      "   Impurity (MSE): 0.0542\n",
      "   Value (mean target): 0.8568\n",
      "----------------------------------------\n",
      "🌿 Leaf 44:\n",
      "   Samples: 1673\n",
      "   Impurity (MSE): 0.0400\n",
      "   Value (mean target): 0.8285\n",
      "----------------------------------------\n",
      "🌿 Leaf 45:\n",
      "   Samples: 1920\n",
      "   Impurity (MSE): 0.0423\n",
      "   Value (mean target): 0.8814\n",
      "----------------------------------------\n",
      "📌 Node 46: feature 3 ≤ -0.0066\n",
      "   Samples: 30050\n",
      "   Impurity (MSE): 0.1508\n",
      "   Value (mean target): 0.6411\n",
      "----------------------------------------\n",
      "📌 Node 47: feature 1 ≤ 0.6794\n",
      "   Samples: 15018\n",
      "   Impurity (MSE): 0.1016\n",
      "   Value (mean target): 0.6416\n",
      "----------------------------------------\n",
      "📌 Node 48: feature 0 ≤ 0.5883\n",
      "   Samples: 8002\n",
      "   Impurity (MSE): 0.0706\n",
      "   Value (mean target): 0.6358\n",
      "----------------------------------------\n",
      "📌 Node 49: feature 3 ≤ -0.5044\n",
      "   Samples: 4024\n",
      "   Impurity (MSE): 0.0495\n",
      "   Value (mean target): 0.3459\n",
      "----------------------------------------\n",
      "🌿 Leaf 50:\n",
      "   Samples: 1983\n",
      "   Impurity (MSE): 0.0368\n",
      "   Value (mean target): 0.3447\n",
      "----------------------------------------\n",
      "📌 Node 51: feature 2 ≤ 0.5012\n",
      "   Samples: 2041\n",
      "   Impurity (MSE): 0.0370\n",
      "   Value (mean target): 0.3470\n",
      "----------------------------------------\n",
      "🌿 Leaf 52:\n",
      "   Samples: 1014\n",
      "   Impurity (MSE): 0.0243\n",
      "   Value (mean target): 0.3237\n",
      "----------------------------------------\n",
      "🌿 Leaf 53:\n",
      "   Samples: 1027\n",
      "   Impurity (MSE): 0.0247\n",
      "   Value (mean target): 0.3700\n",
      "----------------------------------------\n",
      "📌 Node 54: feature 2 ≤ 0.4932\n",
      "   Samples: 3978\n",
      "   Impurity (MSE): 0.0491\n",
      "   Value (mean target): 0.9292\n",
      "----------------------------------------\n",
      "🌿 Leaf 55:\n",
      "   Samples: 1917\n",
      "   Impurity (MSE): 0.0368\n",
      "   Value (mean target): 0.9053\n",
      "----------------------------------------\n",
      "📌 Node 56: feature 3 ≤ -0.5086\n",
      "   Samples: 2061\n",
      "   Impurity (MSE): 0.0366\n",
      "   Value (mean target): 0.9513\n",
      "----------------------------------------\n",
      "🌿 Leaf 57:\n",
      "   Samples: 1001\n",
      "   Impurity (MSE): 0.0239\n",
      "   Value (mean target): 0.9450\n",
      "----------------------------------------\n",
      "🌿 Leaf 58:\n",
      "   Samples: 1060\n",
      "   Impurity (MSE): 0.0245\n",
      "   Value (mean target): 0.9573\n",
      "----------------------------------------\n",
      "📌 Node 59: feature 0 ≤ 0.5919\n",
      "   Samples: 7016\n",
      "   Impurity (MSE): 0.0762\n",
      "   Value (mean target): 0.6483\n",
      "----------------------------------------\n",
      "📌 Node 60: feature 3 ≤ -0.4998\n",
      "   Samples: 3476\n",
      "   Impurity (MSE): 0.0537\n",
      "   Value (mean target): 0.3458\n",
      "----------------------------------------\n",
      "🌿 Leaf 61:\n",
      "   Samples: 1746\n",
      "   Impurity (MSE): 0.0412\n",
      "   Value (mean target): 0.3390\n",
      "----------------------------------------\n",
      "🌿 Leaf 62:\n",
      "   Samples: 1730\n",
      "   Impurity (MSE): 0.0411\n",
      "   Value (mean target): 0.3527\n",
      "----------------------------------------\n",
      "📌 Node 63: feature 2 ≤ 0.5365\n",
      "   Samples: 3540\n",
      "   Impurity (MSE): 0.0536\n",
      "   Value (mean target): 0.9452\n",
      "----------------------------------------\n",
      "🌿 Leaf 64:\n",
      "   Samples: 1874\n",
      "   Impurity (MSE): 0.0415\n",
      "   Value (mean target): 0.9249\n",
      "----------------------------------------\n",
      "🌿 Leaf 65:\n",
      "   Samples: 1666\n",
      "   Impurity (MSE): 0.0409\n",
      "   Value (mean target): 0.9682\n",
      "----------------------------------------\n",
      "📌 Node 66: feature 1 ≤ 0.6664\n",
      "   Samples: 15032\n",
      "   Impurity (MSE): 0.1009\n",
      "   Value (mean target): 0.6405\n",
      "----------------------------------------\n",
      "📌 Node 67: feature 0 ≤ 0.5759\n",
      "   Samples: 8031\n",
      "   Impurity (MSE): 0.0707\n",
      "   Value (mean target): 0.6293\n",
      "----------------------------------------\n",
      "📌 Node 68: feature 3 ≤ 0.4997\n",
      "   Samples: 4022\n",
      "   Impurity (MSE): 0.0494\n",
      "   Value (mean target): 0.3385\n",
      "----------------------------------------\n",
      "📌 Node 69: feature 2 ≤ 0.5156\n",
      "   Samples: 2058\n",
      "   Impurity (MSE): 0.0365\n",
      "   Value (mean target): 0.3432\n",
      "----------------------------------------\n",
      "🌿 Leaf 70:\n",
      "   Samples: 1000\n",
      "   Impurity (MSE): 0.0248\n",
      "   Value (mean target): 0.3180\n",
      "----------------------------------------\n",
      "🌿 Leaf 71:\n",
      "   Samples: 1058\n",
      "   Impurity (MSE): 0.0237\n",
      "   Value (mean target): 0.3671\n",
      "----------------------------------------\n",
      "🌿 Leaf 72:\n",
      "   Samples: 1964\n",
      "   Impurity (MSE): 0.0372\n",
      "   Value (mean target): 0.3335\n",
      "----------------------------------------\n",
      "📌 Node 73: feature 3 ≤ 0.4995\n",
      "   Samples: 4009\n",
      "   Impurity (MSE): 0.0495\n",
      "   Value (mean target): 0.9210\n",
      "----------------------------------------\n",
      "📌 Node 74: feature 2 ≤ 0.5194\n",
      "   Samples: 2038\n",
      "   Impurity (MSE): 0.0367\n",
      "   Value (mean target): 0.9242\n",
      "----------------------------------------\n",
      "🌿 Leaf 75:\n",
      "   Samples: 1000\n",
      "   Impurity (MSE): 0.0246\n",
      "   Value (mean target): 0.9082\n",
      "----------------------------------------\n",
      "🌿 Leaf 76:\n",
      "   Samples: 1038\n",
      "   Impurity (MSE): 0.0243\n",
      "   Value (mean target): 0.9396\n",
      "----------------------------------------\n",
      "🌿 Leaf 77:\n",
      "   Samples: 1971\n",
      "   Impurity (MSE): 0.0370\n",
      "   Value (mean target): 0.9177\n",
      "----------------------------------------\n",
      "📌 Node 78: feature 0 ≤ 0.5868\n",
      "   Samples: 7001\n",
      "   Impurity (MSE): 0.0755\n",
      "   Value (mean target): 0.6533\n",
      "----------------------------------------\n",
      "📌 Node 79: feature 3 ≤ 0.4918\n",
      "   Samples: 3336\n",
      "   Impurity (MSE): 0.0528\n",
      "   Value (mean target): 0.3412\n",
      "----------------------------------------\n",
      "🌿 Leaf 80:\n",
      "   Samples: 1591\n",
      "   Impurity (MSE): 0.0403\n",
      "   Value (mean target): 0.3386\n",
      "----------------------------------------\n",
      "🌿 Leaf 81:\n",
      "   Samples: 1745\n",
      "   Impurity (MSE): 0.0406\n",
      "   Value (mean target): 0.3435\n",
      "----------------------------------------\n",
      "📌 Node 82: feature 3 ≤ 0.5017\n",
      "   Samples: 3665\n",
      "   Impurity (MSE): 0.0538\n",
      "   Value (mean target): 0.9375\n",
      "----------------------------------------\n",
      "🌿 Leaf 83:\n",
      "   Samples: 1849\n",
      "   Impurity (MSE): 0.0409\n",
      "   Value (mean target): 0.9358\n",
      "----------------------------------------\n",
      "🌿 Leaf 84:\n",
      "   Samples: 1816\n",
      "   Impurity (MSE): 0.0412\n",
      "   Value (mean target): 0.9391\n",
      "----------------------------------------\n",
      "📌 Node 85: feature 3 ≤ -0.0009\n",
      "   Samples: 61734\n",
      "   Impurity (MSE): 0.2233\n",
      "   Value (mean target): 1.7331\n",
      "----------------------------------------\n",
      "📌 Node 86: feature 2 ≤ -0.0019\n",
      "   Samples: 30966\n",
      "   Impurity (MSE): 0.1723\n",
      "   Value (mean target): 1.7303\n",
      "----------------------------------------\n",
      "📌 Node 87: feature 1 ≤ 0.8630\n",
      "   Samples: 15386\n",
      "   Impurity (MSE): 0.1227\n",
      "   Value (mean target): 1.6885\n",
      "----------------------------------------\n",
      "📌 Node 88: feature 3 ≤ -0.4995\n",
      "   Samples: 7641\n",
      "   Impurity (MSE): 0.0630\n",
      "   Value (mean target): 1.5436\n",
      "----------------------------------------\n",
      "📌 Node 89: feature 2 ≤ -0.4922\n",
      "   Samples: 3898\n",
      "   Impurity (MSE): 0.0503\n",
      "   Value (mean target): 1.5425\n",
      "----------------------------------------\n",
      "🌿 Leaf 90:\n",
      "   Samples: 1994\n",
      "   Impurity (MSE): 0.0387\n",
      "   Value (mean target): 1.5215\n",
      "----------------------------------------\n",
      "🌿 Leaf 91:\n",
      "   Samples: 1904\n",
      "   Impurity (MSE): 0.0374\n",
      "   Value (mean target): 1.5644\n",
      "----------------------------------------\n",
      "📌 Node 92: feature 2 ≤ -0.4974\n",
      "   Samples: 3743\n",
      "   Impurity (MSE): 0.0506\n",
      "   Value (mean target): 1.5447\n",
      "----------------------------------------\n",
      "🌿 Leaf 93:\n",
      "   Samples: 1815\n",
      "   Impurity (MSE): 0.0376\n",
      "   Value (mean target): 1.5256\n",
      "----------------------------------------\n",
      "🌿 Leaf 94:\n",
      "   Samples: 1928\n",
      "   Impurity (MSE): 0.0384\n",
      "   Value (mean target): 1.5627\n",
      "----------------------------------------\n",
      "📌 Node 95: feature 0 ≤ 2.0497\n",
      "   Samples: 7745\n",
      "   Impurity (MSE): 0.1016\n",
      "   Value (mean target): 1.8314\n",
      "----------------------------------------\n",
      "📌 Node 96: feature 3 ≤ -0.5118\n",
      "   Samples: 5418\n",
      "   Impurity (MSE): 0.0557\n",
      "   Value (mean target): 1.5528\n",
      "----------------------------------------\n",
      "📌 Node 97: feature 2 ≤ -0.5162\n",
      "   Samples: 2736\n",
      "   Impurity (MSE): 0.0434\n",
      "   Value (mean target): 1.5530\n",
      "----------------------------------------\n",
      "🌿 Leaf 98:\n",
      "   Samples: 1323\n",
      "   Impurity (MSE): 0.0308\n",
      "   Value (mean target): 1.5287\n",
      "----------------------------------------\n",
      "🌿 Leaf 99:\n",
      "   Samples: 1413\n",
      "   Impurity (MSE): 0.0315\n",
      "   Value (mean target): 1.5757\n",
      "----------------------------------------\n",
      "📌 Node 100: feature 2 ≤ -0.5181\n",
      "   Samples: 2682\n",
      "   Impurity (MSE): 0.0433\n",
      "   Value (mean target): 1.5527\n",
      "----------------------------------------\n",
      "🌿 Leaf 101:\n",
      "   Samples: 1283\n",
      "   Impurity (MSE): 0.0311\n",
      "   Value (mean target): 1.5254\n",
      "----------------------------------------\n",
      "🌿 Leaf 102:\n",
      "   Samples: 1399\n",
      "   Impurity (MSE): 0.0315\n",
      "   Value (mean target): 1.5777\n",
      "----------------------------------------\n",
      "📌 Node 103: feature 0 ≤ 2.5272\n",
      "   Samples: 2327\n",
      "   Impurity (MSE): 0.0564\n",
      "   Value (mean target): 2.4800\n",
      "----------------------------------------\n",
      "🌿 Leaf 104:\n",
      "   Samples: 1172\n",
      "   Impurity (MSE): 0.0424\n",
      "   Value (mean target): 2.2445\n",
      "----------------------------------------\n",
      "🌿 Leaf 105:\n",
      "   Samples: 1155\n",
      "   Impurity (MSE): 0.0422\n",
      "   Value (mean target): 2.7189\n",
      "----------------------------------------\n",
      "📌 Node 106: feature 1 ≤ 0.8659\n",
      "   Samples: 15580\n",
      "   Impurity (MSE): 0.1231\n",
      "   Value (mean target): 1.7717\n",
      "----------------------------------------\n",
      "📌 Node 107: feature 2 ≤ 0.4890\n",
      "   Samples: 7799\n",
      "   Impurity (MSE): 0.0634\n",
      "   Value (mean target): 1.6292\n",
      "----------------------------------------\n",
      "📌 Node 108: feature 3 ≤ -0.4896\n",
      "   Samples: 3811\n",
      "   Impurity (MSE): 0.0502\n",
      "   Value (mean target): 1.6062\n",
      "----------------------------------------\n",
      "🌿 Leaf 109:\n",
      "   Samples: 1988\n",
      "   Impurity (MSE): 0.0383\n",
      "   Value (mean target): 1.6020\n",
      "----------------------------------------\n",
      "🌿 Leaf 110:\n",
      "   Samples: 1823\n",
      "   Impurity (MSE): 0.0373\n",
      "   Value (mean target): 1.6109\n",
      "----------------------------------------\n",
      "📌 Node 111: feature 3 ≤ -0.5080\n",
      "   Samples: 3988\n",
      "   Impurity (MSE): 0.0516\n",
      "   Value (mean target): 1.6512\n",
      "----------------------------------------\n",
      "🌿 Leaf 112:\n",
      "   Samples: 1962\n",
      "   Impurity (MSE): 0.0394\n",
      "   Value (mean target): 1.6628\n",
      "----------------------------------------\n",
      "📌 Node 113: feature 1 ≤ 0.4340\n",
      "   Samples: 2026\n",
      "   Impurity (MSE): 0.0392\n",
      "   Value (mean target): 1.6400\n",
      "----------------------------------------\n",
      "🌿 Leaf 114:\n",
      "   Samples: 1012\n",
      "   Impurity (MSE): 0.0276\n",
      "   Value (mean target): 1.6426\n",
      "----------------------------------------\n",
      "🌿 Leaf 115:\n",
      "   Samples: 1014\n",
      "   Impurity (MSE): 0.0275\n",
      "   Value (mean target): 1.6374\n",
      "----------------------------------------\n",
      "📌 Node 116: feature 0 ≤ 2.0514\n",
      "   Samples: 7781\n",
      "   Impurity (MSE): 0.1021\n",
      "   Value (mean target): 1.9145\n",
      "----------------------------------------\n",
      "📌 Node 117: feature 2 ≤ 0.5062\n",
      "   Samples: 5456\n",
      "   Impurity (MSE): 0.0557\n",
      "   Value (mean target): 1.6354\n",
      "----------------------------------------\n",
      "📌 Node 118: feature 3 ≤ -0.5078\n",
      "   Samples: 2779\n",
      "   Impurity (MSE): 0.0431\n",
      "   Value (mean target): 1.6153\n",
      "----------------------------------------\n",
      "🌿 Leaf 119:\n",
      "   Samples: 1393\n",
      "   Impurity (MSE): 0.0310\n",
      "   Value (mean target): 1.6290\n",
      "----------------------------------------\n",
      "🌿 Leaf 120:\n",
      "   Samples: 1386\n",
      "   Impurity (MSE): 0.0313\n",
      "   Value (mean target): 1.6014\n",
      "----------------------------------------\n",
      "📌 Node 121: feature 3 ≤ -0.5275\n",
      "   Samples: 2677\n",
      "   Impurity (MSE): 0.0430\n",
      "   Value (mean target): 1.6564\n",
      "----------------------------------------\n",
      "🌿 Leaf 122:\n",
      "   Samples: 1267\n",
      "   Impurity (MSE): 0.0303\n",
      "   Value (mean target): 1.6547\n",
      "----------------------------------------\n",
      "🌿 Leaf 123:\n",
      "   Samples: 1410\n",
      "   Impurity (MSE): 0.0316\n",
      "   Value (mean target): 1.6579\n",
      "----------------------------------------\n",
      "📌 Node 124: feature 0 ≤ 2.5156\n",
      "   Samples: 2325\n",
      "   Impurity (MSE): 0.0560\n",
      "   Value (mean target): 2.5694\n",
      "----------------------------------------\n",
      "🌿 Leaf 125:\n",
      "   Samples: 1137\n",
      "   Impurity (MSE): 0.0419\n",
      "   Value (mean target): 2.3308\n",
      "----------------------------------------\n",
      "🌿 Leaf 126:\n",
      "   Samples: 1188\n",
      "   Impurity (MSE): 0.0427\n",
      "   Value (mean target): 2.7978\n",
      "----------------------------------------\n",
      "📌 Node 127: feature 2 ≤ -0.0142\n",
      "   Samples: 30768\n",
      "   Impurity (MSE): 0.1740\n",
      "   Value (mean target): 1.7359\n",
      "----------------------------------------\n",
      "📌 Node 128: feature 1 ≤ 0.8716\n",
      "   Samples: 15212\n",
      "   Impurity (MSE): 0.1239\n",
      "   Value (mean target): 1.6887\n",
      "----------------------------------------\n",
      "📌 Node 129: feature 3 ≤ 0.4995\n",
      "   Samples: 7639\n",
      "   Impurity (MSE): 0.0634\n",
      "   Value (mean target): 1.5454\n",
      "----------------------------------------\n",
      "📌 Node 130: feature 2 ≤ -0.4963\n",
      "   Samples: 3776\n",
      "   Impurity (MSE): 0.0511\n",
      "   Value (mean target): 1.5402\n",
      "----------------------------------------\n",
      "🌿 Leaf 131:\n",
      "   Samples: 1974\n",
      "   Impurity (MSE): 0.0391\n",
      "   Value (mean target): 1.5154\n",
      "----------------------------------------\n",
      "🌿 Leaf 132:\n",
      "   Samples: 1802\n",
      "   Impurity (MSE): 0.0389\n",
      "   Value (mean target): 1.5674\n",
      "----------------------------------------\n",
      "📌 Node 133: feature 1 ≤ 0.4367\n",
      "   Samples: 3863\n",
      "   Impurity (MSE): 0.0505\n",
      "   Value (mean target): 1.5505\n",
      "----------------------------------------\n",
      "🌿 Leaf 134:\n",
      "   Samples: 1905\n",
      "   Impurity (MSE): 0.0387\n",
      "   Value (mean target): 1.5559\n",
      "----------------------------------------\n",
      "🌿 Leaf 135:\n",
      "   Samples: 1958\n",
      "   Impurity (MSE): 0.0386\n",
      "   Value (mean target): 1.5452\n",
      "----------------------------------------\n",
      "📌 Node 136: feature 0 ≤ 2.0555\n",
      "   Samples: 7573\n",
      "   Impurity (MSE): 0.1030\n",
      "   Value (mean target): 1.8332\n",
      "----------------------------------------\n",
      "📌 Node 137: feature 3 ≤ 0.4998\n",
      "   Samples: 5277\n",
      "   Impurity (MSE): 0.0557\n",
      "   Value (mean target): 1.5484\n",
      "----------------------------------------\n",
      "📌 Node 138: feature 2 ≤ -0.5232\n",
      "   Samples: 2617\n",
      "   Impurity (MSE): 0.0440\n",
      "   Value (mean target): 1.5455\n",
      "----------------------------------------\n",
      "🌿 Leaf 139:\n",
      "   Samples: 1264\n",
      "   Impurity (MSE): 0.0313\n",
      "   Value (mean target): 1.5184\n",
      "----------------------------------------\n",
      "🌿 Leaf 140:\n",
      "   Samples: 1353\n",
      "   Impurity (MSE): 0.0322\n",
      "   Value (mean target): 1.5707\n",
      "----------------------------------------\n",
      "📌 Node 141: feature 2 ≤ -0.5155\n",
      "   Samples: 2660\n",
      "   Impurity (MSE): 0.0431\n",
      "   Value (mean target): 1.5513\n",
      "----------------------------------------\n",
      "🌿 Leaf 142:\n",
      "   Samples: 1259\n",
      "   Impurity (MSE): 0.0314\n",
      "   Value (mean target): 1.5245\n",
      "----------------------------------------\n",
      "🌿 Leaf 143:\n",
      "   Samples: 1401\n",
      "   Impurity (MSE): 0.0312\n",
      "   Value (mean target): 1.5754\n",
      "----------------------------------------\n",
      "📌 Node 144: feature 0 ≤ 2.5375\n",
      "   Samples: 2296\n",
      "   Impurity (MSE): 0.0562\n",
      "   Value (mean target): 2.4877\n",
      "----------------------------------------\n",
      "🌿 Leaf 145:\n",
      "   Samples: 1154\n",
      "   Impurity (MSE): 0.0416\n",
      "   Value (mean target): 2.2491\n",
      "----------------------------------------\n",
      "🌿 Leaf 146:\n",
      "   Samples: 1142\n",
      "   Impurity (MSE): 0.0421\n",
      "   Value (mean target): 2.7288\n",
      "----------------------------------------\n",
      "📌 Node 147: feature 1 ≤ 0.8827\n",
      "   Samples: 15556\n",
      "   Impurity (MSE): 0.1251\n",
      "   Value (mean target): 1.7820\n",
      "----------------------------------------\n",
      "📌 Node 148: feature 2 ≤ 0.5004\n",
      "   Samples: 7777\n",
      "   Impurity (MSE): 0.0648\n",
      "   Value (mean target): 1.6331\n",
      "----------------------------------------\n",
      "📌 Node 149: feature 3 ≤ 0.4999\n",
      "   Samples: 3929\n",
      "   Impurity (MSE): 0.0524\n",
      "   Value (mean target): 1.6090\n",
      "----------------------------------------\n",
      "🌿 Leaf 150:\n",
      "   Samples: 1937\n",
      "   Impurity (MSE): 0.0400\n",
      "   Value (mean target): 1.6117\n",
      "----------------------------------------\n",
      "🌿 Leaf 151:\n",
      "   Samples: 1992\n",
      "   Impurity (MSE): 0.0401\n",
      "   Value (mean target): 1.6064\n",
      "----------------------------------------\n",
      "📌 Node 152: feature 1 ≤ 0.4228\n",
      "   Samples: 3848\n",
      "   Impurity (MSE): 0.0519\n",
      "   Value (mean target): 1.6577\n",
      "----------------------------------------\n",
      "🌿 Leaf 153:\n",
      "   Samples: 1864\n",
      "   Impurity (MSE): 0.0391\n",
      "   Value (mean target): 1.6561\n",
      "----------------------------------------\n",
      "🌿 Leaf 154:\n",
      "   Samples: 1984\n",
      "   Impurity (MSE): 0.0401\n",
      "   Value (mean target): 1.6592\n",
      "----------------------------------------\n",
      "📌 Node 155: feature 0 ≤ 2.0615\n",
      "   Samples: 7779\n",
      "   Impurity (MSE): 0.1036\n",
      "   Value (mean target): 1.9309\n",
      "----------------------------------------\n",
      "📌 Node 156: feature 3 ≤ 0.5096\n",
      "   Samples: 5365\n",
      "   Impurity (MSE): 0.0570\n",
      "   Value (mean target): 1.6419\n",
      "----------------------------------------\n",
      "📌 Node 157: feature 2 ≤ 0.4924\n",
      "   Samples: 2789\n",
      "   Impurity (MSE): 0.0444\n",
      "   Value (mean target): 1.6395\n",
      "----------------------------------------\n",
      "🌿 Leaf 158:\n",
      "   Samples: 1409\n",
      "   Impurity (MSE): 0.0321\n",
      "   Value (mean target): 1.6127\n",
      "----------------------------------------\n",
      "🌿 Leaf 159:\n",
      "   Samples: 1380\n",
      "   Impurity (MSE): 0.0312\n",
      "   Value (mean target): 1.6668\n",
      "----------------------------------------\n",
      "📌 Node 160: feature 2 ≤ 0.4927\n",
      "   Samples: 2576\n",
      "   Impurity (MSE): 0.0442\n",
      "   Value (mean target): 1.6446\n",
      "----------------------------------------\n",
      "🌿 Leaf 161:\n",
      "   Samples: 1329\n",
      "   Impurity (MSE): 0.0312\n",
      "   Value (mean target): 1.6273\n",
      "----------------------------------------\n",
      "🌿 Leaf 162:\n",
      "   Samples: 1247\n",
      "   Impurity (MSE): 0.0324\n",
      "   Value (mean target): 1.6631\n",
      "----------------------------------------\n",
      "📌 Node 163: feature 0 ≤ 2.5244\n",
      "   Samples: 2414\n",
      "   Impurity (MSE): 0.0556\n",
      "   Value (mean target): 2.5732\n",
      "----------------------------------------\n",
      "🌿 Leaf 164:\n",
      "   Samples: 1186\n",
      "   Impurity (MSE): 0.0417\n",
      "   Value (mean target): 2.3375\n",
      "----------------------------------------\n",
      "🌿 Leaf 165:\n",
      "   Samples: 1228\n",
      "   Impurity (MSE): 0.0426\n",
      "   Value (mean target): 2.8009\n",
      "----------------------------------------\n",
      "📌 Node 166: feature 0 ≤ 1.8146\n",
      "   Samples: 127694\n",
      "   Impurity (MSE): 0.3013\n",
      "   Value (mean target): 1.8282\n",
      "----------------------------------------\n",
      "📌 Node 167: feature 3 ≤ -0.0097\n",
      "   Samples: 63192\n",
      "   Impurity (MSE): 0.2299\n",
      "   Value (mean target): 1.2536\n",
      "----------------------------------------\n",
      "📌 Node 168: feature 2 ≤ -0.0031\n",
      "   Samples: 31201\n",
      "   Impurity (MSE): 0.1805\n",
      "   Value (mean target): 1.2488\n",
      "----------------------------------------\n",
      "📌 Node 169: feature 1 ≤ 2.0825\n",
      "   Samples: 15636\n",
      "   Impurity (MSE): 0.1320\n",
      "   Value (mean target): 1.2089\n",
      "----------------------------------------\n",
      "📌 Node 170: feature 0 ≤ 0.9344\n",
      "   Samples: 8013\n",
      "   Impurity (MSE): 0.1046\n",
      "   Value (mean target): 1.0493\n",
      "----------------------------------------\n",
      "📌 Node 171: feature 0 ≤ 0.4653\n",
      "   Samples: 2607\n",
      "   Impurity (MSE): 0.0566\n",
      "   Value (mean target): 0.4219\n",
      "----------------------------------------\n",
      "🌿 Leaf 172:\n",
      "   Samples: 1302\n",
      "   Impurity (MSE): 0.0428\n",
      "   Value (mean target): 0.1901\n",
      "----------------------------------------\n",
      "🌿 Leaf 173:\n",
      "   Samples: 1305\n",
      "   Impurity (MSE): 0.0436\n",
      "   Value (mean target): 0.6532\n",
      "----------------------------------------\n",
      "📌 Node 174: feature 2 ≤ -0.5047\n",
      "   Samples: 5406\n",
      "   Impurity (MSE): 0.0566\n",
      "   Value (mean target): 1.3518\n",
      "----------------------------------------\n",
      "📌 Node 175: feature 3 ≤ -0.5113\n",
      "   Samples: 2701\n",
      "   Impurity (MSE): 0.0434\n",
      "   Value (mean target): 1.3250\n",
      "----------------------------------------\n",
      "🌿 Leaf 176:\n",
      "   Samples: 1343\n",
      "   Impurity (MSE): 0.0314\n",
      "   Value (mean target): 1.3334\n",
      "----------------------------------------\n",
      "🌿 Leaf 177:\n",
      "   Samples: 1358\n",
      "   Impurity (MSE): 0.0310\n",
      "   Value (mean target): 1.3167\n",
      "----------------------------------------\n",
      "📌 Node 178: feature 3 ≤ -0.4986\n",
      "   Samples: 2705\n",
      "   Impurity (MSE): 0.0447\n",
      "   Value (mean target): 1.3786\n",
      "----------------------------------------\n",
      "🌿 Leaf 179:\n",
      "   Samples: 1349\n",
      "   Impurity (MSE): 0.0332\n",
      "   Value (mean target): 1.3728\n",
      "----------------------------------------\n",
      "🌿 Leaf 180:\n",
      "   Samples: 1356\n",
      "   Impurity (MSE): 0.0309\n",
      "   Value (mean target): 1.3844\n",
      "----------------------------------------\n",
      "📌 Node 181: feature 1 ≤ 2.5460\n",
      "   Samples: 7623\n",
      "   Impurity (MSE): 0.0642\n",
      "   Value (mean target): 1.3768\n",
      "----------------------------------------\n",
      "📌 Node 182: feature 2 ≤ -0.4847\n",
      "   Samples: 3787\n",
      "   Impurity (MSE): 0.0512\n",
      "   Value (mean target): 1.3732\n",
      "----------------------------------------\n",
      "🌿 Leaf 183:\n",
      "   Samples: 1947\n",
      "   Impurity (MSE): 0.0386\n",
      "   Value (mean target): 1.3512\n",
      "----------------------------------------\n",
      "🌿 Leaf 184:\n",
      "   Samples: 1840\n",
      "   Impurity (MSE): 0.0383\n",
      "   Value (mean target): 1.3965\n",
      "----------------------------------------\n",
      "📌 Node 185: feature 2 ≤ -0.4853\n",
      "   Samples: 3836\n",
      "   Impurity (MSE): 0.0509\n",
      "   Value (mean target): 1.3803\n",
      "----------------------------------------\n",
      "🌿 Leaf 186:\n",
      "   Samples: 1989\n",
      "   Impurity (MSE): 0.0394\n",
      "   Value (mean target): 1.3591\n",
      "----------------------------------------\n",
      "🌿 Leaf 187:\n",
      "   Samples: 1847\n",
      "   Impurity (MSE): 0.0382\n",
      "   Value (mean target): 1.4031\n",
      "----------------------------------------\n",
      "📌 Node 188: feature 1 ≤ 2.0817\n",
      "   Samples: 15565\n",
      "   Impurity (MSE): 0.1303\n",
      "   Value (mean target): 1.2888\n",
      "----------------------------------------\n",
      "📌 Node 189: feature 0 ≤ 0.9206\n",
      "   Samples: 7886\n",
      "   Impurity (MSE): 0.1046\n",
      "   Value (mean target): 1.1297\n",
      "----------------------------------------\n",
      "📌 Node 190: feature 0 ≤ 0.4646\n",
      "   Samples: 2539\n",
      "   Impurity (MSE): 0.0555\n",
      "   Value (mean target): 0.4937\n",
      "----------------------------------------\n",
      "🌿 Leaf 191:\n",
      "   Samples: 1344\n",
      "   Impurity (MSE): 0.0428\n",
      "   Value (mean target): 0.2823\n",
      "----------------------------------------\n",
      "🌿 Leaf 192:\n",
      "   Samples: 1195\n",
      "   Impurity (MSE): 0.0430\n",
      "   Value (mean target): 0.7314\n",
      "----------------------------------------\n",
      "📌 Node 193: feature 2 ≤ 0.4999\n",
      "   Samples: 5347\n",
      "   Impurity (MSE): 0.0563\n",
      "   Value (mean target): 1.4317\n",
      "----------------------------------------\n",
      "📌 Node 194: feature 3 ≤ -0.5172\n",
      "   Samples: 2667\n",
      "   Impurity (MSE): 0.0437\n",
      "   Value (mean target): 1.4078\n",
      "----------------------------------------\n",
      "🌿 Leaf 195:\n",
      "   Samples: 1277\n",
      "   Impurity (MSE): 0.0305\n",
      "   Value (mean target): 1.4049\n",
      "----------------------------------------\n",
      "🌿 Leaf 196:\n",
      "   Samples: 1390\n",
      "   Impurity (MSE): 0.0327\n",
      "   Value (mean target): 1.4106\n",
      "----------------------------------------\n",
      "📌 Node 197: feature 3 ≤ -0.5081\n",
      "   Samples: 2680\n",
      "   Impurity (MSE): 0.0440\n",
      "   Value (mean target): 1.4554\n",
      "----------------------------------------\n",
      "🌿 Leaf 198:\n",
      "   Samples: 1329\n",
      "   Impurity (MSE): 0.0311\n",
      "   Value (mean target): 1.4631\n",
      "----------------------------------------\n",
      "🌿 Leaf 199:\n",
      "   Samples: 1351\n",
      "   Impurity (MSE): 0.0316\n",
      "   Value (mean target): 1.4478\n",
      "----------------------------------------\n",
      "📌 Node 200: feature 1 ≤ 2.5325\n",
      "   Samples: 7679\n",
      "   Impurity (MSE): 0.0641\n",
      "   Value (mean target): 1.4522\n",
      "----------------------------------------\n",
      "📌 Node 201: feature 2 ≤ 0.5056\n",
      "   Samples: 3731\n",
      "   Impurity (MSE): 0.0507\n",
      "   Value (mean target): 1.4480\n",
      "----------------------------------------\n",
      "🌿 Leaf 202:\n",
      "   Samples: 1936\n",
      "   Impurity (MSE): 0.0380\n",
      "   Value (mean target): 1.4205\n",
      "----------------------------------------\n",
      "🌿 Leaf 203:\n",
      "   Samples: 1795\n",
      "   Impurity (MSE): 0.0387\n",
      "   Value (mean target): 1.4777\n",
      "----------------------------------------\n",
      "📌 Node 204: feature 3 ≤ -0.5089\n",
      "   Samples: 3948\n",
      "   Impurity (MSE): 0.0515\n",
      "   Value (mean target): 1.4561\n",
      "----------------------------------------\n",
      "🌿 Leaf 205:\n",
      "   Samples: 1986\n",
      "   Impurity (MSE): 0.0391\n",
      "   Value (mean target): 1.4592\n",
      "----------------------------------------\n",
      "🌿 Leaf 206:\n",
      "   Samples: 1962\n",
      "   Impurity (MSE): 0.0390\n",
      "   Value (mean target): 1.4530\n",
      "----------------------------------------\n",
      "📌 Node 207: feature 2 ≤ -0.0046\n",
      "   Samples: 31991\n",
      "   Impurity (MSE): 0.1804\n",
      "   Value (mean target): 1.2583\n",
      "----------------------------------------\n",
      "📌 Node 208: feature 1 ≤ 2.0496\n",
      "   Samples: 15950\n",
      "   Impurity (MSE): 0.1309\n",
      "   Value (mean target): 1.2125\n",
      "----------------------------------------\n",
      "📌 Node 209: feature 0 ≤ 0.9299\n",
      "   Samples: 7821\n",
      "   Impurity (MSE): 0.1051\n",
      "   Value (mean target): 1.0428\n",
      "----------------------------------------\n",
      "📌 Node 210: feature 0 ≤ 0.4642\n",
      "   Samples: 2598\n",
      "   Impurity (MSE): 0.0575\n",
      "   Value (mean target): 0.4194\n",
      "----------------------------------------\n",
      "🌿 Leaf 211:\n",
      "   Samples: 1301\n",
      "   Impurity (MSE): 0.0430\n",
      "   Value (mean target): 0.1810\n",
      "----------------------------------------\n",
      "🌿 Leaf 212:\n",
      "   Samples: 1297\n",
      "   Impurity (MSE): 0.0436\n",
      "   Value (mean target): 0.6585\n",
      "----------------------------------------\n",
      "📌 Node 213: feature 3 ≤ 0.4826\n",
      "   Samples: 5223\n",
      "   Impurity (MSE): 0.0561\n",
      "   Value (mean target): 1.3528\n",
      "----------------------------------------\n",
      "📌 Node 214: feature 2 ≤ -0.5223\n",
      "   Samples: 2532\n",
      "   Impurity (MSE): 0.0436\n",
      "   Value (mean target): 1.3470\n",
      "----------------------------------------\n",
      "🌿 Leaf 215:\n",
      "   Samples: 1272\n",
      "   Impurity (MSE): 0.0307\n",
      "   Value (mean target): 1.3267\n",
      "----------------------------------------\n",
      "🌿 Leaf 216:\n",
      "   Samples: 1260\n",
      "   Impurity (MSE): 0.0313\n",
      "   Value (mean target): 1.3675\n",
      "----------------------------------------\n",
      "📌 Node 217: feature 2 ≤ -0.4973\n",
      "   Samples: 2691\n",
      "   Impurity (MSE): 0.0428\n",
      "   Value (mean target): 1.3583\n",
      "----------------------------------------\n",
      "🌿 Leaf 218:\n",
      "   Samples: 1370\n",
      "   Impurity (MSE): 0.0306\n",
      "   Value (mean target): 1.3359\n",
      "----------------------------------------\n",
      "🌿 Leaf 219:\n",
      "   Samples: 1321\n",
      "   Impurity (MSE): 0.0307\n",
      "   Value (mean target): 1.3815\n",
      "----------------------------------------\n",
      "📌 Node 220: feature 1 ≤ 2.5076\n",
      "   Samples: 8129\n",
      "   Impurity (MSE): 0.0658\n",
      "   Value (mean target): 1.3759\n",
      "----------------------------------------\n",
      "📌 Node 221: feature 3 ≤ 0.4872\n",
      "   Samples: 3918\n",
      "   Impurity (MSE): 0.0515\n",
      "   Value (mean target): 1.3779\n",
      "----------------------------------------\n",
      "🌿 Leaf 222:\n",
      "   Samples: 1932\n",
      "   Impurity (MSE): 0.0387\n",
      "   Value (mean target): 1.3735\n",
      "----------------------------------------\n",
      "🌿 Leaf 223:\n",
      "   Samples: 1986\n",
      "   Impurity (MSE): 0.0389\n",
      "   Value (mean target): 1.3822\n",
      "----------------------------------------\n",
      "📌 Node 224: feature 3 ≤ 0.4848\n",
      "   Samples: 4211\n",
      "   Impurity (MSE): 0.0520\n",
      "   Value (mean target): 1.3740\n",
      "----------------------------------------\n",
      "📌 Node 225: feature 2 ≤ -0.5142\n",
      "   Samples: 2045\n",
      "   Impurity (MSE): 0.0389\n",
      "   Value (mean target): 1.3678\n",
      "----------------------------------------\n",
      "🌿 Leaf 226:\n",
      "   Samples: 1001\n",
      "   Impurity (MSE): 0.0268\n",
      "   Value (mean target): 1.3408\n",
      "----------------------------------------\n",
      "🌿 Leaf 227:\n",
      "   Samples: 1044\n",
      "   Impurity (MSE): 0.0271\n",
      "   Value (mean target): 1.3936\n",
      "----------------------------------------\n",
      "📌 Node 228: feature 2 ≤ -0.4762\n",
      "   Samples: 2166\n",
      "   Impurity (MSE): 0.0401\n",
      "   Value (mean target): 1.3799\n",
      "----------------------------------------\n",
      "🌿 Leaf 229:\n",
      "   Samples: 1102\n",
      "   Impurity (MSE): 0.0274\n",
      "   Value (mean target): 1.3586\n",
      "----------------------------------------\n",
      "🌿 Leaf 230:\n",
      "   Samples: 1064\n",
      "   Impurity (MSE): 0.0273\n",
      "   Value (mean target): 1.4019\n",
      "----------------------------------------\n",
      "📌 Node 231: feature 1 ≤ 2.0941\n",
      "   Samples: 16041\n",
      "   Impurity (MSE): 0.1305\n",
      "   Value (mean target): 1.3039\n",
      "----------------------------------------\n",
      "📌 Node 232: feature 0 ≤ 0.9358\n",
      "   Samples: 8101\n",
      "   Impurity (MSE): 0.1046\n",
      "   Value (mean target): 1.1474\n",
      "----------------------------------------\n",
      "📌 Node 233: feature 0 ≤ 0.4697\n",
      "   Samples: 2596\n",
      "   Impurity (MSE): 0.0567\n",
      "   Value (mean target): 0.5154\n",
      "----------------------------------------\n",
      "🌿 Leaf 234:\n",
      "   Samples: 1293\n",
      "   Impurity (MSE): 0.0433\n",
      "   Value (mean target): 0.2830\n",
      "----------------------------------------\n",
      "🌿 Leaf 235:\n",
      "   Samples: 1303\n",
      "   Impurity (MSE): 0.0434\n",
      "   Value (mean target): 0.7459\n",
      "----------------------------------------\n",
      "📌 Node 236: feature 2 ≤ 0.5072\n",
      "   Samples: 5505\n",
      "   Impurity (MSE): 0.0572\n",
      "   Value (mean target): 1.4455\n",
      "----------------------------------------\n",
      "📌 Node 237: feature 3 ≤ 0.4986\n",
      "   Samples: 2812\n",
      "   Impurity (MSE): 0.0448\n",
      "   Value (mean target): 1.4267\n",
      "----------------------------------------\n",
      "🌿 Leaf 238:\n",
      "   Samples: 1418\n",
      "   Impurity (MSE): 0.0322\n",
      "   Value (mean target): 1.4196\n",
      "----------------------------------------\n",
      "🌿 Leaf 239:\n",
      "   Samples: 1394\n",
      "   Impurity (MSE): 0.0312\n",
      "   Value (mean target): 1.4340\n",
      "----------------------------------------\n",
      "📌 Node 240: feature 3 ≤ 0.4740\n",
      "   Samples: 2693\n",
      "   Impurity (MSE): 0.0437\n",
      "   Value (mean target): 1.4651\n",
      "----------------------------------------\n",
      "🌿 Leaf 241:\n",
      "   Samples: 1250\n",
      "   Impurity (MSE): 0.0303\n",
      "   Value (mean target): 1.4619\n",
      "----------------------------------------\n",
      "🌿 Leaf 242:\n",
      "   Samples: 1443\n",
      "   Impurity (MSE): 0.0326\n",
      "   Value (mean target): 1.4678\n",
      "----------------------------------------\n",
      "📌 Node 243: feature 1 ≤ 2.5402\n",
      "   Samples: 7940\n",
      "   Impurity (MSE): 0.0641\n",
      "   Value (mean target): 1.4635\n",
      "----------------------------------------\n",
      "📌 Node 244: feature 3 ≤ 0.4907\n",
      "   Samples: 3895\n",
      "   Impurity (MSE): 0.0513\n",
      "   Value (mean target): 1.4590\n",
      "----------------------------------------\n",
      "🌿 Leaf 245:\n",
      "   Samples: 1919\n",
      "   Impurity (MSE): 0.0386\n",
      "   Value (mean target): 1.4604\n",
      "----------------------------------------\n",
      "🌿 Leaf 246:\n",
      "   Samples: 1976\n",
      "   Impurity (MSE): 0.0391\n",
      "   Value (mean target): 1.4577\n",
      "----------------------------------------\n",
      "📌 Node 247: feature 2 ≤ 0.5009\n",
      "   Samples: 4045\n",
      "   Impurity (MSE): 0.0513\n",
      "   Value (mean target): 1.4678\n",
      "----------------------------------------\n",
      "📌 Node 248: feature 3 ≤ 0.4949\n",
      "   Samples: 2068\n",
      "   Impurity (MSE): 0.0380\n",
      "   Value (mean target): 1.4445\n",
      "----------------------------------------\n",
      "🌿 Leaf 249:\n",
      "   Samples: 1023\n",
      "   Impurity (MSE): 0.0263\n",
      "   Value (mean target): 1.4438\n",
      "----------------------------------------\n",
      "🌿 Leaf 250:\n",
      "   Samples: 1045\n",
      "   Impurity (MSE): 0.0266\n",
      "   Value (mean target): 1.4452\n",
      "----------------------------------------\n",
      "🌿 Leaf 251:\n",
      "   Samples: 1977\n",
      "   Impurity (MSE): 0.0396\n",
      "   Value (mean target): 1.4922\n",
      "----------------------------------------\n",
      "📌 Node 252: feature 2 ≤ 0.0050\n",
      "   Samples: 64502\n",
      "   Impurity (MSE): 0.2069\n",
      "   Value (mean target): 2.3911\n",
      "----------------------------------------\n",
      "📌 Node 253: feature 3 ≤ 0.0161\n",
      "   Samples: 32488\n",
      "   Impurity (MSE): 0.1571\n",
      "   Value (mean target): 2.3505\n",
      "----------------------------------------\n",
      "📌 Node 254: feature 1 ≤ 2.2819\n",
      "   Samples: 16448\n",
      "   Impurity (MSE): 0.1077\n",
      "   Value (mean target): 2.3500\n",
      "----------------------------------------\n",
      "📌 Node 255: feature 0 ≤ 2.3760\n",
      "   Samples: 7440\n",
      "   Impurity (MSE): 0.0792\n",
      "   Value (mean target): 2.3275\n",
      "----------------------------------------\n",
      "📌 Node 256: feature 3 ≤ -0.4975\n",
      "   Samples: 3847\n",
      "   Impurity (MSE): 0.0554\n",
      "   Value (mean target): 2.0326\n",
      "----------------------------------------\n",
      "🌿 Leaf 257:\n",
      "   Samples: 1908\n",
      "   Impurity (MSE): 0.0421\n",
      "   Value (mean target): 2.0328\n",
      "----------------------------------------\n",
      "🌿 Leaf 258:\n",
      "   Samples: 1939\n",
      "   Impurity (MSE): 0.0428\n",
      "   Value (mean target): 2.0323\n",
      "----------------------------------------\n",
      "📌 Node 259: feature 3 ≤ -0.4837\n",
      "   Samples: 3593\n",
      "   Impurity (MSE): 0.0563\n",
      "   Value (mean target): 2.6432\n",
      "----------------------------------------\n",
      "🌿 Leaf 260:\n",
      "   Samples: 1803\n",
      "   Impurity (MSE): 0.0439\n",
      "   Value (mean target): 2.6413\n",
      "----------------------------------------\n",
      "🌿 Leaf 261:\n",
      "   Samples: 1790\n",
      "   Impurity (MSE): 0.0423\n",
      "   Value (mean target): 2.6451\n",
      "----------------------------------------\n",
      "📌 Node 262: feature 0 ≤ 2.4146\n",
      "   Samples: 9008\n",
      "   Impurity (MSE): 0.0739\n",
      "   Value (mean target): 2.3686\n",
      "----------------------------------------\n",
      "📌 Node 263: feature 2 ≤ -0.4849\n",
      "   Samples: 4517\n",
      "   Impurity (MSE): 0.0531\n",
      "   Value (mean target): 2.0765\n",
      "----------------------------------------\n",
      "📌 Node 264: feature 3 ≤ -0.5039\n",
      "   Samples: 2348\n",
      "   Impurity (MSE): 0.0402\n",
      "   Value (mean target): 2.0589\n",
      "----------------------------------------\n",
      "🌿 Leaf 265:\n",
      "   Samples: 1118\n",
      "   Impurity (MSE): 0.0272\n",
      "   Value (mean target): 2.0569\n",
      "----------------------------------------\n",
      "🌿 Leaf 266:\n",
      "   Samples: 1230\n",
      "   Impurity (MSE): 0.0280\n",
      "   Value (mean target): 2.0607\n",
      "----------------------------------------\n",
      "📌 Node 267: feature 3 ≤ -0.5133\n",
      "   Samples: 2169\n",
      "   Impurity (MSE): 0.0403\n",
      "   Value (mean target): 2.0956\n",
      "----------------------------------------\n",
      "🌿 Leaf 268:\n",
      "   Samples: 1028\n",
      "   Impurity (MSE): 0.0268\n",
      "   Value (mean target): 2.0929\n",
      "----------------------------------------\n",
      "🌿 Leaf 269:\n",
      "   Samples: 1141\n",
      "   Impurity (MSE): 0.0274\n",
      "   Value (mean target): 2.0981\n",
      "----------------------------------------\n",
      "📌 Node 270: feature 3 ≤ -0.4868\n",
      "   Samples: 4491\n",
      "   Impurity (MSE): 0.0515\n",
      "   Value (mean target): 2.6623\n",
      "----------------------------------------\n",
      "📌 Node 271: feature 2 ≤ -0.5156\n",
      "   Samples: 2286\n",
      "   Impurity (MSE): 0.0388\n",
      "   Value (mean target): 2.6620\n",
      "----------------------------------------\n",
      "🌿 Leaf 272:\n",
      "   Samples: 1087\n",
      "   Impurity (MSE): 0.0265\n",
      "   Value (mean target): 2.6353\n",
      "----------------------------------------\n",
      "🌿 Leaf 273:\n",
      "   Samples: 1199\n",
      "   Impurity (MSE): 0.0264\n",
      "   Value (mean target): 2.6862\n",
      "----------------------------------------\n",
      "📌 Node 274: feature 2 ≤ -0.5089\n",
      "   Samples: 2205\n",
      "   Impurity (MSE): 0.0388\n",
      "   Value (mean target): 2.6626\n",
      "----------------------------------------\n",
      "🌿 Leaf 275:\n",
      "   Samples: 1064\n",
      "   Impurity (MSE): 0.0266\n",
      "   Value (mean target): 2.6448\n",
      "----------------------------------------\n",
      "🌿 Leaf 276:\n",
      "   Samples: 1141\n",
      "   Impurity (MSE): 0.0265\n",
      "   Value (mean target): 2.6793\n",
      "----------------------------------------\n",
      "📌 Node 277: feature 1 ≤ 2.2737\n",
      "   Samples: 16040\n",
      "   Impurity (MSE): 0.1072\n",
      "   Value (mean target): 2.3511\n",
      "----------------------------------------\n",
      "📌 Node 278: feature 0 ≤ 2.3792\n",
      "   Samples: 7178\n",
      "   Impurity (MSE): 0.0782\n",
      "   Value (mean target): 2.3324\n",
      "----------------------------------------\n",
      "📌 Node 279: feature 2 ≤ -0.4971\n",
      "   Samples: 3667\n",
      "   Impurity (MSE): 0.0534\n",
      "   Value (mean target): 2.0310\n",
      "----------------------------------------\n",
      "🌿 Leaf 280:\n",
      "   Samples: 1784\n",
      "   Impurity (MSE): 0.0412\n",
      "   Value (mean target): 2.0049\n",
      "----------------------------------------\n",
      "🌿 Leaf 281:\n",
      "   Samples: 1883\n",
      "   Impurity (MSE): 0.0408\n",
      "   Value (mean target): 2.0558\n",
      "----------------------------------------\n",
      "📌 Node 282: feature 2 ≤ -0.5046\n",
      "   Samples: 3511\n",
      "   Impurity (MSE): 0.0552\n",
      "   Value (mean target): 2.6471\n",
      "----------------------------------------\n",
      "🌿 Leaf 283:\n",
      "   Samples: 1748\n",
      "   Impurity (MSE): 0.0428\n",
      "   Value (mean target): 2.6190\n",
      "----------------------------------------\n",
      "🌿 Leaf 284:\n",
      "   Samples: 1763\n",
      "   Impurity (MSE): 0.0416\n",
      "   Value (mean target): 2.6749\n",
      "----------------------------------------\n",
      "📌 Node 285: feature 0 ≤ 2.4059\n",
      "   Samples: 8862\n",
      "   Impurity (MSE): 0.0731\n",
      "   Value (mean target): 2.3662\n",
      "----------------------------------------\n",
      "📌 Node 286: feature 2 ≤ -0.5008\n",
      "   Samples: 4429\n",
      "   Impurity (MSE): 0.0519\n",
      "   Value (mean target): 2.0712\n",
      "----------------------------------------\n",
      "📌 Node 287: feature 3 ≤ 0.5150\n",
      "   Samples: 2186\n",
      "   Impurity (MSE): 0.0390\n",
      "   Value (mean target): 2.0481\n",
      "----------------------------------------\n",
      "🌿 Leaf 288:\n",
      "   Samples: 1121\n",
      "   Impurity (MSE): 0.0270\n",
      "   Value (mean target): 2.0449\n",
      "----------------------------------------\n",
      "🌿 Leaf 289:\n",
      "   Samples: 1065\n",
      "   Impurity (MSE): 0.0268\n",
      "   Value (mean target): 2.0515\n",
      "----------------------------------------\n",
      "📌 Node 290: feature 3 ≤ 0.4859\n",
      "   Samples: 2243\n",
      "   Impurity (MSE): 0.0390\n",
      "   Value (mean target): 2.0938\n",
      "----------------------------------------\n",
      "🌿 Leaf 291:\n",
      "   Samples: 1065\n",
      "   Impurity (MSE): 0.0265\n",
      "   Value (mean target): 2.0976\n",
      "----------------------------------------\n",
      "🌿 Leaf 292:\n",
      "   Samples: 1178\n",
      "   Impurity (MSE): 0.0273\n",
      "   Value (mean target): 2.0903\n",
      "----------------------------------------\n",
      "📌 Node 293: feature 2 ≤ -0.4912\n",
      "   Samples: 4433\n",
      "   Impurity (MSE): 0.0507\n",
      "   Value (mean target): 2.6610\n",
      "----------------------------------------\n",
      "📌 Node 294: feature 3 ≤ 0.4978\n",
      "   Samples: 2222\n",
      "   Impurity (MSE): 0.0380\n",
      "   Value (mean target): 2.6388\n",
      "----------------------------------------\n",
      "🌿 Leaf 295:\n",
      "   Samples: 1093\n",
      "   Impurity (MSE): 0.0257\n",
      "   Value (mean target): 2.6347\n",
      "----------------------------------------\n",
      "🌿 Leaf 296:\n",
      "   Samples: 1129\n",
      "   Impurity (MSE): 0.0262\n",
      "   Value (mean target): 2.6428\n",
      "----------------------------------------\n",
      "📌 Node 297: feature 3 ≤ 0.4995\n",
      "   Samples: 2211\n",
      "   Impurity (MSE): 0.0383\n",
      "   Value (mean target): 2.6833\n",
      "----------------------------------------\n",
      "🌿 Leaf 298:\n",
      "   Samples: 1072\n",
      "   Impurity (MSE): 0.0273\n",
      "   Value (mean target): 2.6876\n",
      "----------------------------------------\n",
      "🌿 Leaf 299:\n",
      "   Samples: 1139\n",
      "   Impurity (MSE): 0.0266\n",
      "   Value (mean target): 2.6792\n",
      "----------------------------------------\n",
      "📌 Node 300: feature 3 ≤ 0.0097\n",
      "   Samples: 32014\n",
      "   Impurity (MSE): 0.1571\n",
      "   Value (mean target): 2.4324\n",
      "----------------------------------------\n",
      "📌 Node 301: feature 1 ≤ 2.2867\n",
      "   Samples: 16122\n",
      "   Impurity (MSE): 0.1078\n",
      "   Value (mean target): 2.4338\n",
      "----------------------------------------\n",
      "📌 Node 302: feature 0 ≤ 2.3841\n",
      "   Samples: 7372\n",
      "   Impurity (MSE): 0.0791\n",
      "   Value (mean target): 2.4148\n",
      "----------------------------------------\n",
      "📌 Node 303: feature 3 ≤ -0.4992\n",
      "   Samples: 3867\n",
      "   Impurity (MSE): 0.0561\n",
      "   Value (mean target): 2.1244\n",
      "----------------------------------------\n",
      "🌿 Leaf 304:\n",
      "   Samples: 1938\n",
      "   Impurity (MSE): 0.0431\n",
      "   Value (mean target): 2.1280\n",
      "----------------------------------------\n",
      "🌿 Leaf 305:\n",
      "   Samples: 1929\n",
      "   Impurity (MSE): 0.0434\n",
      "   Value (mean target): 2.1208\n",
      "----------------------------------------\n",
      "📌 Node 306: feature 3 ≤ -0.4919\n",
      "   Samples: 3505\n",
      "   Impurity (MSE): 0.0553\n",
      "   Value (mean target): 2.7351\n",
      "----------------------------------------\n",
      "🌿 Leaf 307:\n",
      "   Samples: 1759\n",
      "   Impurity (MSE): 0.0423\n",
      "   Value (mean target): 2.7334\n",
      "----------------------------------------\n",
      "🌿 Leaf 308:\n",
      "   Samples: 1746\n",
      "   Impurity (MSE): 0.0429\n",
      "   Value (mean target): 2.7369\n",
      "----------------------------------------\n",
      "📌 Node 309: feature 0 ≤ 2.4085\n",
      "   Samples: 8750\n",
      "   Impurity (MSE): 0.0733\n",
      "   Value (mean target): 2.4499\n",
      "----------------------------------------\n",
      "📌 Node 310: feature 3 ≤ -0.4957\n",
      "   Samples: 4436\n",
      "   Impurity (MSE): 0.0520\n",
      "   Value (mean target): 2.1578\n",
      "----------------------------------------\n",
      "📌 Node 311: feature 2 ≤ 0.4752\n",
      "   Samples: 2211\n",
      "   Impurity (MSE): 0.0392\n",
      "   Value (mean target): 2.1574\n",
      "----------------------------------------\n",
      "🌿 Leaf 312:\n",
      "   Samples: 1034\n",
      "   Impurity (MSE): 0.0268\n",
      "   Value (mean target): 2.1378\n",
      "----------------------------------------\n",
      "🌿 Leaf 313:\n",
      "   Samples: 1177\n",
      "   Impurity (MSE): 0.0277\n",
      "   Value (mean target): 2.1747\n",
      "----------------------------------------\n",
      "📌 Node 314: feature 2 ≤ 0.5041\n",
      "   Samples: 2225\n",
      "   Impurity (MSE): 0.0394\n",
      "   Value (mean target): 2.1582\n",
      "----------------------------------------\n",
      "🌿 Leaf 315:\n",
      "   Samples: 1101\n",
      "   Impurity (MSE): 0.0268\n",
      "   Value (mean target): 2.1403\n",
      "----------------------------------------\n",
      "🌿 Leaf 316:\n",
      "   Samples: 1124\n",
      "   Impurity (MSE): 0.0276\n",
      "   Value (mean target): 2.1757\n",
      "----------------------------------------\n",
      "📌 Node 317: feature 3 ≤ -0.4975\n",
      "   Samples: 4314\n",
      "   Impurity (MSE): 0.0506\n",
      "   Value (mean target): 2.7503\n",
      "----------------------------------------\n",
      "📌 Node 318: feature 2 ≤ 0.5022\n",
      "   Samples: 2087\n",
      "   Impurity (MSE): 0.0380\n",
      "   Value (mean target): 2.7481\n",
      "----------------------------------------\n",
      "🌿 Leaf 319:\n",
      "   Samples: 1021\n",
      "   Impurity (MSE): 0.0257\n",
      "   Value (mean target): 2.7288\n",
      "----------------------------------------\n",
      "🌿 Leaf 320:\n",
      "   Samples: 1066\n",
      "   Impurity (MSE): 0.0264\n",
      "   Value (mean target): 2.7666\n",
      "----------------------------------------\n",
      "📌 Node 321: feature 2 ≤ 0.5117\n",
      "   Samples: 2227\n",
      "   Impurity (MSE): 0.0376\n",
      "   Value (mean target): 2.7523\n",
      "----------------------------------------\n",
      "🌿 Leaf 322:\n",
      "   Samples: 1134\n",
      "   Impurity (MSE): 0.0262\n",
      "   Value (mean target): 2.7346\n",
      "----------------------------------------\n",
      "🌿 Leaf 323:\n",
      "   Samples: 1093\n",
      "   Impurity (MSE): 0.0260\n",
      "   Value (mean target): 2.7707\n",
      "----------------------------------------\n",
      "📌 Node 324: feature 1 ≤ 2.2767\n",
      "   Samples: 15892\n",
      "   Impurity (MSE): 0.1071\n",
      "   Value (mean target): 2.4309\n",
      "----------------------------------------\n",
      "📌 Node 325: feature 0 ≤ 2.3757\n",
      "   Samples: 7114\n",
      "   Impurity (MSE): 0.0774\n",
      "   Value (mean target): 2.4102\n",
      "----------------------------------------\n",
      "📌 Node 326: feature 2 ≤ 0.4871\n",
      "   Samples: 3707\n",
      "   Impurity (MSE): 0.0533\n",
      "   Value (mean target): 2.1197\n",
      "----------------------------------------\n",
      "🌿 Leaf 327:\n",
      "   Samples: 1801\n",
      "   Impurity (MSE): 0.0411\n",
      "   Value (mean target): 2.0942\n",
      "----------------------------------------\n",
      "🌿 Leaf 328:\n",
      "   Samples: 1906\n",
      "   Impurity (MSE): 0.0408\n",
      "   Value (mean target): 2.1437\n",
      "----------------------------------------\n",
      "📌 Node 329: feature 2 ≤ 0.5048\n",
      "   Samples: 3407\n",
      "   Impurity (MSE): 0.0554\n",
      "   Value (mean target): 2.7264\n",
      "----------------------------------------\n",
      "🌿 Leaf 330:\n",
      "   Samples: 1714\n",
      "   Impurity (MSE): 0.0424\n",
      "   Value (mean target): 2.7039\n",
      "----------------------------------------\n",
      "🌿 Leaf 331:\n",
      "   Samples: 1693\n",
      "   Impurity (MSE): 0.0433\n",
      "   Value (mean target): 2.7491\n",
      "----------------------------------------\n",
      "📌 Node 332: feature 0 ≤ 2.4104\n",
      "   Samples: 8778\n",
      "   Impurity (MSE): 0.0739\n",
      "   Value (mean target): 2.4476\n",
      "----------------------------------------\n",
      "📌 Node 333: feature 2 ≤ 0.4952\n",
      "   Samples: 4486\n",
      "   Impurity (MSE): 0.0516\n",
      "   Value (mean target): 2.1554\n",
      "----------------------------------------\n",
      "📌 Node 334: feature 3 ≤ 0.5293\n",
      "   Samples: 2189\n",
      "   Impurity (MSE): 0.0391\n",
      "   Value (mean target): 2.1353\n",
      "----------------------------------------\n",
      "🌿 Leaf 335:\n",
      "   Samples: 1134\n",
      "   Impurity (MSE): 0.0270\n",
      "   Value (mean target): 2.1359\n",
      "----------------------------------------\n",
      "🌿 Leaf 336:\n",
      "   Samples: 1055\n",
      "   Impurity (MSE): 0.0273\n",
      "   Value (mean target): 2.1347\n",
      "----------------------------------------\n",
      "📌 Node 337: feature 3 ≤ 0.5122\n",
      "   Samples: 2297\n",
      "   Impurity (MSE): 0.0397\n",
      "   Value (mean target): 2.1745\n",
      "----------------------------------------\n",
      "🌿 Leaf 338:\n",
      "   Samples: 1130\n",
      "   Impurity (MSE): 0.0275\n",
      "   Value (mean target): 2.1761\n",
      "----------------------------------------\n",
      "🌿 Leaf 339:\n",
      "   Samples: 1167\n",
      "   Impurity (MSE): 0.0273\n",
      "   Value (mean target): 2.1730\n",
      "----------------------------------------\n",
      "📌 Node 340: feature 3 ≤ 0.4973\n",
      "   Samples: 4292\n",
      "   Impurity (MSE): 0.0512\n",
      "   Value (mean target): 2.7529\n",
      "----------------------------------------\n",
      "📌 Node 341: feature 2 ≤ 0.5066\n",
      "   Samples: 2114\n",
      "   Impurity (MSE): 0.0381\n",
      "   Value (mean target): 2.7574\n",
      "----------------------------------------\n",
      "🌿 Leaf 342:\n",
      "   Samples: 1097\n",
      "   Impurity (MSE): 0.0262\n",
      "   Value (mean target): 2.7320\n",
      "----------------------------------------\n",
      "🌿 Leaf 343:\n",
      "   Samples: 1017\n",
      "   Impurity (MSE): 0.0257\n",
      "   Value (mean target): 2.7848\n",
      "----------------------------------------\n",
      "📌 Node 344: feature 2 ≤ 0.4943\n",
      "   Samples: 2178\n",
      "   Impurity (MSE): 0.0393\n",
      "   Value (mean target): 2.7486\n",
      "----------------------------------------\n",
      "🌿 Leaf 345:\n",
      "   Samples: 1051\n",
      "   Impurity (MSE): 0.0269\n",
      "   Value (mean target): 2.7336\n",
      "----------------------------------------\n",
      "🌿 Leaf 346:\n",
      "   Samples: 1127\n",
      "   Impurity (MSE): 0.0271\n",
      "   Value (mean target): 2.7626\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "printTreeStats(tree_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62845fe6-71de-448d-9178-e579c33cf7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cf7b3a-ad91-49d0-a225-5d5724443068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c7343e6-6f5a-4d7d-8d6f-8b17124b0d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "class SelectivePruningDecisionTree(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    A Decision Tree Regressor that allows for selective post-pruning or\n",
    "    identification of leaves for expansion based on a custom optimization metric.\n",
    "\n",
    "    This model first trains a standard DecisionTreeRegressor. Then, for each leaf\n",
    "    in the tree, it solves a constrained optimization problem to find a linear\n",
    "    model (M, m_0) and an associated error metric (h). The tree can then be\n",
    "    analyzed to prune leaves with low 'h' values or expand those with high 'h'\n",
    "    values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    decision_tree_args : dict, optional\n",
    "        Arguments to be passed to the underlying `DecisionTreeRegressor`.\n",
    "        This allows for control over pre-pruning parameters like `max_depth`,\n",
    "        `min_samples_leaf`, etc. Default is None.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    tree_ : sklearn.tree.DecisionTreeRegressor\n",
    "        The initially trained decision tree.\n",
    "    leaf_models_ : dict\n",
    "        A dictionary where keys are the leaf node indices and values are\n",
    "        dictionaries containing the trained linear model ('M', 'm0') and\n",
    "        the error metric 'h' for that leaf.\n",
    "    is_fitted_ : bool\n",
    "        Flag indicating if the model has been fitted.\n",
    "    \"\"\"\n",
    "    def __init__(self, **decision_tree_args):\n",
    "        \"\"\"\n",
    "        Initializes the SelectivePruningDecisionTree.\n",
    "\n",
    "        Args:\n",
    "            **decision_tree_args: Arbitrary keyword arguments for the\n",
    "                                  underlying scikit-learn DecisionTreeRegressor.\n",
    "                                  e.g., max_depth=5, min_samples_leaf=10\n",
    "        \"\"\"\n",
    "        self.decision_tree_args = decision_tree_args\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model. This involves three main steps:\n",
    "        1. Train a standard DecisionTreeRegressor.\n",
    "        2. Identify the leaf nodes and the data samples belonging to each leaf.\n",
    "        3. For each leaf, run the constrained optimization to get M, m0, and h.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : array-like of shape (n_samples, n_outputs)\n",
    "            The target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # 1. Input validation\n",
    "        X, y = check_X_y(X, y, multi_output=True, y_numeric=True)\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "\n",
    "        # 2. Train the initial decision tree\n",
    "        self.tree_ = DecisionTreeRegressor(**self.decision_tree_args)\n",
    "        self.tree_.fit(X, y)\n",
    "\n",
    "        # 3. Calculate leaf assignments for the training data\n",
    "        leaf_ids = self.tree_.apply(X)\n",
    "        unique_leaves = np.unique(leaf_ids)\n",
    "\n",
    "        # 4. Calculate metrics (M, m0, h) for each leaf\n",
    "        self.leaf_models_ = self._calculate_leaf_metrics(X, y, leaf_ids, unique_leaves)\n",
    "\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def _calculate_leaf_metrics(self, X, y, leaf_ids, unique_leaves):\n",
    "        \"\"\"\n",
    "        Iterates through each leaf, segments the data, and runs the\n",
    "        optimization function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            The full training dataset.\n",
    "        y : np.ndarray\n",
    "            The full target dataset.\n",
    "        leaf_ids : np.ndarray\n",
    "            An array where each element is the leaf index for a sample in X.\n",
    "        unique_leaves : np.ndarray\n",
    "            An array of unique leaf indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary containing the fitted models for each leaf.\n",
    "        \"\"\"\n",
    "        leaf_models = {}\n",
    "        for leaf in unique_leaves:\n",
    "            # Find all data points that fall into the current leaf\n",
    "            mask = leaf_ids == leaf\n",
    "            X_leaf = X[mask]\n",
    "            y_leaf = y[mask]\n",
    "\n",
    "            if X_leaf.shape[0] > 0: # Ensure the leaf is not empty\n",
    "                M_val, m0_val, h_val = self.constrained_optimization_gurobi(X_leaf, y_leaf)\n",
    "                leaf_models[leaf] = {\n",
    "                    'M': M_val,\n",
    "                    'm0': m0_val,\n",
    "                    'h': h_val,\n",
    "                    'n_samples': X_leaf.shape[0]\n",
    "                }\n",
    "        return leaf_models\n",
    "\n",
    "    @staticmethod\n",
    "    def constrained_optimization_gurobi(X_leaf, y_leaf):\n",
    "        \"\"\"\n",
    "        Solves the constrained optimization problem for a single leaf.\n",
    "\n",
    "        This function is kept static as it doesn't depend on the state of the\n",
    "        class instance ('self'), promoting modularity.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_leaf : np.ndarray\n",
    "            Data samples belonging to a specific leaf.\n",
    "        y_leaf : np.ndarray\n",
    "            Target values for the samples in X_leaf.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            A tuple containing (M_val, m0_val, h_val). Returns (None, None, np.inf)\n",
    "            on optimization failure.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X_leaf.shape\n",
    "        n_outputs = y_leaf.shape[1]\n",
    "\n",
    "        model = gp.Model(\"constrained_optimization\")\n",
    "        model.setParam(\"OutputFlag\", 0)\n",
    "\n",
    "        M = model.addVars(n_outputs, n_features, lb=-GRB.INFINITY, name=\"M\")\n",
    "        m0 = model.addVars(n_outputs, lb=-GRB.INFINITY, name=\"m0\")\n",
    "        h = model.addVar(lb=0, name=\"h\")\n",
    "\n",
    "        residuals = gp.QuadExpr()\n",
    "        for i in range(n_samples):\n",
    "            for k in range(n_outputs):\n",
    "                pred_ik = m0[k] + gp.quicksum(M[k, j] * X_leaf[i, j] for j in range(n_features))\n",
    "                residual = pred_ik - y_leaf[i, k]\n",
    "                residuals += residual * residual\n",
    "\n",
    "        model.addConstr(residuals <= h, name=\"residual_constraint\")\n",
    "        model.setObjective(h, GRB.MINIMIZE)\n",
    "        model.optimize()\n",
    "\n",
    "        if model.status in [GRB.OPTIMAL, GRB.SUBOPTIMAL] and model.SolCount > 0:\n",
    "            M_val = np.array([[M[k, j].X for j in range(n_features)] for k in range(n_outputs)])\n",
    "            m0_val = np.array([m0[k].X for k in range(n_outputs)])\n",
    "            h_val = h.X\n",
    "        else:\n",
    "            print(f\"Optimization failed for a leaf with {n_samples} samples. Status: {model.status}\")\n",
    "            M_val, m0_val, h_val = None, None, np.inf\n",
    "\n",
    "        return M_val, m0_val, h_val\n",
    "\n",
    "    def get_leaves_to_expand(self, h_threshold):\n",
    "        \"\"\"\n",
    "        Identifies leaves that have an 'h' value greater than a given threshold.\n",
    "        These are candidates for further splitting (expansion).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        h_threshold : float\n",
    "            The threshold for the 'h' metric.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            A list of leaf node indices to be considered for expansion.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        leaves_to_expand = []\n",
    "        for leaf, model_info in self.leaf_models_.items():\n",
    "            if model_info['h'] > h_threshold:\n",
    "                leaves_to_expand.append(leaf)\n",
    "        return leaves_to_expand\n",
    "\n",
    "    def get_leaves_to_prune(self, h_threshold):\n",
    "        \"\"\"\n",
    "        Identifies leaves that have an 'h' value less than or equal to a\n",
    "        given threshold. These are candidates for pruning.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        h_threshold : float\n",
    "            The threshold for the 'h' metric.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            A list of leaf node indices to be considered for pruning.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        leaves_to_prune = []\n",
    "        for leaf, model_info in self.leaf_models_.items():\n",
    "            if model_info['h'] <= h_threshold:\n",
    "                leaves_to_prune.append(leaf)\n",
    "        return leaves_to_prune\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for X.\n",
    "\n",
    "        For each sample in X, it finds the corresponding leaf in the tree\n",
    "        and uses the leaf's linear model (M, m0) to make a prediction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input samples to predict.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array-like of shape (n_samples, n_outputs)\n",
    "            The predicted values.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "\n",
    "        leaf_ids = self.tree_.apply(X)\n",
    "        # Assuming y has at least one output\n",
    "        n_outputs = self.leaf_models_[list(self.leaf_models_.keys())[0]]['M'].shape[0]\n",
    "        y_pred = np.zeros((X.shape[0], n_outputs))\n",
    "\n",
    "        for i, leaf_id in enumerate(leaf_ids):\n",
    "            model = self.leaf_models_.get(leaf_id)\n",
    "            if model and model['M'] is not None:\n",
    "                # Prediction: y = X @ M.T + m0\n",
    "                y_pred[i, :] = X[i, :] @ model['M'].T + model['m0']\n",
    "            else:\n",
    "                # Fallback: use the tree's original prediction for this leaf\n",
    "                # This handles cases where optimization failed for a leaf.\n",
    "                y_pred[i, :] = self.tree_.predict(X[i, :].reshape(1, -1))\n",
    "\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ceb19ef-b450-4c56-8584-1b1bb8a7a993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of Fitted Leaf Models:\n",
      "--------------------------------------------------\n",
      "Leaf Node ID: 3\n",
      "  - Number of Samples: 24\n",
      "  - H-value (error): 100.2010\n",
      "--------------------\n",
      "Leaf Node ID: 4\n",
      "  - Number of Samples: 10\n",
      "  - H-value (error): 222.3151\n",
      "--------------------\n",
      "Leaf Node ID: 6\n",
      "  - Number of Samples: 26\n",
      "  - H-value (error): 723.6372\n",
      "--------------------\n",
      "Leaf Node ID: 7\n",
      "  - Number of Samples: 45\n",
      "  - H-value (error): 1648.6170\n",
      "--------------------\n",
      "Leaf Node ID: 10\n",
      "  - Number of Samples: 14\n",
      "  - H-value (error): 100.2937\n",
      "--------------------\n",
      "Leaf Node ID: 11\n",
      "  - Number of Samples: 19\n",
      "  - H-value (error): 98.4425\n",
      "--------------------\n",
      "Leaf Node ID: 13\n",
      "  - Number of Samples: 12\n",
      "  - H-value (error): 75.4971\n",
      "--------------------\n",
      "Leaf Node ID: 14\n",
      "  - Number of Samples: 10\n",
      "  - H-value (error): 15.1974\n",
      "--------------------\n",
      "\n",
      "Based on h_threshold = 1000.0:\n",
      "Leaves identified for EXPANSION (high error): [np.int64(7)]\n",
      "Leaves identified for PRUNING (low error): [np.int64(3), np.int64(4), np.int64(6), np.int64(10), np.int64(11), np.int64(13), np.int64(14)]\n",
      "\n",
      "This information can now be used to guide a second, more refined modeling step.\n",
      "\n",
      "Mean Squared Error on Test Set: 21.0128\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # 1. Generate synthetic data\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(200, 3) * 10\n",
    "    # Create a piecewise linear relationship\n",
    "    y1 = 2 * X[:, 0] + 3 * X[:, 1] + 5\n",
    "    y2 = -1.5 * X[:, 0] + 0.5 * X[:, 2] - 2\n",
    "    # Add noise\n",
    "    y1 += np.random.normal(0, 2, size=y1.shape)\n",
    "    y2 += np.random.normal(0, 2, size=y2.shape)\n",
    "    # Different relationship for a different segment of data\n",
    "    mask = X[:, 0] > 5\n",
    "    y1[mask] = 4 * X[mask, 0] - 1 * X[mask, 1] + 10 + np.random.normal(0, 2, size=y1[mask].shape)\n",
    "    y2[mask] = 1 * X[mask, 0] - 2 * X[mask, 2] + 3 + np.random.normal(0, 2, size=y2[mask].shape)\n",
    "\n",
    "    y = np.vstack([y1, y2]).T\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 2. Instantiate and fit the model\n",
    "    # We use max_depth=3 to create a few leaves to analyze\n",
    "    spt = SelectivePruningDecisionTree(max_depth=3, min_samples_leaf=10, random_state=42)\n",
    "    spt.fit(X_train, y_train)\n",
    "\n",
    "    # 3. Analyze the results\n",
    "    print(\"Analysis of Fitted Leaf Models:\")\n",
    "    print(\"-\" * 50)\n",
    "    for leaf, model_info in spt.leaf_models_.items():\n",
    "        print(f\"Leaf Node ID: {leaf}\")\n",
    "        print(f\"  - Number of Samples: {model_info['n_samples']}\")\n",
    "        print(f\"  - H-value (error): {model_info['h']:.4f}\")\n",
    "        # print(f\"  - M matrix:\\n{model_info['M']}\") # Uncomment for more detail\n",
    "        # print(f\"  - m0 vector: {model_info['m0']}\") # Uncomment for more detail\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    # 4. Use the model to identify leaves for expansion/pruning\n",
    "    # Let's set a threshold. Leaves with error > threshold should be split further.\n",
    "    # Leaves with error <= threshold are good candidates for pruning.\n",
    "    h_error_threshold = 1000.0\n",
    "\n",
    "    leaves_to_expand = spt.get_leaves_to_expand(h_error_threshold)\n",
    "    leaves_to_prune = spt.get_leaves_to_prune(h_error_threshold)\n",
    "\n",
    "    print(f\"\\nBased on h_threshold = {h_error_threshold}:\")\n",
    "    print(f\"Leaves identified for EXPANSION (high error): {leaves_to_expand}\")\n",
    "    print(f\"Leaves identified for PRUNING (low error): {leaves_to_prune}\")\n",
    "    print(\"\\nThis information can now be used to guide a second, more refined modeling step.\")\n",
    "\n",
    "    # 5. Make predictions and evaluate\n",
    "    y_pred = spt.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"\\nMean Squared Error on Test Set: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f7163-392b-411f-917a-6fb5db18ec0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62a7eaf4-4b21-421d-b654-0611e4d217d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "class SelectivePruningDecisionTree(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    A Decision Tree Regressor that allows for selective post-pruning or\n",
    "    identification of leaves for expansion based on a custom optimization metric.\n",
    "\n",
    "    This model first trains a standard DecisionTreeRegressor. Then, for each leaf\n",
    "    in the tree, it solves a constrained optimization problem to find a linear\n",
    "    model (M, m_0) and an associated error metric (h). The tree can then be\n",
    "    analyzed to prune leaves with low 'h' values or expand those with high 'h'\n",
    "    values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    decision_tree_args : dict, optional\n",
    "        Arguments to be passed to the underlying `DecisionTreeRegressor`.\n",
    "        This allows for control over pre-pruning parameters like `max_depth`,\n",
    "        `min_samples_leaf`, etc. Default is None.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    tree_ : sklearn.tree.DecisionTreeRegressor\n",
    "        The initially trained decision tree.\n",
    "    leaf_models_ : dict\n",
    "        A dictionary where keys are the leaf node indices and values are\n",
    "        dictionaries containing the trained linear model ('M', 'm0') and\n",
    "        the error metric 'h' for that leaf.\n",
    "    sub_trees_ : dict\n",
    "        A dictionary to hold new sub-trees for leaves that have been expanded.\n",
    "    pruned_leaves_ : set\n",
    "        A set of leaf node indices that have been marked for pruning.\n",
    "    is_fitted_ : bool\n",
    "        Flag indicating if the model has been fitted.\n",
    "    \"\"\"\n",
    "    def __init__(self, **decision_tree_args):\n",
    "        \"\"\"\n",
    "        Initializes the SelectivePruningDecisionTree.\n",
    "\n",
    "        Args:\n",
    "            **decision_tree_args: Arbitrary keyword arguments for the\n",
    "                                  underlying scikit-learn DecisionTreeRegressor.\n",
    "                                  e.g., max_depth=5, min_samples_leaf=10\n",
    "        \"\"\"\n",
    "        self.decision_tree_args = decision_tree_args\n",
    "        self.sub_trees_ = {}\n",
    "        self.pruned_leaves_ = set()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model. This involves three main steps:\n",
    "        1. Train a standard DecisionTreeRegressor.\n",
    "        2. Identify the leaf nodes and the data samples belonging to each leaf.\n",
    "        3. For each leaf, run the constrained optimization to get M, m0, and h.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : array-like of shape (n_samples, n_outputs)\n",
    "            The target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # 1. Input validation\n",
    "        X, y = check_X_y(X, y, multi_output=True, y_numeric=True)\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "\n",
    "        # 2. Train the initial decision tree\n",
    "        self.tree_ = DecisionTreeRegressor(**self.decision_tree_args)\n",
    "        self.tree_.fit(X, y)\n",
    "\n",
    "        # 3. Calculate leaf assignments for the training data\n",
    "        leaf_ids = self.tree_.apply(X)\n",
    "        unique_leaves = np.unique(leaf_ids)\n",
    "\n",
    "        # 4. Calculate metrics (M, m0, h) for each leaf\n",
    "        self.leaf_models_ = self._calculate_leaf_metrics(X, y, leaf_ids, unique_leaves)\n",
    "\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def _calculate_leaf_metrics(self, X, y, leaf_ids, unique_leaves):\n",
    "        \"\"\"\n",
    "        Iterates through each leaf, segments the data, and runs the\n",
    "        optimization function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            The full training dataset.\n",
    "        y : np.ndarray\n",
    "            The full target dataset.\n",
    "        leaf_ids : np.ndarray\n",
    "            An array where each element is the leaf index for a sample in X.\n",
    "        unique_leaves : np.ndarray\n",
    "            An array of unique leaf indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary containing the fitted models for each leaf.\n",
    "        \"\"\"\n",
    "        leaf_models = {}\n",
    "        for leaf in unique_leaves:\n",
    "            # Find all data points that fall into the current leaf\n",
    "            mask = leaf_ids == leaf\n",
    "            X_leaf = X[mask]\n",
    "            y_leaf = y[mask]\n",
    "\n",
    "            if X_leaf.shape[0] > 0: # Ensure the leaf is not empty\n",
    "                M_val, m0_val, h_val = self.constrained_optimization_gurobi(X_leaf, y_leaf)\n",
    "                leaf_models[leaf] = {\n",
    "                    'M': M_val,\n",
    "                    'm0': m0_val,\n",
    "                    'h': h_val,\n",
    "                    'n_samples': X_leaf.shape[0]\n",
    "                }\n",
    "        return leaf_models\n",
    "\n",
    "    @staticmethod\n",
    "    def constrained_optimization_gurobi(X_leaf, y_leaf):\n",
    "        \"\"\"\n",
    "        Solves the constrained optimization problem for a single leaf.\n",
    "\n",
    "        This function is kept static as it doesn't depend on the state of the\n",
    "        class instance ('self'), promoting modularity.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_leaf : np.ndarray\n",
    "            Data samples belonging to a specific leaf.\n",
    "        y_leaf : np.ndarray\n",
    "            Target values for the samples in X_leaf.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            A tuple containing (M_val, m0_val, h_val). Returns (None, None, np.inf)\n",
    "            on optimization failure.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X_leaf.shape\n",
    "        n_outputs = y_leaf.shape[1]\n",
    "\n",
    "        model = gp.Model(\"constrained_optimization\")\n",
    "        model.setParam(\"OutputFlag\", 0)\n",
    "\n",
    "        M = model.addVars(n_outputs, n_features, lb=-GRB.INFINITY, name=\"M\")\n",
    "        m0 = model.addVars(n_outputs, lb=-GRB.INFINITY, name=\"m0\")\n",
    "        h = model.addVar(lb=0, name=\"h\")\n",
    "\n",
    "        residuals = gp.QuadExpr()\n",
    "        for i in range(n_samples):\n",
    "            for k in range(n_outputs):\n",
    "                pred_ik = m0[k] + gp.quicksum(M[k, j] * X_leaf[i, j] for j in range(n_features))\n",
    "                residual = pred_ik - y_leaf[i, k]\n",
    "                residuals += residual * residual\n",
    "\n",
    "        model.addConstr(residuals <= h, name=\"residual_constraint\")\n",
    "        model.setObjective(h, GRB.MINIMIZE)\n",
    "        model.optimize()\n",
    "\n",
    "        if model.status in [GRB.OPTIMAL, GRB.SUBOPTIMAL] and model.SolCount > 0:\n",
    "            M_val = np.array([[M[k, j].X for j in range(n_features)] for k in range(n_outputs)])\n",
    "            m0_val = np.array([m0[k].X for k in range(n_outputs)])\n",
    "            h_val = h.X\n",
    "        else:\n",
    "            print(f\"Optimization failed for a leaf with {n_samples} samples. Status: {model.status}\")\n",
    "            M_val, m0_val, h_val = None, None, np.inf\n",
    "\n",
    "        return M_val, m0_val, h_val\n",
    "\n",
    "    def get_leaves_to_expand(self, h_threshold):\n",
    "        \"\"\"\n",
    "        Identifies leaves that have an 'h' value greater than a given threshold.\n",
    "        These are candidates for further splitting (expansion).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        h_threshold : float\n",
    "            The threshold for the 'h' metric.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            A list of leaf node indices to be considered for expansion.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        leaves_to_expand = []\n",
    "        for leaf, model_info in self.leaf_models_.items():\n",
    "            if model_info['h'] > h_threshold:\n",
    "                leaves_to_expand.append(leaf)\n",
    "        return leaves_to_expand\n",
    "\n",
    "    def get_leaves_to_prune(self, h_threshold):\n",
    "        \"\"\"\n",
    "        Identifies leaves that have an 'h' value less than or equal to a\n",
    "        given threshold. These are candidates for pruning.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        h_threshold : float\n",
    "            The threshold for the 'h' metric.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            A list of leaf node indices to be considered for pruning.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        leaves_to_prune = []\n",
    "        for leaf, model_info in self.leaf_models_.items():\n",
    "            if model_info['h'] <= h_threshold:\n",
    "                leaves_to_prune.append(leaf)\n",
    "        return leaves_to_prune\n",
    "\n",
    "    def refine(self, X, y, h_threshold, expansion_args=None):\n",
    "        \"\"\"\n",
    "        Refines the tree by expanding high-error leaves and marking low-error\n",
    "        leaves for pruning.\n",
    "\n",
    "        Expansion: Trains a new sub-tree on the data points of a high-error leaf.\n",
    "        Pruning: Marks a leaf to use the original tree's simpler prediction\n",
    "                 (mean of values) instead of its own linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The training input samples, needed for training sub-trees.\n",
    "        y : array-like of shape (n_samples, n_outputs)\n",
    "            The target values, needed for training sub-trees.\n",
    "        h_threshold : float\n",
    "            The threshold for the 'h' metric to decide on expansion/pruning.\n",
    "        expansion_args : dict, optional\n",
    "            Arguments for the DecisionTreeRegressor used for the sub-trees,\n",
    "            e.g., {'max_depth': 2}. Default is None.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        if expansion_args is None:\n",
    "            expansion_args = {}\n",
    "\n",
    "        leaves_to_expand = self.get_leaves_to_expand(h_threshold)\n",
    "        self.pruned_leaves_ = set(self.get_leaves_to_prune(h_threshold))\n",
    "\n",
    "        # We need the original data to train sub-trees\n",
    "        leaf_ids = self.tree_.apply(X)\n",
    "\n",
    "        print(f\"\\nRefining tree based on h_threshold = {h_threshold}:\")\n",
    "        print(f\" -> {len(leaves_to_expand)} leaves to expand: {leaves_to_expand}\")\n",
    "        print(f\" -> {len(self.pruned_leaves_)} leaves to prune: {list(self.pruned_leaves_)}\")\n",
    "\n",
    "        for leaf in leaves_to_expand:\n",
    "            print(f\"  - Expanding leaf {leaf}...\")\n",
    "            mask = (leaf_ids == leaf)\n",
    "            X_leaf = X[mask]\n",
    "            y_leaf = y[mask]\n",
    "\n",
    "            if X_leaf.shape[0] > 1: # Need at least 2 samples to split\n",
    "                # For simplicity, we use a standard DecisionTreeRegressor for the sub-tree\n",
    "                sub_tree = DecisionTreeRegressor(**expansion_args)\n",
    "                sub_tree.fit(X_leaf, y_leaf)\n",
    "                self.sub_trees_[leaf] = sub_tree\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for X.\n",
    "\n",
    "        The prediction logic is as follows:\n",
    "        1. If a sample falls into a leaf that was expanded, use the corresponding\n",
    "           sub-tree to make a prediction.\n",
    "        2. If a sample falls into a leaf that was pruned, use the original\n",
    "           tree's simple (mean) prediction for that leaf.\n",
    "        3. Otherwise, use the leaf's specialized linear model (M, m0).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input samples to predict.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array-like of shape (n_samples, n_outputs)\n",
    "            The predicted values.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "\n",
    "        leaf_ids = self.tree_.apply(X)\n",
    "\n",
    "        # Robustly find n_outputs from the first available leaf model\n",
    "        first_valid_leaf = next((leaf for leaf in self.leaf_models_.values() if leaf['M'] is not None), None)\n",
    "        if first_valid_leaf is None:\n",
    "            raise RuntimeError(\"No valid leaf models were fitted. Cannot determine number of outputs.\")\n",
    "        n_outputs = first_valid_leaf['M'].shape[0]\n",
    "        y_pred = np.zeros((X.shape[0], n_outputs))\n",
    "\n",
    "        for i, leaf_id in enumerate(leaf_ids):\n",
    "            # Case 1: The leaf was expanded into a sub-tree\n",
    "            if leaf_id in self.sub_trees_:\n",
    "                y_pred[i, :] = self.sub_trees_[leaf_id].predict(X[i, :].reshape(1, -1))\n",
    "                continue\n",
    "\n",
    "            # Case 2: The leaf was marked for pruning (use simple mean)\n",
    "            if leaf_id in self.pruned_leaves_:\n",
    "                y_pred[i, :] = self.tree_.predict(X[i, :].reshape(1, -1))\n",
    "                continue\n",
    "\n",
    "            # Case 3: Standard prediction using the leaf's linear model\n",
    "            model = self.leaf_models_.get(leaf_id)\n",
    "            if model and model['M'] is not None:\n",
    "                y_pred[i, :] = X[i, :] @ model['M'].T + model['m0']\n",
    "            else:\n",
    "                # Fallback for leaves where optimization failed initially\n",
    "                y_pred[i, :] = self.tree_.predict(X[i, :].reshape(1, -1))\n",
    "\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81a016a4-3c82-457c-9e70-a5c5edde154f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial Analysis of Fitted Leaf Models ---\n",
      "--------------------------------------------------\n",
      "Leaf Node ID: 4, Samples: 15197, H-value: 22.50\n",
      "Leaf Node ID: 5, Samples: 14986, H-value: 22.30\n",
      "Leaf Node ID: 7, Samples: 15869, H-value: 23.69\n",
      "Leaf Node ID: 8, Samples: 15162, H-value: 22.63\n",
      "Leaf Node ID: 11, Samples: 15729, H-value: 13.40\n",
      "Leaf Node ID: 12, Samples: 15776, H-value: 13.14\n",
      "Leaf Node ID: 14, Samples: 15390, H-value: 13.17\n",
      "Leaf Node ID: 15, Samples: 15605, H-value: 13.32\n",
      "Leaf Node ID: 19, Samples: 15755, H-value: 13.10\n",
      "Leaf Node ID: 20, Samples: 15286, H-value: 13.00\n",
      "Leaf Node ID: 22, Samples: 15922, H-value: 13.33\n",
      "Leaf Node ID: 23, Samples: 15914, H-value: 13.47\n",
      "Leaf Node ID: 26, Samples: 15783, H-value: 23.27\n",
      "Leaf Node ID: 27, Samples: 15811, H-value: 24.05\n",
      "Leaf Node ID: 29, Samples: 15932, H-value: 23.75\n",
      "Leaf Node ID: 30, Samples: 15882, H-value: 24.44\n",
      "\n",
      "Mean Squared Error on Test Set (BEFORE refinement): 0.0003\n",
      "\n",
      "Refining tree based on h_threshold = 1.0:\n",
      " -> 16 leaves to expand: [np.int64(4), np.int64(5), np.int64(7), np.int64(8), np.int64(11), np.int64(12), np.int64(14), np.int64(15), np.int64(19), np.int64(20), np.int64(22), np.int64(23), np.int64(26), np.int64(27), np.int64(29), np.int64(30)]\n",
      " -> 0 leaves to prune: []\n",
      "  - Expanding leaf 4...\n",
      "  - Expanding leaf 5...\n",
      "  - Expanding leaf 7...\n",
      "  - Expanding leaf 8...\n",
      "  - Expanding leaf 11...\n",
      "  - Expanding leaf 12...\n",
      "  - Expanding leaf 14...\n",
      "  - Expanding leaf 15...\n",
      "  - Expanding leaf 19...\n",
      "  - Expanding leaf 20...\n",
      "  - Expanding leaf 22...\n",
      "  - Expanding leaf 23...\n",
      "  - Expanding leaf 26...\n",
      "  - Expanding leaf 27...\n",
      "  - Expanding leaf 29...\n",
      "  - Expanding leaf 30...\n",
      "\n",
      "Mean Squared Error on Test Set (AFTER refinement): 0.0533\n",
      "Improvement: -18059.61%\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # # 1. Generate synthetic data\n",
    "    # np.random.seed(42)\n",
    "    # X = np.random.rand(200, 3) * 10\n",
    "    # # Create a piecewise linear relationship\n",
    "    # y1 = 2 * X[:, 0] + 3 * X[:, 1] + 5\n",
    "    # y2 = -1.5 * X[:, 0] + 0.5 * X[:, 2] - 2\n",
    "    # # Add noise\n",
    "    # y1 += np.random.normal(0, 2, size=y1.shape)\n",
    "    # y2 += np.random.normal(0, 2, size=y2.shape)\n",
    "    # # Different relationship for a different segment of data\n",
    "    # mask = X[:, 0] > 5\n",
    "    # y1[mask] = 4 * X[mask, 0] - 1 * X[mask, 1] + 10 + np.random.normal(0, 2, size=y1[mask].shape)\n",
    "    # y2[mask] = 1 * X[mask, 0] - 2 * X[mask, 2] + 3 + np.random.normal(0, 2, size=y2[mask].shape)\n",
    "\n",
    "    # y = np.vstack([y1, y2]).T\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 2. Instantiate and fit the model\n",
    "    spt = SelectivePruningDecisionTree(max_depth=5, min_samples_leaf=10000, random_state=42)\n",
    "    spt.fit(X_train, y_train)\n",
    "\n",
    "    # 3. Analyze the results\n",
    "    print(\"--- Initial Analysis of Fitted Leaf Models ---\")\n",
    "    print(\"-\" * 50)\n",
    "    for leaf, model_info in spt.leaf_models_.items():\n",
    "        print(f\"Leaf Node ID: {leaf}, Samples: {model_info['n_samples']}, H-value: {model_info['h']:.2f}\")\n",
    "\n",
    "    # 4. Evaluate the model BEFORE refinement\n",
    "    y_pred_before = spt.predict(X_test)\n",
    "    mse_before = mean_squared_error(y_test, y_pred_before)\n",
    "    print(f\"\\nMean Squared Error on Test Set (BEFORE refinement): {mse_before:.4f}\")\n",
    "\n",
    "    # 5. REFINE the tree based on the h-value threshold\n",
    "    h_error_threshold = 1.0\n",
    "    # For sub-trees, let's give them a bit more depth\n",
    "    expansion_params = {'max_depth': 2, 'min_samples_leaf': 1000, 'random_state': 42}\n",
    "    spt.refine(X_train, y_train, h_threshold=h_error_threshold, expansion_args=expansion_params)\n",
    "\n",
    "    # 6. Evaluate the model AFTER refinement\n",
    "    y_pred_after = spt.predict(X_test)\n",
    "    mse_after = mean_squared_error(y_test, y_pred_after)\n",
    "    print(f\"\\nMean Squared Error on Test Set (AFTER refinement): {mse_after:.4f}\")\n",
    "    \n",
    "    improvement = ((mse_before - mse_after) / mse_before) * 100\n",
    "    print(f\"Improvement: {improvement:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f906fe-3a28-4028-968c-7ef171ca243d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
