{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9b1f563-04e5-4687-93f5-bfe01acb7140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d168980d-3a06-4336-8ebd-4c3aa75c4b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QuadraticConstraintModel import get_leaf_samples\n",
    "\n",
    "from QuadraticConstraintModel import constrained_optimization_gurobi\n",
    "\n",
    "from QuadraticConstraintModel import predict_from_COF\n",
    "\n",
    "from QuadraticConstraintModel import  get_h_from_COF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bffc5904-e58b-40d2-8d98-42050d9224fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load DataSet\n",
    "def load_dataset(file_path, num_attributes=2, num_classes=2):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.iloc[:, 0 :  num_attributes].values\n",
    "    y = data.iloc[:,  num_attributes:  num_attributes + num_classes].values\n",
    "    # y = data.iloc[:, 9:10].values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01ee8741-38e5-4bf0-9d02-02a4ab023b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_root_mean_square_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Normalized Root Mean Square Error (NRMSE) between y_true and y_pred.\n",
    "    If the range of y_true is zero, it normalizes by the number of samples * outputs.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (np.ndarray): Ground truth values, shape (n_samples, n_outputs)\n",
    "        y_pred (np.ndarray): Predicted values, shape (n_samples, n_outputs)\n",
    "\n",
    "    Returns:\n",
    "        float: NRMSE value\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Compute RMSE\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    \n",
    "    # Compute range\n",
    "    y_range = np.max(y_true) - np.min(y_true)\n",
    "    \n",
    "    if y_range != 0:\n",
    "        # Normalize by range\n",
    "        return rmse / y_range\n",
    "    else:\n",
    "        # Normalize by n_samples * n_outputs\n",
    "        n_samples, n_outputs = y_true.shape\n",
    "        return np.sqrt(np.sum((y_true - y_pred) ** 2) / (n_samples * n_outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb8b22c9-6e80-4189-abdf-32e194218f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_leaf(leaf_id, indices, X_train, y_train, feature_names, optimizer):\n",
    "    X_leaf = X_train[indices]\n",
    "    y_leaf = y_train[indices]\n",
    "    \n",
    "    # Choose optimizer\n",
    "    if optimizer == \"gurobi\":\n",
    "        M, m0, h = constrained_optimization_gurobi(X_leaf, y_leaf)\n",
    "    elif optimizer == \"gurobi_MSE\":\n",
    "        M, m0, h = constrained_optimization_MSE_gurobi(X_leaf, y_leaf)\n",
    "    elif optimizer == \"least_squares\":\n",
    "        M, m0, h = least_squares_solution(X_leaf, y_leaf)\n",
    "    elif optimizer == \"gurobi_l2\":\n",
    "        # print(\"Optimizing with L2 regularization\")\n",
    "        M, m0, h = constrained_optimization_regularization_gurobi(X_leaf, y_leaf)\n",
    "    elif optimizer == \"gurobi_MSE_l2\":\n",
    "        # print(\"Optimizing with MSE L2 regularization\")\n",
    "        M, m0, h = constrained_optimization_MSE_regularization_gurobi(X_leaf, y_leaf)\n",
    "    else:\n",
    "        M, m0, h = constrained_optimization(X_leaf, y_leaf)\n",
    "    \n",
    "    # Build model info\n",
    "    model = {\n",
    "        \"leaf_id\": leaf_id,\n",
    "        \"CO_Model\": {'M': M, 'm0': m0, 'h': h},\n",
    "        \"no_samples\": len(indices),\n",
    "        \"indices\": indices,\n",
    "        \"bounds\": {\n",
    "            feature_names[i]: (X_leaf[:, i].min(), X_leaf[:, i].max())\n",
    "            for i in range(X_leaf.shape[1])\n",
    "        }\n",
    "    }\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_COF_on_leaves_parallel(X_train, y_train, tree, feature_names=None, optimizer=\"gurobi\", n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Train constrained optimization models on tree leaves in parallel.\n",
    "\n",
    "    Parameters:\n",
    "        X_train, y_train : np.ndarray\n",
    "        tree : fitted sklearn tree\n",
    "        feature_names : list of feature names (optional)\n",
    "        optimizer : {\"gurobi\", \"CVXPY + SCS\"}\n",
    "        n_jobs : number of parallel workers (-1 = all cores)\n",
    "    \"\"\"\n",
    "    leaf_samples = get_leaf_samples(tree, X_train)\n",
    "\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"feature_{i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "    # Run leaf computations in parallel\n",
    "    tree_extracted_info = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_leaf)(leaf_id, indices, X_train, y_train, feature_names, optimizer)\n",
    "        for leaf_id, indices in leaf_samples.items()\n",
    "    )\n",
    "\n",
    "    return tree_extracted_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1b0dcd42-ad5a-452f-937c-42a7f0d7bad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_name = \"navigation_old\"\n",
    "n_samples = 500000\n",
    "X, y = load_dataset(f\"Dataset/{sys_name}/{sys_name}_{n_samples}/data_{sys_name}_{n_samples}.csv\",num_attributes=4, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "22bbd20f-efde-453b-9c99-05cff69ff0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1892 ,  2.4081 , -0.49971, -0.74031],\n",
       "       [ 1.7434 ,  1.0898 , -0.10954, -0.41258],\n",
       "       [ 1.0537 ,  0.13005,  0.93942, -0.14472],\n",
       "       ...,\n",
       "       [ 2.4862 ,  1.4989 , -0.75639,  0.54465],\n",
       "       [ 1.352  ,  0.87072, -0.68432, -0.90102],\n",
       "       [ 2.4397 ,  2.1971 , -0.22278, -0.79128]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "36307310-a932-4e33-b6fb-8fef65c6613c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1451 ,  2.3306 , -0.44092, -0.77409],\n",
       "       [ 1.7419 ,  1.0445 , -0.01462, -0.45312],\n",
       "       [ 1.1482 ,  0.11716,  0.94498, -0.1289 ],\n",
       "       ...,\n",
       "       [ 2.4205 ,  1.5353 , -0.65719,  0.36334],\n",
       "       [ 1.3018 ,  0.78931, -0.50192, -0.81411],\n",
       "       [ 2.4202 ,  2.1154 , -0.19575, -0.81685]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "81f23ada-a13d-4edb-93f6-9c15b54f9348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of X_Training = (449999, 4) \n",
      " Shape of X_Testing = (50000, 4)\n",
      " Shape of Y_Training = (449999, 4) \n",
      " Shape of Y_Testing = (50000, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.1)\n",
    "print(f\" Shape of X_Training = {X_train.shape} \\n Shape of X_Testing = {X_test.shape}\")\n",
    "print(f\" Shape of Y_Training = {y_train.shape} \\n Shape of Y_Testing = {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "764cf700-1483-4d95-989b-8caca1b2fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_prune_COF_tree_v3(\n",
    "    X_train, y_train, X_test=None, y_test=None,\n",
    "    initial_tree_params=None, optimizer=\"gurobi\",\n",
    "    alpha=1e-6, h_min=0, ignore_h=False, n_jobs=-1\n",
    "):\n",
    "    \"\"\"\n",
    "    Train, build COF models, and prune tree iteratively with h_min tweak.\n",
    "    \"\"\"\n",
    "    # 1️⃣ Train initial tree\n",
    "    if initial_tree_params is None:\n",
    "        initial_tree_params = {\"max_depth\": 5}\n",
    "    tree = DecisionTreeRegressor(**initial_tree_params)\n",
    "    tree.fit(X_train, y_train)\n",
    "\n",
    "    # 2️⃣ Build COF models for leaves\n",
    "    COF_model_tree = train_COF_on_leaves_parallel(X_train, y_train, tree, optimizer=optimizer, n_jobs=n_jobs)\n",
    "\n",
    "    # 3️⃣ Leaf info mapping\n",
    "    children_left = tree.tree_.children_left\n",
    "    children_right = tree.tree_.children_right\n",
    "    leaf_nodes = np.where(children_left == -1)[0]\n",
    "    \n",
    "    leaf_h_dict = {leaf: COF_model_tree[i]['CO_Model']['h'] for i, leaf in enumerate(leaf_nodes)}\n",
    "    leaf_indices_dict = {leaf: COF_model_tree[i]['indices'] for i, leaf in enumerate(leaf_nodes)}\n",
    "    leaf_COFS_dict = {leaf: COF_model_tree[i] for i, leaf in enumerate(leaf_nodes)}\n",
    "\n",
    "    # Helper function to compute h dynamically\n",
    "    def compute_h(indices):\n",
    "        if ignore_h:\n",
    "            return 0\n",
    "        if optimizer == \"gurobi\":\n",
    "            _, _, h = constrained_optimization_gurobi(X_train[indices], y_train[indices])\n",
    "        elif optimizer == \"gurobi_MSE\":\n",
    "            _, _, h = constrained_optimization_MSE_gurobi(X_train[indices], y_train[indices])\n",
    "        else:\n",
    "            _, _, h = constrained_optimization(X_train[indices], y_train[indices])\n",
    "        return h\n",
    "\n",
    "    # -------------------------------\n",
    "    # 4️⃣ Recursive pruning function\n",
    "    # -------------------------------\n",
    "    def prune_node(node):\n",
    "        left = children_left[node]\n",
    "        right = children_right[node]\n",
    "\n",
    "        if left == -1 and right == -1:\n",
    "            # Leaf node\n",
    "            h_leaf = leaf_h_dict[node]\n",
    "            return h_leaf, 1, leaf_indices_dict[node]\n",
    "\n",
    "        # Evaluate left/right subtrees in parallel\n",
    "        results = Parallel(n_jobs=2)(\n",
    "            delayed(prune_node)(child) for child in [left, right]\n",
    "        )\n",
    "        (left_cost, left_leaves, left_indices), (right_cost, right_leaves, right_indices) = results\n",
    "\n",
    "        combined_indices = np.concatenate([left_indices, right_indices])\n",
    "        subtree_cost = left_cost + right_cost\n",
    "        subtree_leaves = left_leaves + right_leaves\n",
    "\n",
    "        # Compute parent h\n",
    "        h_parent = compute_h(combined_indices)\n",
    "        prune_cost = h_parent + alpha\n",
    "        prune_leaves = 1\n",
    "\n",
    "        # --- Pruning decision ---\n",
    "        prune_flag = False\n",
    "        if not ignore_h and h_parent < h_min:\n",
    "            prune_flag = True\n",
    "        elif prune_cost <= subtree_cost:\n",
    "            prune_flag = True\n",
    "\n",
    "        if prune_flag:\n",
    "            # Print pruning info\n",
    "            print(f\"Pruning triggered on node {node}, parent_h={h_parent:.6f}, alpha={alpha}\")\n",
    "\n",
    "            # Prune children\n",
    "            children_left[node] = -1\n",
    "            children_right[node] = -1\n",
    "\n",
    "            # Remove pruned child leaves from COF dicts\n",
    "            for child in [left, right]:\n",
    "                if child in leaf_COFS_dict:\n",
    "                    del leaf_COFS_dict[child]\n",
    "                if child in leaf_h_dict:\n",
    "                    del leaf_h_dict[child]\n",
    "                if child in leaf_indices_dict:\n",
    "                    del leaf_indices_dict[child]\n",
    "\n",
    "            # Update leaf info for parent\n",
    "            tree.tree_.value[node] = np.array([[h_parent]])\n",
    "            leaf_h_dict[node] = h_parent\n",
    "            leaf_indices_dict[node] = combined_indices\n",
    "            leaf_COFS_dict[node] = {\n",
    "                \"leaf_id\": node,\n",
    "                \"CO_Model\": {\"h\": h_parent},\n",
    "                \"indices\": combined_indices,\n",
    "                \"no_samples\": len(combined_indices)\n",
    "            }\n",
    "            return prune_cost, prune_leaves, combined_indices\n",
    "        else:\n",
    "            return subtree_cost, subtree_leaves, combined_indices\n",
    "\n",
    "    # -------------------------------\n",
    "    # 5️⃣ Stats before pruning\n",
    "    # -------------------------------\n",
    "    h_values_before = list(leaf_h_dict.values())\n",
    "    num_leaves_before = len(leaf_nodes)\n",
    "    if X_test is not None and y_test is not None:\n",
    "        nrmse_train_before = normalized_root_mean_square_error(y_train, tree.predict(X_train))\n",
    "        nrmse_test_before = normalized_root_mean_square_error(y_test, tree.predict(X_test))\n",
    "        nrmse_train_COF_before = normalized_root_mean_square_error(y_train, predict_from_COF(COF_model_tree, X_train, tree))\n",
    "        nrmse_test_COF_before = normalized_root_mean_square_error(y_test, predict_from_COF(COF_model_tree, X_test, tree))\n",
    "        print(f\"Before pruning: Leaves={num_leaves_before}, NRMSE[Tree]={nrmse_train_before:.4f}/{nrmse_test_before:.4f}, \"\n",
    "              f\"NRMSE[COF]={nrmse_train_COF_before:.4f}/{nrmse_test_COF_before:.4f}\")\n",
    "        print(f\"h values before pruning: {h_values_before}\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # 6️⃣ Iterative pruning until no changes\n",
    "    # -------------------------------\n",
    "    previous_leaf_count = -1\n",
    "    while True:\n",
    "        prune_node(0)\n",
    "        COF_model_tree_pruned = list(leaf_COFS_dict.values())\n",
    "        current_leaf_count = len(COF_model_tree_pruned)\n",
    "        if current_leaf_count == previous_leaf_count:\n",
    "            break\n",
    "        previous_leaf_count = current_leaf_count\n",
    "\n",
    "    # -------------------------------\n",
    "    # 7️⃣ Stats after pruning\n",
    "    # -------------------------------\n",
    "    leaf_h_dict_after = {leaf['leaf_id']: leaf['CO_Model']['h'] for leaf in COF_model_tree_pruned}\n",
    "    h_values_after = list(leaf_h_dict_after.values())\n",
    "    num_leaves_after = len(COF_model_tree_pruned)\n",
    "    if X_test is not None and y_test is not None:\n",
    "        nrmse_train_after = normalized_root_mean_square_error(y_train, tree.predict(X_train))\n",
    "        nrmse_test_after = normalized_root_mean_square_error(y_test, tree.predict(X_test))\n",
    "        nrmse_train_COF_after = normalized_root_mean_square_error(y_train, predict_from_COF(COF_model_tree_pruned, X_train, tree))\n",
    "        nrmse_test_COF_after = normalized_root_mean_square_error(y_test, predict_from_COF(COF_model_tree_pruned, X_test, tree))\n",
    "        print(f\"After pruning: Leaves={num_leaves_after}, NRMSE[Tree]={nrmse_train_after:.4f}/{nrmse_test_after:.4f}, \"\n",
    "              f\"NRMSE[COF]={nrmse_train_COF_after:.4f}/{nrmse_test_COF_after:.4f}\")\n",
    "        print(f\"h values after pruning: {h_values_after}\")\n",
    "\n",
    "    return tree, COF_model_tree_pruned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d5a891b-e3e8-43e6-9cd1-454c0aa19555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning triggered on node 53, h_parent=0.000154\n",
      "Pruning triggered on node 45, h_parent=0.000038\n",
      "Pruning triggered on node 53, h_parent=0.000154\n",
      "Pruning triggered on node 45, h_parent=0.000038\n",
      "Pruning triggered on node 22, h_parent=0.001071\n",
      "Pruning triggered on node 22, h_parent=0.001071\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Before pruning: Leaves=32, NRMSE[Tree]=0.0687/0.0686, NRMSE[COF]=0.0034/0.0034\n",
      "h values before pruning: [15.247495125548335, 15.970116469724417, 9.896257219805027e-05, 13.297375726471527, 10.063166303461312, 19.84039996511953, 0.0002915225348566648, 26.351826120993845, 19.731365798253936, 27.784298026201956, 0.020334491560324075, 2.8069198481649876e-05, 11.76965344336756, 12.840628849843235, 19.50031001173883, 12.95491713564067, 26.714420813368676, 19.631612981318483, 2.882897122440768e-05, 12.976188609804481, 15.59830223027343, 5.081453184122862e-05, 3.616854578928122e-05, 5.152554209046204e-05, 0.00012612581272515916, 12.577742652167915, 1.3682289603790343e-05, 3.816671606296249e-05, 26.894228586387467, 12.080145465898758, 0.00013950234297040176, 15.719675279783754]\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "After pruning: Leaves=32, NRMSE[Tree]=0.0687/0.0686, NRMSE[COF]=0.0034/0.0034\n",
      "h values after pruning: [15.247495125548335, 15.970116469724417, 9.896257219805027e-05, 13.297375726471527, 10.063166303461312, 19.84039996511953, 0.0002915225348566648, 26.351826120993845, 19.731365798253936, 27.784298026201956, 0.020334491560324075, 2.8069198481649876e-05, 11.76965344336756, 12.840628849843235, 19.50031001173883, 12.95491713564067, 26.714420813368676, 19.631612981318483, 2.882897122440768e-05, 12.976188609804481, 15.59830223027343, 5.081453184122862e-05, 3.616854578928122e-05, 5.152554209046204e-05, 0.00012612581272515916, 12.577742652167915, 1.3682289603790343e-05, 3.816671606296249e-05, 26.894228586387467, 12.080145465898758, 0.00013950234297040176, 15.719675279783754]\n"
     ]
    }
   ],
   "source": [
    "tree, COF_model =train_and_prune_COF_tree_v3(X_train, y_train, X_test=X_test, y_test=y_test, \n",
    "                                initial_tree_params=None, optimizer=\"gurobi\", \n",
    "                                alpha=2, h_min=1, ignore_h=False, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76b76037-01d1-459d-b2da-91d442a34289",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_and_prune_COF_tree_v4(\n",
    "    X_train, y_train, X_test=None, y_test=None,\n",
    "    initial_tree_params=None, optimizer=\"gurobi\",\n",
    "    alpha=1e-6, h_min=0, ignore_h=False, n_jobs=-1\n",
    "):\n",
    "    \"\"\"\n",
    "    Train COF tree and prune bottom-up in parallel.\n",
    "    \"\"\"\n",
    "    # 1️⃣ Train initial tree\n",
    "    if initial_tree_params is None:\n",
    "        initial_tree_params = {\"max_depth\": 5}\n",
    "    tree = DecisionTreeRegressor(**initial_tree_params)\n",
    "    tree.fit(X_train, y_train)\n",
    "\n",
    "    # 2️⃣ Build COF models for leaves\n",
    "    COF_model_tree = train_COF_on_leaves_parallel(\n",
    "        X_train, y_train, tree, optimizer=optimizer, n_jobs=n_jobs\n",
    "    )\n",
    "\n",
    "    # 3️⃣ Leaf info mapping\n",
    "    children_left = tree.tree_.children_left\n",
    "    children_right = tree.tree_.children_right\n",
    "    leaf_nodes = np.where(children_left == -1)[0]\n",
    "\n",
    "    leaf_h_dict = {leaf: COF_model_tree[i]['CO_Model']['h'] for i, leaf in enumerate(leaf_nodes)}\n",
    "    leaf_indices_dict = {leaf: COF_model_tree[i]['indices'] for i, leaf in enumerate(leaf_nodes)}\n",
    "    leaf_COFS_dict = {leaf: COF_model_tree[i] for i, leaf in enumerate(leaf_nodes)}\n",
    "\n",
    "    # -------------------------------\n",
    "    # 4️⃣ Bottom-up parallel pruning\n",
    "    # -------------------------------\n",
    "    def prune_subtree(node):\n",
    "        left = children_left[node]\n",
    "        right = children_right[node]\n",
    "\n",
    "        if left == -1 and right == -1:\n",
    "            # Leaf node\n",
    "            return node, leaf_h_dict[node], 1, leaf_indices_dict[node]\n",
    "\n",
    "        # Process children in parallel\n",
    "        results = Parallel(n_jobs=2)(\n",
    "            delayed(prune_subtree)(child) for child in [left, right]\n",
    "        )\n",
    "        (left_node, left_h, left_leaves, left_indices), (right_node, right_h, right_leaves, right_indices) = results\n",
    "\n",
    "        combined_indices = np.concatenate([left_indices, right_indices])\n",
    "        subtree_cost = left_h + right_h\n",
    "        subtree_leaves = left_leaves + right_leaves\n",
    "\n",
    "        # Compute parent h\n",
    "        if ignore_h:\n",
    "            h_parent = 0\n",
    "        else:\n",
    "            if optimizer == \"gurobi\":\n",
    "                _, _, h_parent = constrained_optimization_gurobi(X_train[combined_indices], y_train[combined_indices])\n",
    "            elif optimizer == \"gurobi_MSE\":\n",
    "                _, _, h_parent = constrained_optimization_MSE_gurobi(X_train[combined_indices], y_train[combined_indices])\n",
    "            else:\n",
    "                _, _, h_parent = constrained_optimization(X_train[combined_indices], y_train[combined_indices])\n",
    "\n",
    "        prune_cost = h_parent + alpha\n",
    "        prune_flag = (not ignore_h and h_parent < h_min) or (prune_cost <= subtree_cost)\n",
    "\n",
    "        if prune_flag:\n",
    "            # Prune children\n",
    "            children_left[node] = -1\n",
    "            children_right[node] = -1\n",
    "\n",
    "            # Update tree and COF info\n",
    "            tree.tree_.value[node] = np.array([[h_parent]])\n",
    "            leaf_h_dict[node] = h_parent\n",
    "            leaf_indices_dict[node] = combined_indices\n",
    "            leaf_COFS_dict[node] = {\n",
    "                \"leaf_id\": node,\n",
    "                \"CO_Model\": {\"h\": h_parent},\n",
    "                \"indices\": combined_indices,\n",
    "                \"no_samples\": len(combined_indices)\n",
    "            }\n",
    "\n",
    "            # Remove pruned children from COF dict\n",
    "            for ch in [left_node, right_node]:\n",
    "                if ch in leaf_COFS_dict:\n",
    "                    del leaf_COFS_dict[ch]\n",
    "\n",
    "            print(f\"Pruning triggered at node {node} with h={h_parent:.6f}\")\n",
    "            return node, prune_cost, 1, combined_indices\n",
    "        else:\n",
    "            return node, subtree_cost, subtree_leaves, combined_indices\n",
    "\n",
    "    # -------------------------------\n",
    "    # 5️⃣ Stats before pruning\n",
    "    # -------------------------------\n",
    "    h_values_before = list(leaf_h_dict.values())\n",
    "    num_leaves_before = len(leaf_nodes)\n",
    "    if X_test is not None and y_test is not None:\n",
    "        nrmse_train_before = normalized_root_mean_square_error(y_train, tree.predict(X_train))\n",
    "        nrmse_test_before = normalized_root_mean_square_error(y_test, tree.predict(X_test))\n",
    "        nrmse_train_COF_before = normalized_root_mean_square_error(y_train, predict_from_COF(COF_model_tree, X_train, tree))\n",
    "        nrmse_test_COF_before = normalized_root_mean_square_error(y_test, predict_from_COF(COF_model_tree, X_test, tree))\n",
    "        print(f\"Before pruning: Leaves={num_leaves_before}, NRMSE[Tree]={nrmse_train_before:.4f}/{nrmse_test_before:.4f}, \"\n",
    "              f\"NRMSE[COF]={nrmse_train_COF_before:.4f}/{nrmse_test_COF_before:.4f}\")\n",
    "        print(f\"h values before pruning: {h_values_before}\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # 6️⃣ Start bottom-up pruning\n",
    "    # -------------------------------\n",
    "    prune_subtree(0)\n",
    "    COF_model_tree_pruned = list(leaf_COFS_dict.values())\n",
    "\n",
    "    # -------------------------------\n",
    "    # 7️⃣ Stats after pruning\n",
    "    # -------------------------------\n",
    "    leaf_h_dict_after = {leaf['leaf_id']: leaf['CO_Model']['h'] for leaf in COF_model_tree_pruned}\n",
    "    h_values_after = list(leaf_h_dict_after.values())\n",
    "    num_leaves_after = len(COF_model_tree_pruned)\n",
    "    if X_test is not None and y_test is not None:\n",
    "        nrmse_train_after = normalized_root_mean_square_error(y_train, tree.predict(X_train))\n",
    "        nrmse_test_after = normalized_root_mean_square_error(y_test, tree.predict(X_test))\n",
    "        nrmse_train_COF_after = normalized_root_mean_square_error(y_train, predict_from_COF(COF_model_tree_pruned, X_train, tree))\n",
    "        nrmse_test_COF_after = normalized_root_mean_square_error(y_test, predict_from_COF(COF_model_tree_pruned, X_test, tree))\n",
    "        print(f\"After pruning: Leaves={num_leaves_after}, NRMSE[Tree]={nrmse_train_after:.4f}/{nrmse_test_after:.4f}, \"\n",
    "              f\"NRMSE[COF]={nrmse_train_COF_after:.4f}/{nrmse_test_COF_after:.4f}\")\n",
    "        print(f\"h values after pruning: {h_values_after}\")\n",
    "\n",
    "    return tree, COF_model_tree_pruned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "593a29fd-1380-45cc-9c20-04a8b60c4a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning triggered on node 22, parent_h=0.001071, alpha=2\n",
      "Pruning triggered on node 22, parent_h=0.001071, alpha=2\n",
      "Pruning triggered on node 53, parent_h=0.000154, alpha=2\n",
      "Pruning triggered on node 45, parent_h=0.000038, alpha=2\n",
      "Pruning triggered on node 53, parent_h=0.000154, alpha=2\n",
      "Pruning triggered on node 45, parent_h=0.000038, alpha=2\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Before pruning: Leaves=32, NRMSE[Tree]=0.0687/0.0686, NRMSE[COF]=0.0034/0.0034\n",
      "h values before pruning: [15.247495125548335, 15.970116469724417, 9.896257219805027e-05, 13.297375726471527, 10.063166303461312, 19.84039996511953, 0.0002915225348566648, 26.351826120993845, 19.731365798253936, 27.784298026201956, 0.020334491560324075, 2.8069198481649876e-05, 11.76965344336756, 12.840628849843235, 19.50031001173883, 12.95491713564067, 26.714420813368676, 19.631612981318483, 2.882897122440768e-05, 12.976188609804481, 15.59830223027343, 5.081453184122862e-05, 3.616854578928122e-05, 5.152554209046204e-05, 0.00012612581272515916, 12.577742652167915, 1.3682289603790343e-05, 3.816671606296249e-05, 26.894228586387467, 12.080145465898758, 0.00013950234297040176, 15.719675279783754]\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "After pruning: Leaves=32, NRMSE[Tree]=0.0687/0.0686, NRMSE[COF]=0.0034/0.0034\n",
      "h values after pruning: [15.247495125548335, 15.970116469724417, 9.896257219805027e-05, 13.297375726471527, 10.063166303461312, 19.84039996511953, 0.0002915225348566648, 26.351826120993845, 19.731365798253936, 27.784298026201956, 0.020334491560324075, 2.8069198481649876e-05, 11.76965344336756, 12.840628849843235, 19.50031001173883, 12.95491713564067, 26.714420813368676, 19.631612981318483, 2.882897122440768e-05, 12.976188609804481, 15.59830223027343, 5.081453184122862e-05, 3.616854578928122e-05, 5.152554209046204e-05, 0.00012612581272515916, 12.577742652167915, 1.3682289603790343e-05, 3.816671606296249e-05, 26.894228586387467, 12.080145465898758, 0.00013950234297040176, 15.719675279783754]\n",
      "Pruning triggered at node 22 with h=0.001071\n",
      "Pruning triggered at node 53 with h=0.000154\n",
      "Pruning triggered at node 45 with h=0.000038\n"
     ]
    }
   ],
   "source": [
    "tree, COF_model =train_and_prune_COF_tree_v4(X_train, y_train, X_test=X_test, y_test=y_test, \n",
    "                                initial_tree_params=None, optimizer=\"gurobi\", \n",
    "                                alpha=2, h_min=1, ignore_h=False, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdca94e-e062-4211-94ad-05e53be52981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e5f7922a-bf40-45fa-b862-d7f6bd0b4bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Tuple, List, Any\n",
    "\n",
    "# Optional imports for per-leaf optimizers\n",
    "try:\n",
    "    import cvxpy as cp\n",
    "except Exception:\n",
    "    cp = None\n",
    "\n",
    "try:\n",
    "    import gurobipy as gp\n",
    "    from gurobipy import GRB\n",
    "except Exception:\n",
    "    gp = None\n",
    "    GRB = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class _Node:\n",
    "    # Split info\n",
    "    feature: int = -1\n",
    "    threshold: float = 0.0\n",
    "    left: Optional[int] = None\n",
    "    right: Optional[int] = None\n",
    "\n",
    "    # Stats\n",
    "    n_samples: int = 0\n",
    "    sum_y: Optional[np.ndarray] = None       # shape (n_outputs,)\n",
    "    sum_y2: float = 0.0                      # scalar: sum over samples of ||y||^2\n",
    "\n",
    "    # Prediction at leaf\n",
    "    value: Optional[np.ndarray] = None       # mean y, shape (n_outputs,)\n",
    "\n",
    "    # Bookkeeping\n",
    "    depth: int = 0\n",
    "    is_leaf: bool = True\n",
    "    leaf_id: Optional[int] = None\n",
    "\n",
    "    # Cached pruning metric (computed when needed)\n",
    "    _g_alpha: Optional[float] = None\n",
    "\n",
    "\n",
    "class CustomDecisionTreeRegressor:\n",
    "    \"\"\"\n",
    "    A fast, multi-output regression tree with:\n",
    "    - MSE-based splits and stopping when leaf MSE (averaged across targets) ≤ threshold\n",
    "    - max_depth stopping\n",
    "    - post-pruning via cost-complexity (ccp_alpha)\n",
    "    - min_leaf_samples enforced post-fit (does not influence split choices)\n",
    "    - per-leaf constrained optimizers (cvxpy or gurobi) with prediction\n",
    "    - utilities: predict, apply, get_leaf_indices, leaf_bounds (data-driven),\n",
    "                 leaf_box_bounds (path constraints), score (R^2), prune, get_h\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth: Optional[int] = None,\n",
    "        mse_threshold: float = 0.0,              # stop when leaf MSE_avg ≤ threshold\n",
    "        min_leaf_samples: Optional[int] = None,  # enforced post-fit only\n",
    "        ccp_alpha: float = 0.0,                  # default no pruning at fit()\n",
    "        random_state: Optional[int] = None,\n",
    "        min_improvement: float = 0.0             # minimal SSE reduction required to split\n",
    "    ):\n",
    "        self.max_depth = max_depth\n",
    "        self.mse_threshold = float(mse_threshold)\n",
    "        self.min_leaf_samples = int(min_leaf_samples) if min_leaf_samples is not None else 0\n",
    "        self.ccp_alpha = float(ccp_alpha)\n",
    "        self.random_state = random_state\n",
    "        self.min_improvement = float(min_improvement)\n",
    "\n",
    "        # Fitted attributes\n",
    "        self.n_features_in_: Optional[int] = None\n",
    "        self.n_outputs_: Optional[int] = None\n",
    "        self._nodes: List[_Node] = []\n",
    "        self._root: Optional[int] = None\n",
    "        self._fitted_X: Optional[np.ndarray] = None\n",
    "        self._fitted_y: Optional[np.ndarray] = None\n",
    "\n",
    "        # Per-leaf linear models from constrained optimization: leaf_id -> dict(M, m0, h, solver)\n",
    "        self._leaf_models: Dict[int, Dict[str, Any]] = {}\n",
    "\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Public API\n",
    "    # ---------------------------\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        self._validate_X_y(X, y)\n",
    "        n_samples = X.shape[0]\n",
    "        idx = np.arange(n_samples, dtype=np.int64)\n",
    "\n",
    "        self._nodes = []\n",
    "        self._root = self._build_node(X, y, idx, depth=0)\n",
    "        self._assign_leaf_ids()\n",
    "\n",
    "        # Optional pruning immediately after fit if ccp_alpha > 0\n",
    "        if self.ccp_alpha > 0.0:\n",
    "            self.prune(self.ccp_alpha)\n",
    "\n",
    "        # Enforce min_leaf_samples post-fit without affecting split decisions\n",
    "        if self.min_leaf_samples > 0:\n",
    "            self._enforce_min_leaf_samples()\n",
    "\n",
    "        # Re-assign leaf ids after any pruning\n",
    "        self._assign_leaf_ids()\n",
    "\n",
    "        # Cache training data (useful for utilities)\n",
    "        self._fitted_X = X.copy()\n",
    "        self._fitted_y = y.copy()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        self._check_fitted()\n",
    "        X = self._validate_X_only(X)\n",
    "        preds = np.zeros((X.shape[0], self.n_outputs_), dtype=float)\n",
    "        for i in range(X.shape[0]):\n",
    "            node_idx = self._traverse(self._root, X[i])\n",
    "            node = self._nodes[node_idx]\n",
    "            preds[i] = node.value\n",
    "        return preds\n",
    "\n",
    "    def apply(self, X: np.ndarray) -> np.ndarray:\n",
    "        self._check_fitted()\n",
    "        X = self._validate_X_only(X)\n",
    "        leaf_ids = np.empty(X.shape[0], dtype=np.int64)\n",
    "        for i in range(X.shape[0]):\n",
    "            node_idx = self._traverse(self._root, X[i])\n",
    "            leaf_ids[i] = self._nodes[node_idx].leaf_id\n",
    "        return leaf_ids\n",
    "\n",
    "    def get_leaf_indices(self, X: Optional[np.ndarray] = None) -> Dict[int, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Returns mapping leaf_id -> sample indices that fall into that leaf.\n",
    "        If X is None, uses the training data indices.\n",
    "        \"\"\"\n",
    "        self._check_fitted()\n",
    "        if X is None:\n",
    "            X = self._fitted_X\n",
    "            indices = np.arange(X.shape[0], dtype=np.int64)\n",
    "        else:\n",
    "            X = self._validate_X_only(X)\n",
    "            indices = np.arange(X.shape[0], dtype=np.int64)\n",
    "\n",
    "        leaf_ids = self.apply(X)\n",
    "        mapping: Dict[int, List[int]] = {}\n",
    "        for i, lid in enumerate(leaf_ids):\n",
    "            mapping.setdefault(int(lid), []).append(int(indices[i]))\n",
    "        return {lid: np.array(idxs, dtype=np.int64) for lid, idxs in mapping.items()}\n",
    "\n",
    "    def leaf_bounds(self, X: Optional[np.ndarray] = None) -> Dict[int, Dict[str, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Data-driven bounds: for each leaf, returns min/max for each feature among samples in that leaf.\n",
    "        \"\"\"\n",
    "        self._check_fitted()\n",
    "        if X is None:\n",
    "            X = self._fitted_X\n",
    "        X = self._validate_X_only(X)\n",
    "\n",
    "        mapping = self.get_leaf_indices(X)\n",
    "        bounds = {}\n",
    "        for lid, idxs in mapping.items():\n",
    "            Xi = X[idxs]\n",
    "            bounds[lid] = {\n",
    "                \"min\": Xi.min(axis=0),\n",
    "                \"max\": Xi.max(axis=0),\n",
    "            }\n",
    "        return bounds\n",
    "\n",
    "    def leaf_box_bounds(self, X: Optional[np.ndarray] = None) -> Dict[int, Dict[str, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Returns path-based box constraints for each leaf, refined by actual data bounds.\n",
    "        \"\"\"\n",
    "        self._check_fitted()\n",
    "        if X is None:\n",
    "            X = self._fitted_X\n",
    "        X = self._validate_X_only(X)\n",
    "        leaf_indices = self.get_leaf_indices(X)\n",
    "    \n",
    "        bounds = {}\n",
    "    \n",
    "        def dfs(nid: int, path: List[Tuple[int, float, str]]):\n",
    "            node = self._nodes[nid]\n",
    "            if node.is_leaf:\n",
    "                lid = node.leaf_id\n",
    "                idxs = leaf_indices.get(lid, [])\n",
    "                if len(idxs) == 0:\n",
    "                    return\n",
    "                X_leaf = X[idxs]\n",
    "                lower = X_leaf.min(axis=0)\n",
    "                upper = X_leaf.max(axis=0)\n",
    "                for f, thr, direction in path:\n",
    "                    if direction == \"left\":\n",
    "                        upper[f] = min(upper[f], thr)\n",
    "                    else:\n",
    "                        lower[f] = max(lower[f], np.nextafter(thr, np.inf))\n",
    "                bounds[lid] = {\"lower\": lower, \"upper\": upper}\n",
    "                return\n",
    "            dfs(node.left, path + [(node.feature, node.threshold, \"left\")])\n",
    "            dfs(node.right, path + [(node.feature, node.threshold, \"right\")])\n",
    "    \n",
    "        dfs(self._root, [])\n",
    "        return bounds\n",
    "\n",
    "    def score(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Multi-output R^2 averaged across outputs.\n",
    "        \"\"\"\n",
    "        self._check_fitted()\n",
    "        X = self._validate_X_only(X)\n",
    "        y = self._validate_y_only(y)\n",
    "        y_pred = self.predict(X)\n",
    "        y_mean = y.mean(axis=0)\n",
    "        ss_res = ((y - y_pred) ** 2).sum(axis=0)\n",
    "        ss_tot = ((y - y_mean) ** 2).sum(axis=0)\n",
    "        valid = ss_tot > 0\n",
    "        if not np.any(valid):\n",
    "            return 1.0\n",
    "        r2_per_output = np.ones(self.n_outputs_, dtype=float)\n",
    "        r2_per_output[valid] = 1.0 - (ss_res[valid] / ss_tot[valid])\n",
    "        return float(r2_per_output.mean())\n",
    "\n",
    "    def prune(self, ccp_alpha: float):\n",
    "        \"\"\"\n",
    "        Cost-complexity post-pruning. Prunes all internal nodes whose g_alpha ≤ ccp_alpha.\n",
    "        \"\"\"\n",
    "        self._check_fitted()\n",
    "        alpha = float(ccp_alpha)\n",
    "        if alpha <= 0.0:\n",
    "            return\n",
    "\n",
    "        changed = True\n",
    "        while changed:\n",
    "            _, _, prunable = self._compute_ccp_alphas(self._root)\n",
    "            to_prune = [nid for (nid, g) in prunable if g <= alpha]\n",
    "            changed = len(to_prune) > 0\n",
    "            for nid in to_prune:\n",
    "                self._prune_subtree_to_leaf(nid)\n",
    "        self._assign_leaf_ids()\n",
    "\n",
    "    def fit_leaf_optimizers(\n",
    "        self,\n",
    "        X: Optional[np.ndarray] = None,\n",
    "        y: Optional[np.ndarray] = None,\n",
    "        optimizer: str = \"cvxpy\",  # \"cvxpy\" or \"gurobi\"\n",
    "        gurobi_params: Optional[Dict[str, Any]] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train constrained optimization model on each leaf using samples that reach that leaf.\n",
    "        Stores per-leaf M, m0, and h. Use predict_with_optimizers() to predict.\n",
    "        \"\"\"\n",
    "        self._check_fitted()\n",
    "        if X is None:\n",
    "            X = self._fitted_X\n",
    "        if y is None:\n",
    "            y = self._fitted_y\n",
    "        X = self._validate_X_only(X)\n",
    "        y = self._validate_y_only(y)\n",
    "\n",
    "        leaf_to_indices = self.get_leaf_indices(X)\n",
    "        self._leaf_models = {}\n",
    "\n",
    "        for lid, idxs in leaf_to_indices.items():\n",
    "            if idxs.size == 0:\n",
    "                continue\n",
    "            X_leaf = X[idxs]\n",
    "            y_leaf = y[idxs]\n",
    "            if optimizer.lower() == \"cvxpy\":\n",
    "                if cp is None:\n",
    "                    raise ImportError(\"cvxpy is not available. Install cvxpy or choose optimizer='gurobi'.\")\n",
    "                M, m0, h = self._constrained_optimization_cvxpy(X_leaf, y_leaf)\n",
    "                solver_name = \"cvxpy\"\n",
    "            elif optimizer.lower() == \"gurobi\":\n",
    "                if gp is None or GRB is None:\n",
    "                    raise ImportError(\"gurobi is not available. Install gurobipy or choose optimizer='cvxpy'.\")\n",
    "                M, m0, h = self._constrained_optimization_gurobi(X_leaf, y_leaf, gurobi_params)\n",
    "                solver_name = \"gurobi\"\n",
    "            else:\n",
    "                raise ValueError(\"optimizer must be 'cvxpy' or 'gurobi'.\")\n",
    "\n",
    "            self._leaf_models[int(lid)] = {\"M\": M, \"m0\": m0, \"h\": float(h), \"solver\": solver_name}\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_with_optimizers(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict using per-leaf linear models (M, m0). Falls back to tree mean if a leaf has no model.\n",
    "        \"\"\"\n",
    "        self._check_fitted()\n",
    "        X = self._validate_X_only(X)\n",
    "        y_pred = np.zeros((X.shape[0], self.n_outputs_), dtype=float)\n",
    "        leaf_ids = self.apply(X)\n",
    "        for i, lid in enumerate(leaf_ids):\n",
    "            mdl = self._leaf_models.get(int(lid))\n",
    "            if mdl is not None and mdl[\"M\"] is not None and mdl[\"m0\"] is not None:\n",
    "                y_pred[i] = X[i] @ mdl[\"M\"].T + mdl[\"m0\"]\n",
    "            else:\n",
    "                node_idx = self._leaf_node_from_leaf_id(int(lid))\n",
    "                y_pred[i] = self._nodes[node_idx].value\n",
    "        return y_pred\n",
    "\n",
    "    def get_h(self) -> Dict[int, float]:\n",
    "        \"\"\"\n",
    "        Returns mapping leaf_id -> h from the constrained optimization.\n",
    "        \"\"\"\n",
    "        return {int(lid): float(v[\"h\"]) for lid, v in self._leaf_models.items() if \"h\" in v}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Building and splitting\n",
    "    # ---------------------------\n",
    "\n",
    "    def _build_node(self, X: np.ndarray, y: np.ndarray, idx: np.ndarray, depth: int) -> int:\n",
    "        node_id = len(self._nodes)\n",
    "        node = _Node(depth=depth)\n",
    "        self._nodes.append(node)\n",
    "\n",
    "        # Stats for this node\n",
    "        Y = y[idx]\n",
    "        n_node = Y.shape[0]\n",
    "        sum_y = Y.sum(axis=0)\n",
    "        sum_y2 = float((Y ** 2).sum())\n",
    "        node.n_samples = n_node\n",
    "        node.sum_y = sum_y\n",
    "        node.sum_y2 = sum_y2\n",
    "        node.value = sum_y / max(n_node, 1)\n",
    "    \n",
    "        # Compute NRMSE for stopping\n",
    "        y_pred = np.tile(node.value, (n_node, 1))\n",
    "        y_range = np.max(Y) - np.min(Y)\n",
    "        if y_range != 0:\n",
    "            nrmse = np.sqrt(np.mean((Y - y_pred) ** 2)) / y_range\n",
    "        else:\n",
    "            nrmse = np.sqrt(np.sum((Y - y_pred) ** 2) / (n_node * self.n_outputs_))\n",
    "    \n",
    "        stop_by_depth = (self.max_depth is not None and depth >= self.max_depth)\n",
    "        if n_node <= 1 or stop_by_depth or nrmse <= self.mse_threshold:\n",
    "            node.is_leaf = True\n",
    "            return node_id\n",
    "        \n",
    "        # Find best split\n",
    "        best = self._best_split(X, y, idx, sum_y, sum_y2, n_node)\n",
    "        if best is None:\n",
    "            node.is_leaf = True\n",
    "            return node_id\n",
    "\n",
    "        feat, thr, left_idx, right_idx, sse_left, sse_right = best\n",
    "        parent_sse = sse\n",
    "        gain = parent_sse - (sse_left + sse_right)\n",
    "        if gain <= self.min_improvement:\n",
    "            node.is_leaf = True\n",
    "            return node_id\n",
    "\n",
    "        # Create children\n",
    "        node.is_leaf = False\n",
    "        node.feature = int(feat)\n",
    "        node.threshold = float(thr)\n",
    "        node.left = self._build_node(X, y, left_idx, depth + 1)\n",
    "        node.right = self._build_node(X, y, right_idx, depth + 1)\n",
    "        return node_id\n",
    "\n",
    "    def _best_split(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        idx: np.ndarray,\n",
    "        sum_y: np.ndarray,\n",
    "        sum_y2: float,\n",
    "        n_node: int\n",
    "    ):\n",
    "        X_node = X[idx]                                  # (n_node, n_features)\n",
    "        Y_node = y[idx]                                  # (n_node, n_outputs)\n",
    "        total_sum_y = sum_y\n",
    "        total_sum_y2 = sum_y2\n",
    "\n",
    "        best_feat = None\n",
    "        best_thr = None\n",
    "        best_sse_left = None\n",
    "        best_sse_right = None\n",
    "        best_left_idx = None\n",
    "        best_right_idx = None\n",
    "        parent_sse = self._sse(total_sum_y, total_sum_y2, n_node)\n",
    "\n",
    "        # For each feature, compute optimal split via cumulative sums\n",
    "        for f in range(self.n_features_in_):\n",
    "            x = X_node[:, f]\n",
    "            order = np.argsort(x, kind='mergesort')      # stable sort\n",
    "            x_sorted = x[order]\n",
    "            Y_sorted = Y_node[order]                     # (n_node, n_outputs)\n",
    "\n",
    "            # Valid split positions where feature value changes\n",
    "            diffs = x_sorted[1:] - x_sorted[:-1]\n",
    "            valid = diffs != 0.0\n",
    "            if not np.any(valid):\n",
    "                continue\n",
    "\n",
    "            # Cumulative sums for SSE computations\n",
    "            csum_y = np.cumsum(Y_sorted, axis=0)                       # (n_node, n_outputs)\n",
    "            row_sq = np.einsum('ij,ij->i', Y_sorted, Y_sorted)         # (n_node,)\n",
    "            csum_y2 = np.cumsum(row_sq)                                 # (n_node,)\n",
    "\n",
    "            split_positions = np.nonzero(valid)[0]  # indices i where split is between i and i+1\n",
    "\n",
    "            left_n = (split_positions + 1).astype(np.int64)\n",
    "            right_n = n_node - left_n\n",
    "\n",
    "            left_sum_y = csum_y[split_positions]                        # (m, n_outputs)\n",
    "            right_sum_y = total_sum_y - left_sum_y                      # (m, n_outputs)\n",
    "            left_sum_y2 = csum_y2[split_positions]                      # (m,)\n",
    "            right_sum_y2 = total_sum_y2 - left_sum_y2                   # (m,)\n",
    "\n",
    "            # SSE for left/right: SSE = sum(y^2) - sum(y)^2 / n\n",
    "            left_sse = left_sum_y2 - np.sum(left_sum_y ** 2, axis=1) / left_n\n",
    "            right_sse = right_sum_y2 - np.sum(right_sum_y ** 2, axis=1) / right_n\n",
    "            total_sse_after = left_sse + right_sse\n",
    "\n",
    "            # Best position for this feature\n",
    "            best_pos = int(np.argmin(total_sse_after))\n",
    "            candidate_sse = float(total_sse_after[best_pos])\n",
    "            if candidate_sse >= parent_sse:\n",
    "                continue  # no gain\n",
    "\n",
    "            # Candidate threshold: midpoint between two adjacent values\n",
    "            i = split_positions[best_pos]\n",
    "            thr = 0.5 * (x_sorted[i] + x_sorted[i + 1])\n",
    "\n",
    "            # Derive indices for children (on original node data)\n",
    "            mask_left = x <= thr\n",
    "            left_idx = idx[mask_left]\n",
    "            right_idx = idx[~mask_left]\n",
    "            if left_idx.size == 0 or right_idx.size == 0:\n",
    "                continue\n",
    "\n",
    "            # Store if overall best\n",
    "            if best_thr is None or candidate_sse < (best_sse_left + best_sse_right):\n",
    "                best_feat = f\n",
    "                best_thr = thr\n",
    "                best_sse_left = float(left_sse[best_pos])\n",
    "                best_sse_right = float(right_sse[best_pos])\n",
    "                best_left_idx = left_idx\n",
    "                best_right_idx = right_idx\n",
    "\n",
    "        if best_thr is None:\n",
    "            return None\n",
    "        return best_feat, best_thr, best_left_idx, best_right_idx, best_sse_left, best_sse_right\n",
    "\n",
    "    # ---------------------------\n",
    "    # Pruning helpers (cost-complexity)\n",
    "    # ---------------------------\n",
    "\n",
    "    def _compute_ccp_alphas(self, nid: int) -> Tuple[float, int, List[Tuple[int, float]]]:\n",
    "        node = self._nodes[nid]\n",
    "        prunable: List[Tuple[int, float]] = []\n",
    "        if node.is_leaf:\n",
    "            R_T = self._node_impurity_as_leaf(node)\n",
    "            return R_T, 1, prunable\n",
    "\n",
    "        # Recurse\n",
    "        R_left, L_left, P_left = self._compute_ccp_alphas(node.left)\n",
    "        R_right, L_right, P_right = self._compute_ccp_alphas(node.right)\n",
    "        prunable.extend(P_left)\n",
    "        prunable.extend(P_right)\n",
    "\n",
    "        R_T = R_left + R_right\n",
    "        L_T = L_left + L_right\n",
    "\n",
    "        # If we collapse node to a leaf:\n",
    "        R_t = self._node_impurity_as_leaf(node)\n",
    "        g = (R_t - R_T) / (L_T - 1.0) if L_T > 1 else np.inf\n",
    "        node._g_alpha = g\n",
    "        prunable.append((nid, g))\n",
    "        return R_T, L_T, prunable\n",
    "\n",
    "    def _prune_subtree_to_leaf(self, nid: int):\n",
    "        node = self._nodes[nid]\n",
    "        node.is_leaf = True\n",
    "        node.left = None\n",
    "        node.right = None\n",
    "        node.feature = -1\n",
    "        node.threshold = 0.0\n",
    "        node._g_alpha = None\n",
    "\n",
    "    def _node_impurity_as_leaf(self, node: _Node) -> float:\n",
    "        n = node.n_samples\n",
    "        if n == 0:\n",
    "            return 0.0\n",
    "        sse = self._sse(node.sum_y, node.sum_y2, n)\n",
    "        # Use SSE (not averaged) to maintain additivity across subtree\n",
    "        return sse\n",
    "\n",
    "    def _enforce_min_leaf_samples(self):\n",
    "        if self.min_leaf_samples <= 0:\n",
    "            return\n",
    "\n",
    "        def postorder(nid: int) -> int:\n",
    "            node = self._nodes[nid]\n",
    "            if node.is_leaf:\n",
    "                return node.n_samples\n",
    "            ln = postorder(node.left)\n",
    "            rn = postorder(node.right)\n",
    "\n",
    "            left_node = self._nodes[node.left]\n",
    "            right_node = self._nodes[node.right]\n",
    "            need_prune = False\n",
    "            if left_node.is_leaf and left_node.n_samples < self.min_leaf_samples:\n",
    "                need_prune = True\n",
    "            if right_node.is_leaf and right_node.n_samples < self.min_leaf_samples:\n",
    "                need_prune = True\n",
    "\n",
    "            if need_prune:\n",
    "                self._prune_subtree_to_leaf(nid)\n",
    "                return node.n_samples\n",
    "            return ln + rn\n",
    "\n",
    "        postorder(self._root)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Per-leaf constrained optimization\n",
    "    # ---------------------------\n",
    "\n",
    "    def _constrained_optimization_cvxpy(self, X_leaf: np.ndarray, y_leaf: np.ndarray):\n",
    "        n_samples, n_features = X_leaf.shape\n",
    "        n_outputs = y_leaf.shape[1]\n",
    "\n",
    "        M = cp.Variable((n_outputs, n_features))\n",
    "        m0 = cp.Variable((n_outputs,))\n",
    "        h = cp.Variable(nonneg=True)\n",
    "\n",
    "        prediction = X_leaf @ M.T + m0\n",
    "        constraint_expr = cp.sum_squares(prediction - y_leaf)\n",
    "        objective = cp.Minimize(h)\n",
    "        constraints = [constraint_expr <= h]\n",
    "\n",
    "        prob = cp.Problem(objective, constraints)\n",
    "        try:\n",
    "            prob.solve(solver=cp.SCS, verbose=False)\n",
    "        except Exception:\n",
    "            prob.solve(verbose=False)\n",
    "\n",
    "        Mv = M.value if M.value is not None else None\n",
    "        m0v = m0.value if m0.value is not None else None\n",
    "        hv = h.value if h.value is not None else np.inf\n",
    "        return Mv, m0v, float(hv)\n",
    "\n",
    "    def _constrained_optimization_gurobi(\n",
    "        self,\n",
    "        X_leaf: np.ndarray,\n",
    "        y_leaf: np.ndarray,\n",
    "        gurobi_params: Optional[Dict[str, Any]] = None\n",
    "    ):\n",
    "        n_samples, n_features = X_leaf.shape\n",
    "        n_outputs = y_leaf.shape[1]\n",
    "\n",
    "        model = gp.Model(\"constrained_optimization\")\n",
    "        model.setParam(\"OutputFlag\", 0)\n",
    "        if gurobi_params:\n",
    "            for k, v in gurobi_params.items():\n",
    "                model.setParam(k, v)\n",
    "\n",
    "        # Decision variables\n",
    "        M = model.addVars(n_outputs, n_features, lb=-GRB.INFINITY, name=\"M\")\n",
    "        m0 = model.addVars(n_outputs, lb=-GRB.INFINITY, name=\"m0\")\n",
    "        h = model.addVar(lb=0.0, name=\"h\")\n",
    "\n",
    "        # Residuals squared sum\n",
    "        quad_expr = gp.QuadExpr()\n",
    "        for i in range(n_samples):\n",
    "            for k in range(n_outputs):\n",
    "                expr = m0[k]\n",
    "                for j in range(n_features):\n",
    "                    expr = expr + M[k, j] * float(X_leaf[i, j])\n",
    "                diff = expr - float(y_leaf[i, k])\n",
    "                quad_expr.add(diff * diff)\n",
    "\n",
    "        model.addQConstr(quad_expr <= h, name=\"residual_bound\")\n",
    "        model.setObjective(h, GRB.MINIMIZE)\n",
    "        model.optimize()\n",
    "\n",
    "        if model.status in [GRB.OPTIMAL, GRB.SUBOPTIMAL] or model.SolCount > 0:\n",
    "            M_val = np.array([[M[k, j].X for j in range(n_features)] for k in range(n_outputs)], dtype=float)\n",
    "            m0_val = np.array([m0[k].X for k in range(n_outputs)], dtype=float)\n",
    "            h_val = float(h.X)\n",
    "        else:\n",
    "            M_val, m0_val, h_val = None, None, np.inf\n",
    "\n",
    "        return M_val, m0_val, h_val\n",
    "\n",
    "    # ---------------------------\n",
    "    # Traversal and utilities\n",
    "    # ---------------------------\n",
    "\n",
    "    def _assign_leaf_ids(self):\n",
    "        counter = 0\n",
    "\n",
    "        def dfs(nid: int):\n",
    "            nonlocal counter\n",
    "            node = self._nodes[nid]\n",
    "            if node.is_leaf:\n",
    "                node.leaf_id = counter\n",
    "                counter += 1\n",
    "            else:\n",
    "                dfs(node.left)\n",
    "                dfs(node.right)\n",
    "\n",
    "        dfs(self._root)\n",
    "\n",
    "    def _leaf_node_from_leaf_id(self, leaf_id: int) -> int:\n",
    "        for i, node in enumerate(self._nodes):\n",
    "            if node.is_leaf and node.leaf_id == leaf_id:\n",
    "                return i\n",
    "        raise KeyError(f\"Leaf id {leaf_id} not found.\")\n",
    "\n",
    "    def _traverse(self, nid: int, x: np.ndarray) -> int:\n",
    "        node = self._nodes[nid]\n",
    "        while not node.is_leaf:\n",
    "            if x[node.feature] <= node.threshold:\n",
    "                nid = node.left\n",
    "            else:\n",
    "                nid = node.right\n",
    "            node = self._nodes[nid]\n",
    "        return nid\n",
    "\n",
    "    @staticmethod\n",
    "    def _sse(sum_y: np.ndarray, sum_y2: float, n: int) -> float:\n",
    "        if n <= 0:\n",
    "            return 0.0\n",
    "        # SSE across outputs = sum(y^2) - sum(y)^2 / n\n",
    "        return float(sum_y2 - float(np.sum(sum_y ** 2)) / n)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Validation and checks\n",
    "    # ---------------------------\n",
    "\n",
    "    def _validate_X_y(self, X: np.ndarray, y: np.ndarray):\n",
    "        if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n",
    "            raise TypeError(\"X and y must be NumPy arrays.\")\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X must be 2D with shape (n_samples, n_features).\")\n",
    "        if y.ndim != 2:\n",
    "            raise ValueError(\"y must be 2D with shape (n_samples, n_targets).\")\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"X and y must have the same number of samples.\")\n",
    "        if not np.issubdtype(X.dtype, np.number) or not np.issubdtype(y.dtype, np.number):\n",
    "            raise TypeError(\"X and y must be numeric.\")\n",
    "        if not np.isfinite(X).all() or not np.isfinite(y).all():\n",
    "            raise ValueError(\"X and y must be finite (no NaNs or infs).\")\n",
    "\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        self.n_outputs_ = y.shape[1]\n",
    "\n",
    "        # Ensure contiguous float arrays\n",
    "        if X.dtype != np.float64:\n",
    "            X = X.astype(np.float64, copy=False)\n",
    "        if y.dtype != np.float64:\n",
    "            y = y.astype(np.float64, copy=False)\n",
    "\n",
    "        # Store back (caller passes references)\n",
    "        # No return needed; callers already hold X, y\n",
    "\n",
    "    def _validate_X_only(self, X: np.ndarray) -> np.ndarray:\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            raise TypeError(\"X must be a NumPy array.\")\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X must be 2D with shape (n_samples, n_features).\")\n",
    "        if self.n_features_in_ is None:\n",
    "            raise RuntimeError(\"Model is not fitted yet.\")\n",
    "        if X.shape[1] != self.n_features_in_:\n",
    "            raise ValueError(f\"X has {X.shape[1]} features, expected {self.n_features_in_}.\")\n",
    "        if not np.issubdtype(X.dtype, np.number):\n",
    "            raise TypeError(\"X must be numeric.\")\n",
    "        if not np.isfinite(X).all():\n",
    "            raise ValueError(\"X must be finite (no NaNs or infs).\")\n",
    "        if X.dtype != np.float64:\n",
    "            X = X.astype(np.float64, copy=False)\n",
    "        return X\n",
    "\n",
    "    def _validate_y_only(self, y: np.ndarray) -> np.ndarray:\n",
    "        if not isinstance(y, np.ndarray):\n",
    "            raise TypeError(\"y must be a NumPy array.\")\n",
    "        if y.ndim != 2:\n",
    "            raise ValueError(\"y must be 2D with shape (n_samples, n_targets).\")\n",
    "        if self.n_outputs_ is None:\n",
    "            raise RuntimeError(\"Model is not fitted yet.\")\n",
    "        if y.shape[1] != self.n_outputs_:\n",
    "            raise ValueError(f\"y has {y.shape[1]} targets, expected {self.n_outputs_}.\")\n",
    "        if not np.issubdtype(y.dtype, np.number):\n",
    "            raise TypeError(\"y must be numeric.\")\n",
    "        if not np.isfinite(y).all():\n",
    "            raise ValueError(\"y must be finite (no NaNs or infs).\")\n",
    "        if y.dtype != np.float64:\n",
    "            y = y.astype(np.float64, copy=False)\n",
    "        return y\n",
    "\n",
    "    def _check_fitted(self):\n",
    "        if self._root is None or self.n_features_in_ is None or self.n_outputs_ is None:\n",
    "            raise RuntimeError(\"Estimator is not fitted yet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "05b846de-cf94-44a5-b207-c40a6131f13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree R² score: -0.0000\n",
      "Optimizer R² score: 0.9968\n",
      "Number of leaves: 1\n",
      "Leaf 0: samples=50000, h=1505.3948\n",
      "  Data bounds: min=[ 1.2628e-05  9.9412e-05 -9.9999e-01 -9.9999e-01], max=[3.      3.      0.99996 1.     ]\n",
      "  Box bounds: lower=[ 9.6247e-06  2.8502e-06 -9.9999e-01 -1.0000e+00], upper=[3.      3.      0.99999 0.99999]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "\n",
    "# Initialize and train the tree\n",
    "tree = CustomDecisionTreeRegressor(mse_threshold=0.5)\n",
    "\n",
    "tree.fit(X_train, y_train) \n",
    "#max_depth=5,\n",
    "#mse_threshold=0.05,\n",
    "#min_leaf_samples=10,\n",
    "#ccp_alpha=0.0,\n",
    "#random_state=42\n",
    "#tree.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate tree predictions\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "r2_tree = r2_score(y_test, y_pred_tree, multioutput='uniform_average')\n",
    "print(f\"Tree R² score: {r2_tree:.4f}\")\n",
    "\n",
    "# Fit constrained optimizers on leaves\n",
    "tree.fit_leaf_optimizers(optimizer=\"gurobi\")\n",
    "\n",
    "# Predict using leaf optimizers\n",
    "y_pred_opt = tree.predict_with_optimizers(X_test)\n",
    "r2_opt = r2_score(y_test, y_pred_opt, multioutput='uniform_average')\n",
    "print(f\"Optimizer R² score: {r2_opt:.4f}\")\n",
    "\n",
    "# Get leaf IDs and bounds\n",
    "leaf_ids = tree.apply(X_test)\n",
    "leaf_indices = tree.get_leaf_indices(X_test)\n",
    "leaf_bounds = tree.leaf_bounds(X_test)\n",
    "leaf_box_bounds = tree.leaf_box_bounds()\n",
    "leaf_h_values = tree.get_h()\n",
    "\n",
    "# Print summary\n",
    "print(f\"Number of leaves: {len(leaf_indices)}\")\n",
    "for lid in sorted(leaf_indices):\n",
    "    print(f\"Leaf {lid}: samples={len(leaf_indices[lid])}, h={leaf_h_values.get(lid, 'N/A'):.4f}\")\n",
    "    print(f\"  Data bounds: min={leaf_bounds[lid]['min']}, max={leaf_bounds[lid]['max']}\")\n",
    "    print(f\"  Box bounds: lower={leaf_box_bounds[lid]['lower']}, upper={leaf_box_bounds[lid]['upper']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0779ec38-7528-408a-bacc-15f5ad424a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h values from constrained optimization per leaf:\n",
      "Leaf 0: h = 1505.394794\n"
     ]
    }
   ],
   "source": [
    "# Get and print h values\n",
    "h_values = tree.get_h()\n",
    "print(\"\\n📊 h values from constrained optimization per leaf:\")\n",
    "for lid in sorted(h_values):\n",
    "    print(f\"Leaf {lid}: h = {h_values[lid]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f634de8-e724-4740-915d-cf6a0aaece10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89395fa-24ca-4381-88ae-d1db74299090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ffb403a-3927-406f-a864-cea280f5a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def bottom_up_tree_debug(tree, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Iterate tree bottom-up, printing node info (supports multi-output).\n",
    "    \"\"\"\n",
    "    children_left = tree.tree_.children_left\n",
    "    children_right = tree.tree_.children_right\n",
    "    node_count = tree.tree_.node_count\n",
    "\n",
    "    # Step 1: assign leaf indices for all samples\n",
    "    sample_leaves = tree.apply(X_train)\n",
    "    node_indices = {i: np.where(sample_leaves == i)[0] for i in range(node_count)}\n",
    "\n",
    "    # Step 2: recursively collect indices for all nodes\n",
    "    def collect_indices(node):\n",
    "        left = children_left[node]\n",
    "        right = children_right[node]\n",
    "\n",
    "        if left == -1 and right == -1:\n",
    "            # Leaf: already stored\n",
    "            return node_indices[node]\n",
    "\n",
    "        # Internal node: collect from children\n",
    "        left_indices = collect_indices(left)\n",
    "        right_indices = collect_indices(right)\n",
    "        combined = np.concatenate([left_indices, right_indices])\n",
    "        node_indices[node] = combined\n",
    "        return combined\n",
    "\n",
    "    collect_indices(0)  # populate all nodes\n",
    "\n",
    "    # Step 3: Bottom-up traversal\n",
    "    def bottom_up(node):\n",
    "        left = children_left[node]\n",
    "        right = children_right[node]\n",
    "\n",
    "        # Process children first\n",
    "        if left != -1:\n",
    "            bottom_up(left)\n",
    "        if right != -1:\n",
    "            bottom_up(right)\n",
    "\n",
    "        indices = node_indices[node]\n",
    "        node_value = tree.tree_.value[node].flatten()  # may be >1D if multi-output\n",
    "\n",
    "        # Compute error properly for multi-output\n",
    "        if len(indices) > 0:\n",
    "            preds = np.tile(node_value, (len(indices), 1))  # repeat value for samples\n",
    "            true_vals = y_train[indices]\n",
    "            error = np.mean(np.sum((true_vals - preds) ** 2, axis=1))\n",
    "        else:\n",
    "            error = np.nan\n",
    "\n",
    "        print(f\"Node index: {node}\")\n",
    "        print(f\"Training indices on node: {indices}\")\n",
    "        print(f\"Node value: {node_value}\")\n",
    "        print(f\"MSE error at node: {error:.4f}\\n\")\n",
    "\n",
    "    bottom_up(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9d8cc65-b761-4d93-8297-c72630500ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node index: 3\n",
      "Training indices on node: [     5      9     12 ... 449972 449994 449998]\n",
      "Node value: [ 0.55406066  0.65800726 -0.3498799  -0.03258353]\n",
      "MSE error at node: 0.6105\n",
      "\n",
      "Node index: 4\n",
      "Training indices on node: [     6      7     17 ... 449990 449993 449997]\n",
      "Node value: [ 0.64185204  0.65272199  0.53929005 -0.02492924]\n",
      "MSE error at node: 0.6070\n",
      "\n",
      "Node index: 2\n",
      "Training indices on node: [     5      9     12 ... 449990 449993 449997]\n",
      "Node value: [ 0.59770144  0.65537997  0.0921233  -0.02877861]\n",
      "MSE error at node: 0.8083\n",
      "\n",
      "Node index: 6\n",
      "Training indices on node: [    11     16     29 ... 449980 449988 449995]\n",
      "Node value: [ 1.73530217  0.772283    0.08735064 -0.49267212]\n",
      "MSE error at node: 0.6963\n",
      "\n",
      "Node index: 7\n",
      "Training indices on node: [     2      3     14 ... 449984 449986 449991]\n",
      "Node value: [1.73734368 0.8609972  0.09517587 0.3965426 ]\n",
      "MSE error at node: 0.7008\n",
      "\n",
      "Node index: 5\n",
      "Training indices on node: [    11     16     29 ... 449984 449986 449991]\n",
      "Node value: [ 1.73632069  0.81654309  0.0912547  -0.04903711]\n",
      "MSE error at node: 0.8982\n",
      "\n",
      "Node index: 1\n",
      "Training indices on node: [     5      9     12 ... 449984 449986 449991]\n",
      "Node value: [ 1.17238319  0.73672192  0.0916849  -0.03900344]\n",
      "MSE error at node: 1.1844\n",
      "\n",
      "Node index: 10\n",
      "Training indices on node: [    13     34     35 ... 449983 449987 449992]\n",
      "Node value: [ 1.26137088  2.10769694  0.08370144 -0.4950596 ]\n",
      "MSE error at node: 0.7171\n",
      "\n",
      "Node index: 11\n",
      "Training indices on node: [     4     19     41 ... 449974 449975 449981]\n",
      "Node value: [1.26651072 2.19485703 0.09257358 0.39060973]\n",
      "MSE error at node: 0.7174\n",
      "\n",
      "Node index: 9\n",
      "Training indices on node: [    13     34     35 ... 449974 449975 449981]\n",
      "Node value: [ 1.26396926  2.1517596   0.08818664 -0.04732086]\n",
      "MSE error at node: 0.9153\n",
      "\n",
      "Node index: 13\n",
      "Training indices on node: [     1      8     25 ... 449979 449982 449985]\n",
      "Node value: [ 2.35754923  2.31422269 -0.41909957 -0.10431474]\n",
      "MSE error at node: 0.6219\n",
      "\n",
      "Node index: 14\n",
      "Training indices on node: [     0     10     15 ... 449962 449989 449996]\n",
      "Node value: [ 2.4415775   2.31611743  0.46952388 -0.0928135 ]\n",
      "MSE error at node: 0.6219\n",
      "\n",
      "Node index: 12\n",
      "Training indices on node: [     1      8     25 ... 449962 449989 449996]\n",
      "Node value: [ 2.39948043  2.31516819  0.02433514 -0.09857547]\n",
      "MSE error at node: 0.8211\n",
      "\n",
      "Node index: 8\n",
      "Training indices on node: [    13     34     35 ... 449962 449989 449996]\n",
      "Node value: [ 1.83383455  2.2337675   0.05614226 -0.07304339]\n",
      "MSE error at node: 1.1987\n",
      "\n",
      "Node index: 0\n",
      "Training indices on node: [     5      9     12 ... 449962 449989 449996]\n",
      "Node value: [ 1.50624636  1.49234571  0.07374499 -0.05618488]\n",
      "MSE error at node: 1.8618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "tree = DecisionTreeRegressor(max_depth=3).fit(X_train, y_train)\n",
    "bottom_up_tree_debug(tree, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59a7ee7-5bc4-4fb1-9599-dfd0a0361a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051eb6e-e222-4a23-994e-c12b96dbac0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cd1cb0-638b-4047-9ddf-4b488ec949cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcadbd9a-14b3-4e93-b1e3-e3ae4e76800a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236cf8d-5923-4b60-a257-f7723d0c6c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef4f721-deff-400a-a116-0e2b295bd6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5783089e-d60c-439f-bfb4-2c6a946f3fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f675fd-e5b7-4016-8706-f6ddaa8c794f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "214b5a02-d626-4d82-9316-0be545f74fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_and_prune_COF_tree_v2(X_train, y_train, X_test=None, y_test=None, \n",
    "                                initial_tree_params=None, optimizer=\"gurobi\", \n",
    "                                alpha=1e-6, h_min=0, ignore_h=False, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Train, build COF models, prune in parallel, and print stats.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Train a DecisionTreeRegressor, build COF models for leaves, and prune the tree in place.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train, y_train : np.ndarray\n",
    "        Training data\n",
    "    initial_tree_params : dict\n",
    "        Parameters to initialize DecisionTreeRegressor\n",
    "    optimizer : str\n",
    "        COF optimizer (\"gurobi\", \"gurobi_MSE\", etc.)\n",
    "    alpha : float\n",
    "        Penalty for number of leaves\n",
    "    h_min : float\n",
    "        Minimum allowed h for pruning\n",
    "    ignore_h : bool\n",
    "        If True, h is ignored in pruning\n",
    "    n_jobs : int\n",
    "        Number of parallel jobs for leaf computation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tree : DecisionTreeRegressor\n",
    "        The pruned tree\n",
    "    COF_model_tree : list of dict\n",
    "        COF models (updated after pruning)\n",
    "    \"\"\"\n",
    "    # 1️⃣ Train initial tree\n",
    "    if initial_tree_params is None:\n",
    "        initial_tree_params = {\"max_depth\": 5}\n",
    "    tree = DecisionTreeRegressor(**initial_tree_params)\n",
    "    tree.fit(X_train, y_train)\n",
    "\n",
    "    # 2️⃣ Build COF models for leaves\n",
    "    COF_model_tree = train_COF_on_leaves_parallel(X_train, y_train, tree, optimizer=optimizer, n_jobs=n_jobs)\n",
    "\n",
    "    # 3️⃣ Leaf info mapping\n",
    "    children_left = tree.tree_.children_left\n",
    "    children_right = tree.tree_.children_right\n",
    "    leaf_nodes = np.where(children_left == -1)[0]\n",
    "    leaf_h_dict = {leaf: COF_model_tree[i]['CO_Model']['h'] for i, leaf in enumerate(leaf_nodes)}\n",
    "    leaf_indices_dict = {leaf: COF_model_tree[i]['indices'] for i, leaf in enumerate(leaf_nodes)}\n",
    "    leaf_COFS_dict = {leaf: COF_model_tree[i] for i, leaf in enumerate(leaf_nodes)}\n",
    "\n",
    "    # Helper function to compute h dynamically\n",
    "    def compute_h(indices):\n",
    "        if ignore_h:\n",
    "            return 0\n",
    "        if optimizer == \"gurobi\":\n",
    "            _, _, h = constrained_optimization_gurobi(X_train[indices], y_train[indices])\n",
    "        elif optimizer == \"gurobi_MSE\":\n",
    "            _, _, h = constrained_optimization_MSE_gurobi(X_train[indices], y_train[indices])\n",
    "        else:\n",
    "            _, _, h = constrained_optimization(X_train[indices], y_train[indices])\n",
    "        return max(h, h_min)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 4️⃣ Parallelized recursive pruning\n",
    "    # -------------------------------\n",
    "    def prune_node(node):\n",
    "        left = children_left[node]\n",
    "        right = children_right[node]\n",
    "\n",
    "        if left == -1 and right == -1:\n",
    "            # Leaf\n",
    "            h_leaf = leaf_h_dict[node]\n",
    "            return h_leaf, 1, leaf_indices_dict[node]\n",
    "\n",
    "        # Evaluate left/right subtrees in parallel\n",
    "        results = Parallel(n_jobs=2)(\n",
    "            delayed(prune_node)(child) for child in [left, right]\n",
    "        )\n",
    "        (left_cost, left_leaves, left_indices), (right_cost, right_leaves, right_indices) = results\n",
    "\n",
    "        combined_indices = np.concatenate([left_indices, right_indices])\n",
    "        subtree_cost = left_cost + right_cost\n",
    "        subtree_leaves = left_leaves + right_leaves\n",
    "\n",
    "        h_parent = compute_h(combined_indices)\n",
    "        prune_cost = h_parent + alpha\n",
    "        prune_leaves = 1\n",
    "\n",
    "        if prune_cost <= subtree_cost:\n",
    "            # Prune children\n",
    "            children_left[node] = -1\n",
    "            children_right[node] = -1\n",
    "\n",
    "            # Update tree.value\n",
    "            tree.tree_.value[node] = np.array([[h_parent]])\n",
    "            leaf_h_dict[node] = h_parent\n",
    "            leaf_indices_dict[node] = combined_indices\n",
    "\n",
    "            leaf_COFS_dict[node] = {\n",
    "                \"leaf_id\": node,\n",
    "                \"CO_Model\": {\"h\": h_parent},\n",
    "                \"indices\": combined_indices,\n",
    "                \"no_samples\": len(combined_indices)\n",
    "            }\n",
    "            return prune_cost, prune_leaves, combined_indices\n",
    "        else:\n",
    "            return subtree_cost, subtree_leaves, combined_indices\n",
    "\n",
    "    # 5️⃣ Stats before pruning\n",
    "    num_leaves_before = len(leaf_nodes)\n",
    "    if X_test is not None and y_test is not None:\n",
    "        nrmse_train_before = normalized_root_mean_square_error(y_train, tree.predict(X_train))\n",
    "        nrmse_test_before = normalized_root_mean_square_error(y_test, tree.predict(X_test))\n",
    "        nrmse_train_COF_before = normalized_root_mean_square_error(y_train, predict_from_COF(COF_model_tree, X_train, tree))\n",
    "        nrmse_test_COF_before = normalized_root_mean_square_error(y_test, predict_from_COF(COF_model_tree, X_test, tree))\n",
    "        print(f\"Before pruning: Leaves={num_leaves_before}, NRMSE[Tree]={nrmse_train_before:.4f}/{nrmse_test_before:.4f}, NRMSE[COF]={nrmse_train_COF_before:.4f}/{nrmse_test_COF_before:.4f}\")\n",
    "\n",
    "    # 6️⃣ Start pruning from root\n",
    "    prune_node(0)\n",
    "\n",
    "    # 7️⃣ Stats after pruning\n",
    "    COF_model_tree_pruned = list(leaf_COFS_dict.values())\n",
    "    num_leaves_after = len(COF_model_tree_pruned)\n",
    "    if X_test is not None and y_test is not None:\n",
    "        nrmse_train_after = normalized_root_mean_square_error(y_train, tree.predict(X_train))\n",
    "        nrmse_test_after = normalized_root_mean_square_error(y_test, tree.predict(X_test))\n",
    "        nrmse_train_COF_after = normalized_root_mean_square_error(y_train, predict_from_COF(COF_model_tree_pruned, X_train, tree))\n",
    "        nrmse_test_COF_after = normalized_root_mean_square_error(y_test, predict_from_COF(COF_model_tree_pruned, X_test, tree))\n",
    "        print(f\"After pruning: Leaves={num_leaves_after}, NRMSE[Tree]={nrmse_train_after:.4f}/{nrmse_test_after:.4f}, NRMSE[COF]={nrmse_train_COF_after:.4f}/{nrmse_test_COF_after:.4f}\")\n",
    "\n",
    "    return tree, COF_model_tree_pruned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccd594c4-d8d0-4c37-a986-77ed830589d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Before pruning: Leaves=32, NRMSE[Tree]=0.0687/0.0686, NRMSE[COF]=0.0034/0.0034\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "After pruning: Leaves=32, NRMSE[Tree]=0.0687/0.0686, NRMSE[COF]=0.0034/0.0034\n"
     ]
    }
   ],
   "source": [
    "tree, COF_model =train_and_prune_COF_tree_v2(X_train, y_train, X_test=X_test, y_test=y_test, \n",
    "                                initial_tree_params=None, optimizer=\"gurobi\", \n",
    "                                alpha=2, h_min=1, ignore_h=False, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a7578ad-939c-4c40-becf-3c458b3ae23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.247495125548335,\n",
       " 15.970116469724417,\n",
       " 9.896257219805027e-05,\n",
       " 13.297375726471527,\n",
       " 10.063166303461312,\n",
       " 19.84039996511953,\n",
       " 0.0002915225348566648,\n",
       " 26.351826120993845,\n",
       " 19.731365798253936,\n",
       " 27.784298026201956,\n",
       " 0.020334491560324075,\n",
       " 2.8069198481649876e-05,\n",
       " 11.76965344336756,\n",
       " 12.840628849843235,\n",
       " 19.50031001173883,\n",
       " 12.95491713564067,\n",
       " 26.714420813368676,\n",
       " 19.631612981318483,\n",
       " 2.882897122440768e-05,\n",
       " 12.976188609804481,\n",
       " 15.59830223027343,\n",
       " 5.081453184122862e-05,\n",
       " 3.616854578928122e-05,\n",
       " 5.152554209046204e-05,\n",
       " 0.00012612581272515916,\n",
       " 12.577742652167915,\n",
       " 1.3682289603790343e-05,\n",
       " 3.816671606296249e-05,\n",
       " 26.894228586387467,\n",
       " 12.080145465898758,\n",
       " 0.00013950234297040176,\n",
       " 15.719675279783754]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_h_from_COF(COF_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960b945-3a25-4368-aa32-168d7fe757c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1b2ecff-9161-47e9-822f-ef1b4a12e98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of leaves: 32\n",
      "Total depth of tree: 5\n",
      "Number of nodes: 63\n"
     ]
    }
   ],
   "source": [
    "numLeaves = tree.get_n_leaves()\n",
    "print(f\"Number of leaves: {tree.get_n_leaves()}\")\n",
    "print(f\"Total depth of tree: {tree.get_depth()}\")\n",
    "print(f\"Number of nodes: {tree.tree_.node_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce816d7-2264-435a-9a48-c772bf20b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pruning path\n",
    "path = tree.cost_complexity_pruning_path(X, y)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "# Train trees for each alpha\n",
    "trees = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeRegressor(random_state=42, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X, y)\n",
    "    trees.append(clf)\n",
    "\n",
    "# Evaluate on validation set and pick best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb06771e-a5b8-4975-8ae0-d40b3ca5125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Train/Val/Test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 2. Train full tree and get pruning path\n",
    "tree = DecisionTreeRegressor(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "path = tree.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "\n",
    "# 3. Train/prune with different alphas and evaluate on validation set\n",
    "val_mse = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeRegressor(random_state=42, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    val_mse.append(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "# 4. Find best alpha\n",
    "best_alpha = ccp_alphas[np.argmin(val_mse)]\n",
    "\n",
    "# 5. Retrain final tree on Train+Val\n",
    "X_trainval = np.vstack([X_train, X_val])\n",
    "y_trainval = np.hstack([y_train, y_val])\n",
    "\n",
    "final_tree = DecisionTreeRegressor(random_state=42, ccp_alpha=best_alpha)\n",
    "final_tree.fit(X_trainval, y_trainval)\n",
    "\n",
    "# 6. Test evaluation\n",
    "y_test_pred = final_tree.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"Best alpha:\", best_alpha)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "\n",
    "# 7. Plot validation curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(ccp_alphas, val_mse, marker=\"o\", drawstyle=\"steps-post\")\n",
    "plt.axvline(best_alpha, color=\"red\", linestyle=\"--\", label=f\"Best α = {best_alpha:.5f}\")\n",
    "plt.xlabel(\"ccp_alpha (complexity parameter)\")\n",
    "plt.ylabel(\"Validation MSE\")\n",
    "plt.title(\"Validation Curve for Cost Complexity Pruning\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26c9bc36-e378-4536-9da1-4b0eed2960f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/ResearchTasks/Extracting_BSR_Benchmark_Neural_Abstraction/venv/lib/python3.10/site-packages/joblib/parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResearchTasks/Extracting_BSR_Benchmark_Neural_Abstraction/venv/lib/python3.10/site-packages/joblib/parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1798\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[1;32m   1799\u001b[0m ):\n\u001b[0;32m-> 1800\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1801\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     y_val_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mean_squared_error(y_val, y_val_pred)\n\u001b[0;32m---> 25\u001b[0m val_mse \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_alpha\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mccp_alphas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 4. Pick best alpha\u001b[39;00m\n\u001b[1;32m     28\u001b[0m best_alpha \u001b[38;5;241m=\u001b[39m ccp_alphas[np\u001b[38;5;241m.\u001b[39margmin(val_mse)]\n",
      "File \u001b[0;32m~/ResearchTasks/Extracting_BSR_Benchmark_Neural_Abstraction/venv/lib/python3.10/site-packages/joblib/parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ResearchTasks/Extracting_BSR_Benchmark_Neural_Abstraction/venv/lib/python3.10/site-packages/joblib/parallel.py:1732\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m   1731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1732\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_abort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;66;03m# Store the unconsumed tasks and terminate the workers if necessary\u001b[39;00m\n",
      "File \u001b[0;32m~/ResearchTasks/Extracting_BSR_Benchmark_Neural_Abstraction/venv/lib/python3.10/site-packages/joblib/parallel.py:1646\u001b[0m, in \u001b[0;36mParallel._abort\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborted \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(backend, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabort_everything\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;66;03m# If the backend is managed externally we need to make sure\u001b[39;00m\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;66;03m# to leave it in a working state to allow for future jobs\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m     \u001b[38;5;66;03m# scheduling.\u001b[39;00m\n\u001b[1;32m   1645\u001b[0m     ensure_ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_managed_backend\n\u001b[0;32m-> 1646\u001b[0m     \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabort_everything\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensure_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ready\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/ResearchTasks/Extracting_BSR_Benchmark_Neural_Abstraction/venv/lib/python3.10/site-packages/joblib/_parallel_backends.py:725\u001b[0m, in \u001b[0;36mLokyBackend.abort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mabort_everything\u001b[39m(\u001b[38;5;28mself\u001b[39m, ensure_ready\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    724\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Shutdown the workers and restart a new one with the same parameters\"\"\"\u001b[39;00m\n\u001b[0;32m--> 725\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkill_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ensure_ready:\n",
      "File \u001b[0;32m~/ResearchTasks/Extracting_BSR_Benchmark_Neural_Abstraction/venv/lib/python3.10/site-packages/joblib/executor.py:86\u001b[0m, in \u001b[0;36mMemmappingExecutor.terminate\u001b[0;34m(self, kill_workers)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mterminate\u001b[39m(\u001b[38;5;28mself\u001b[39m, kill_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkill_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkill_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# When workers are killed in a brutal manner, they cannot execute the\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# finalizer of their shared memmaps. The refcount of those memmaps may\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# be off by an unknown number, so instead of decref'ing them, we force\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# with allow_non_empty=True but if we can't, it will be clean up later\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# on by the resource_tracker.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submit_resize_lock:\n",
      "File \u001b[0;32m~/ResearchTasks/Extracting_BSR_Benchmark_Neural_Abstraction/venv/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:1333\u001b[0m, in \u001b[0;36mProcessPoolExecutor.shutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executor_manager_thread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m wait:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# This locks avoids concurrent join if the interpreter\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# is shutting down.\u001b[39;00m\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _global_shutdown_lock:\n\u001b[0;32m-> 1333\u001b[0m         \u001b[43mexecutor_manager_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m         _threads_wakeups\u001b[38;5;241m.\u001b[39mpop(executor_manager_thread, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;66;03m# To reduce the risk of opening too many files, remove references to\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;66;03m# objects that use file descriptors.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# 1. Train/Val/Test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 2. Get pruning path\n",
    "tree = DecisionTreeRegressor(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "path = tree.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "\n",
    "# 3. Parallel training & validation\n",
    "def evaluate_alpha(ccp_alpha):\n",
    "    clf = DecisionTreeRegressor(random_state=42, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    return mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "val_mse = Parallel(n_jobs=-1)(delayed(evaluate_alpha)(alpha) for alpha in ccp_alphas)\n",
    "\n",
    "# 4. Pick best alpha\n",
    "best_alpha = ccp_alphas[np.argmin(val_mse)]\n",
    "\n",
    "# 5. Retrain final tree on Train+Val\n",
    "X_trainval = np.vstack([X_train, X_val])\n",
    "y_trainval = np.hstack([y_train, y_val])\n",
    "\n",
    "final_tree = DecisionTreeRegressor(random_state=42, ccp_alpha=best_alpha)\n",
    "final_tree.fit(X_trainval, y_trainval)\n",
    "\n",
    "# 6. Test evaluation\n",
    "y_test_pred = final_tree.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"Best alpha:\", best_alpha)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "\n",
    "# 7. Plot validation curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(ccp_alphas, val_mse, marker=\"o\", drawstyle=\"steps-post\")\n",
    "plt.axvline(best_alpha, color=\"red\", linestyle=\"--\", label=f\"Best α = {best_alpha:.5f}\")\n",
    "plt.xlabel(\"ccp_alpha (complexity parameter)\")\n",
    "plt.ylabel(\"Validation MSE\")\n",
    "plt.title(\"Validation Curve for Cost Complexity Pruning\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06b650-53ee-4c89-a8c7-578a93ba45aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best alpha:\", best_alpha)\n",
    "print(\"Test MSE:\", test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26d00eb-e20f-4d6c-97d6-79ce5c683752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
